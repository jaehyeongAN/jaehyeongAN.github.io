<!DOCTYPE html>
<html lang="en">

<head><meta name="generator" content="Hexo 3.9.0">

  <!-- Minima -->
  <!-- Hexo theme created by @adisaktijrs -->

  <!-- Basic Page Needs
  â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“ -->
  <meta charset="utf-8">

  
  <title>[Paper Review] PEGASUS:Pre-training with Extracted Gap-sentences for Abstractive Summarization</title>
  
  <link rel="sitemap" href="https://jaehyeongan.github.iositemap.xml">
  
  <link rel="canonical" href="https://jaehyeongan.github.io/2020/08/01/PEGASUS/">
  
  <meta name="description" content="Introìµœê·¼ NLPì˜ downstream tasks ì¤‘ í•˜ë‚˜ì¸ Summarizationë¶„ì•¼ì— â€œPEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarizationâ€ì´ë¼ëŠ” ìƒˆë¡œìš´ ë…¼ë¬¸(ë©‹ì§„ ì´">
  
  
  <meta name="author" content>
  
  <meta property="og:image" content="https://jaehyeongan.github.ioundefined">
  
  <meta property="og:site_name" content="jaehyeong&#39;s ds">
  <meta property="og:type" content="article">
  <meta property="og:title" content="[Paper Review] PEGASUS:Pre-training with Extracted Gap-sentences for Abstractive Summarization">
  
  <meta property="og:description" content="Introìµœê·¼ NLPì˜ downstream tasks ì¤‘ í•˜ë‚˜ì¸ Summarizationë¶„ì•¼ì— â€œPEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarizationâ€ì´ë¼ëŠ” ìƒˆë¡œìš´ ë…¼ë¬¸(ë©‹ì§„ ì´">
  
  <meta property="og:url" content="https://jaehyeongan.github.io/2020/08/01/PEGASUS/">

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="[Paper Review] PEGASUS:Pre-training with Extracted Gap-sentences for Abstractive Summarization">
  
  <meta name="twitter:description" content="Introìµœê·¼ NLPì˜ downstream tasks ì¤‘ í•˜ë‚˜ì¸ Summarizationë¶„ì•¼ì— â€œPEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarizationâ€ì´ë¼ëŠ” ìƒˆë¡œìš´ ë…¼ë¬¸(ë©‹ì§„ ì´">
  
  
  <meta name="twitter:image" content="https://jaehyeongan.github.ioundefined">
  
  <meta name="twitter:url" content="https://jaehyeongan.github.io/2020/08/01/PEGASUS/">

  <!-- Mobile Specific Metas
  â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“ -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Preload fonts
  â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“ -->
  <link rel="preload" href="../fonts/dm-serif-display-v4-latin-regular.woff2" as="font" type="font/woff2" crossorigin>
  <link rel="preload" href="../fonts/inter-v2-latin-regular.woff2" as="font" type="font/woff2" crossorigin>

  <!-- CSS
  â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“ -->
  <link rel="stylesheet" href="/css/normalize.css">
  <link rel="stylesheet" href="/css/skeleton.css">
  <link rel="stylesheet" href="/css/custom.css">
  <link rel="stylesheet" href="/css/prism-dark.css">
  <link rel="stylesheet" href="/css/prism-line-numbers.css">
  <!-- User css -->
  
  <link rel="stylesheet" href="/css/user.css">
  

  <!-- Favicon
  â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“ -->
  <link rel="icon" type="image/png" href="/images/astronaut.png">

  <!-- Custom Theme Color Style
  â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“ -->
  <style>
  a:not(.icon) {
    text-decoration-color: #0FA0CE;
    background-image: linear-gradient(
      to bottom,
      rgba(0, 0, 0, 0) 50%,
      #0FA0CE 50%
    );
  }
  blockquote {
    border-left: 8px solid #0FA0CE;
  }
  .nanobar .bar {
    background: #0FA0CE;
  }
  .button.button-primary:hover,
  button.button-primary:hover,
  input[type="submit"].button-primary:hover,
  input[type="reset"].button-primary:hover,
  input[type="button"].button-primary:hover,
  .button.button-primary:focus,
  button.button-primary:focus,
  input[type="submit"].button-primary:focus,
  input[type="reset"].button-primary:focus,
  input[type="button"].button-primary:focus {
    background-color: #0FA0CE;
    border-color: #0FA0CE;
  }
  input[type="email"]:focus,
  input[type="number"]:focus,
  input[type="search"]:focus,
  input[type="text"]:focus,
  input[type="tel"]:focus,
  input[type="url"]:focus,
  input[type="password"]:focus,
  textarea:focus,
  select:focus {
    border: 1px solid #0FA0CE;
  }
</style>

  <!-- Google Analytics (With Privacy Settings On)
  â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“ -->
  

</head>

<body>
  <div class="container">
    <div class="row">
      <div>

        <div class="row">
  <div class="two columns" style="max-width: 50px">
    <h1 class="mt-2 mode">
      <div onclick="setDarkMode(true)" id="darkBtn">ğŸŒ‘</div>
      <div onclick="setDarkMode(false)" id="lightBtn" class="hidden">â˜€ï¸</div>
      <script>
        if (localStorage.getItem('preferredTheme') == 'dark') {
          setDarkMode(true)
        }
        function setDarkMode(isDark) {
          var darkBtn = document.getElementById('darkBtn')
          var lightBtn = document.getElementById('lightBtn')
          if (isDark) {
            lightBtn.style.display = "block"
            darkBtn.style.display = "none"
            localStorage.setItem('preferredTheme', 'dark');
          } else {
            lightBtn.style.display = "none"
            darkBtn.style.display = "block"
            localStorage.removeItem('preferredTheme');
          }
          document.body.classList.toggle("darkmode");
        }
      </script>
    </h1>
  </div>

  <div class="six columns ml-1">
    <h1 class="mt-2">
      â¬… Apply Dark.
    </h1>
  </div>

  <div class="twelve columns">
    <div class="row">
      <div class="nine columns left">
        <a href="/">Home</a>
        
          
          <a href="/about" class="ml">About</a>
          
        
        
          
            <a href="mailto:nonamed000000@gmail.com" target="_blank" class="ml">Email</a>
          
        
      </div>
    </div>
    <hr style="margin-bottom: 2.6rem">
  </div>
</div>

        <div class="trans">
            <h2>[Paper Review] PEGASUS:Pre-training with Extracted Gap-sentences for Abstractive Summarization</h2>

  <h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h2><p>ìµœê·¼ NLPì˜ downstream tasks ì¤‘ í•˜ë‚˜ì¸ Summarizationë¶„ì•¼ì— â€œPEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarizationâ€ì´ë¼ëŠ” ìƒˆë¡œìš´ ë…¼ë¬¸(ë©‹ì§„ ì´ë¦„ì´ë‹¤..)ì´ ë“±ì¥í•˜ì—¬ ê°„ëµí•˜ê²Œ ì†Œê°œí•´ë³´ë ¤ê³  í•œë‹¤.</p>
<h2 id><a href="#" class="headerlink" title></a><p style="text-align: center;"><img src="/image/pegasus-image.jpg" width="550"></p></h2><h2 id="What-is-Text-Summarization"><a href="#What-is-Text-Summarization" class="headerlink" title="What is Text Summarization?"></a>What is Text Summarization?</h2><p>Text Summarizationì€ ìì—°ì–´ ì²˜ë¦¬ ë¶„ì•¼ì˜ ì—¬ëŸ¬ ê°œì˜ Downstram tasksì¤‘ í•˜ë‚˜ì´ë‹¤.<br>ì´ë¦„ì—ì„œë¶€í„° ì‰½ê²Œ ì•Œ ìˆ˜ ìˆë“¯ì´ Text Summarizationì€ ë¬¸ì„œë¥¼ ìš”ì•½í•˜ëŠ” ê¸°ìˆ ì„ ì˜ë¯¸í•œë‹¤. </p>
<p>Text Summarizationì€ í¬ê²Œ ì•„ë˜ì™€ ê°™ì´ ë‘ ê°€ì§€ë¡œ ë¶„ë¥˜ê°€ ëœë‹¤.</p>
<blockquote>
<p><strong>1. Extractive Summarization</strong><br><strong>2. Abstractive Summarization</strong></p>
</blockquote>
<p><img src="/image/extractive-abstractive.PNG" width="900"><br>ìœ„ ë‘ ë°©ì‹ì€ ìš”ì•½(summarization)ì„ í•œë‹¤ëŠ” ì¸¡ë©´ì—ì„œëŠ” ë™ì¼í•˜ë‚˜, ê·¸ ë°©ë²•ì— ì°¨ì´ê°€ ìˆë‹¤.<br>ìœ„ ì˜ˆì‹œì™€ ê°™ì´ ExtractiveëŠ” ì›ë¬¸ í…ìŠ¤íŠ¸ë¡œë¶€í„° ì£¼ìš” Sentenceë¥¼ ì›ë¬¸ ê·¸ëŒ€ë¡œ ì¶”ì¶œí•´ë‚´ëŠ” ë°©ì‹ì´ë¼ë©´, AbstractiveëŠ” ìš°ë¦¬ê°€ ì›ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ë³´ê³  ìƒê°ê³¼ ëŠë‚Œì„ í•œ ì¤„ ìš”ì•½í•˜ë“¯ì´ í‘œí˜„í•˜ëŠ” ë°©ì‹ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤. </p>
<p>Extractive Summarizationì—ì„œ ê°€ì¥ ë§ì´ ì•Œë ¤ì§„ ì•Œê³ ë¦¬ì¦˜ì€ ì•„ë¬´ë˜ë„ Text-Rankì¼ ê²ƒì´ë‹¤. ì´ˆê¸° êµ¬ê¸€ì˜ ê²€ìƒ‰ì—”ì§„ë­í‚¹ ì•Œê³ ë¦¬ì¦˜ì¸ Page-Rankë¥¼ Textì— ì ìš©í•œ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ, ì ì€ ì—°ì‚°ëŸ‰ìœ¼ë¡œë„ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ê³  ìˆë‹¤. Text-Rankì•Œê³ ë¦¬ì¦˜ì€ Document ë‚´ì—ì„œ Term-Frequencyê°€ ë†’ê³ , Co-occurenceê°€ ë†’ì€ ë‹¨ì–´ë¥¼ keywordë¡œ íŒë‹¨í•˜ë©°, ê·¸ëŸ¬í•œ keywordë¥¼ ë§ì´ ê°–ëŠ” Sentenceë¥¼ Key-Sentenceì¼ ê²ƒì´ë¼ ê°€ì •í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤.<br>Text-Rankì˜ ìì„¸í•œ ì„¤ëª…ì€ í•´ë‹¹ ë§í¬(<a href="https://lovit.github.io/nlp/2019/04/30/textrank/" target="_blank" rel="noopener">TextRank ë¥¼ ì´ìš©í•œ í‚¤ì›Œë“œ ì¶”ì¶œê³¼ í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ</a>) ì°¸ì¡°í•˜ë©´ ì¢‹ì„ ê²ƒ ê°™ë‹¤.</p>
<h4 id="Extractive-Summarization-vs-Abstractive-Summarization"><a href="#Extractive-Summarization-vs-Abstractive-Summarization" class="headerlink" title="Extractive Summarization vs Abstractive Summarization"></a>Extractive Summarization vs Abstractive Summarization</h4><p>ë‘˜ ì¤‘ì— ìµœê·¼ ê°€ì¥ í™œë°œíˆ ì—°êµ¬ë˜ëŠ” ë¶„ì•¼ëŠ” ì•„ë¬´ë˜ë„ Abstractive Summarizationì´ë‹¤.<br>Abstractiveë°©ì‹ì´ Extractiveë°©ì‹ë³´ë‹¤ í›¨ì”¬ ì–´ë ¤ìš´ ë‚œì´ë„ì˜ taskì¼ ë¿ë§Œ ì•„ë‹ˆë¼ ì›ë¬¸ì„ ê·¸ëŒ€ë¡œ ì¶”ì¶œí•´ë‚´ëŠ” ê²ƒì´ ì•„ë‹Œ ë‹¤ì–‘í•œ í‘œí˜„ë°©ì‹ìœ¼ë¡œ Generateí•˜ê¸° ë•Œë¬¸ì— í›¨ì”¬ ë” ë‹¤ì–‘í•œ ë¶„ì•¼ì— ì‚¬ìš©ë  ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤.<br>ìµœê·¼ ëª‡ ë…„ ì‚¬ì´ Seqence-to-Sequence, Attention mechanism, Transformer ë“±ê³¼ ê°™ì€ ì•„í‚¤í…ì²˜ê°€ ë“±ì¥í•˜ê³  Bertì™€ ê°™ì€ ëŒ€ëŸ‰ì˜ corpusë¡œ í•™ìŠµëœ pre-training ëª¨ë¸ì´ ë“±ì¥í•˜ë©° ì´ëŸ¬í•œ generatorëª¨ë¸ì˜ ì„±ëŠ¥ë„ ë‚˜ë‚ ì´ í–¥ìƒë˜ëŠ” ì¶”ì„¸ì´ë‹¤.<br>ì´ì œ ì•„ë˜ì—ì„œ ê°€ì¥ ìµœê·¼ Abstractive Summarizaion ë…¼ë¬¸ìœ¼ë¡œ ë“±ì¥í•œ PEGASUSì— ëŒ€í•´ ì•Œì•„ë³´ì.<br><br></p>
<h1 id="PEGASUS-Pre-training-with-Extracted-Gap-sentences-for-Abstractive-Summarization"><a href="#PEGASUS-Pre-training-with-Extracted-Gap-sentences-for-Abstractive-Summarization" class="headerlink" title="PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization"></a>PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>ìµœê·¼ ëŒ€ëŸ‰ì˜ text corporaë¡œ self-supervisedëœ pre-training Transfomers ëª¨ë¸ë“¤ì´ text summarizationì„ í¬í•¨í•œ fine-tuning downstream NLP taskì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ê³  ìˆë‹¤. í•˜ì§€ë§Œ, Abstractive Text Summarizationì˜ ëª©ì ì— ë§ê²Œ pre-trainingëœ ëª¨ë¸ì€ ì°¾ì•„ë³´ê¸° í˜ë“¤ê³ , ë”ìš±ì´ ë‹¤ì–‘í•œ domainì„ ì»¤ë²„í• ë§Œ í•œ ì²´ê³„ì ì¸ í‰ê°€ ë°©ë²•ë„ ë¶€ì¡±í•œ ìƒí™©ì´ë‹¤. </p>
<p>ë”°ë¼ì„œ, í•´ë‹¹ ë…¼ë¬¸ì—ì„œëŠ” ëŒ€ëŸ‰ì˜ text coporaë¡œ self-supervisedëœ encoder-decoder ê¸°ë°˜ì˜ pre-training Transformer ëª¨ë¸ì¸ PEGASUSë¥¼ ì†Œê°œí•œë‹¤. PEGASUSì˜ ì£¼ìš” íŠ¹ì§•ì€ GSG(Gap sentence generation)ì„ ì‚¬ìš©í•œë‹¤ëŠ” ê²ƒì¸ë°, ê°„ë‹¨íˆ ì„¤ëª…í•˜ë©´ MLMë°©ì‹ì—ì„œëŠ” token ë‹¨ìœ„ë¡œ maskingí•˜ì—¬ masked tokenì„ ì˜ˆì¸¡í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµí–ˆë˜ ê²ƒê³¼ ìœ ì‚¬í•˜ê²Œ, GSGëŠ” token ë‹¨ìœ„ê°€ ì•„ë‹Œ Importance Sentence ë‹¨ìœ„ë¡œ maskingì„ í•˜ì—¬ í•™ìŠµì„ ìˆ˜í–‰í•œë‹¤. ì—¬ê¸°ì„œ ë§í•˜ëŠ” Importance Sentenceë€ document ë‚´ì—ì„œ ë‹¤ë¥¸ ë¬¸ì¥ì— ë¹„í•´ ì „ì²´ì ì¸ contextë¥¼ ì˜ ì„¤ëª…í•  ìˆ˜ ìˆëŠ” ë¬¸ì¥ì„ ë§í•œë‹¤. </p>
<p>PEGASUS ëª¨ë¸ì€ 12ê°œì˜ downstream summarization tasksë¡œë¶€í„° ROUGE scoreë¥¼ ê¸°ë°˜ìœ¼ë¡œ SOTAë¥¼ ë‹¬ì„±í•˜ì˜€ê³ , ê·¸ ì¤‘ 6ê°œì˜ ë°ì´í„° ì…‹ì—ì„œ ì˜¤ì§ 1,000ê°œì˜ examplesë§Œìœ¼ë¡œë„ SOTAë¥¼ ë‹¬ì„±í•  ë§Œí¼ ì ì€ ë¦¬ì†ŒìŠ¤ ë¹„ìš©ìœ¼ë¡œ ë†€ë¼ìš´ ì„±ëŠ¥ì„ ë‚˜íƒ€ë‚´ì—ˆë‹¤. </p>
<h2 id="The-Basic-architecture-of-PEGASUS"><a href="#The-Basic-architecture-of-PEGASUS" class="headerlink" title="The Basic architecture of PEGASUS"></a>The Basic architecture of PEGASUS</h2><p><img src="/image/PEGASUS-architecture.PNG" width="800"><br><img src="/image/pegasus-transformer.gif" width="800"></p>
<p>PEGASUSëŠ” ê¸°ë³¸ì ìœ¼ë¡œ encoder-decoderê¸°ë°˜ì˜ Transformerêµ¬ì¡°ë¥¼ í•˜ê³  ìˆìœ¼ë©°, ê¸°ì¡´ MLM(Masked Language Model)ê³¼ ìœ ì‚¬í•˜ê²Œ Input textì˜ ì¼ë¶€ë¥¼ maskingí•˜ì—¬ Encoderì˜ inputìœ¼ë¡œ ë³´ë‚´ê²Œ ëœë‹¤. í•˜ì§€ë§Œ ê¸°ì¡´ MLMê³¼ ë‹¤ë¥¸ ì ì€ ë°”ë¡œ Sentence ìì²´ë¥¼ maskingí•œë‹¤ëŠ” ì ì´ë‹¤.<br>ê¸°ì¡´ MLM ëª¨ë¸ë“¤ì€ token ë‹¨ìœ„ë¡œ maskingí•˜ì—¬ masked tokenì„ ì˜ˆì¸¡í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµì„ ì§„í–‰í•˜ì˜€ì§€ë§Œ, PEGASUSëŠ” Input Documentë¡œë¶€í„° Sentence ë‹¨ìœ„ë¡œ Maskingì„ í•œ í›„ ë‚¨ì€ Sentenceë¥¼ ê¸°ë°˜ìœ¼ë¡œ masked sentenceë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµëœë‹¤. ë…¼ë¬¸ì—ì„œëŠ” ì´ëŸ¬í•œ ë°©ì‹ì„ <strong>Gap-Sentences-Generation(GSG)</strong>ë¼ê³  ë§í•˜ê³  ìˆë‹¤.<br><br></p>
<h3 id="Gap-Sentences-Generation-GSG"><a href="#Gap-Sentences-Generation-GSG" class="headerlink" title="Gap Sentences Generation (GSG)"></a>Gap Sentences Generation (GSG)</h3><p>í•´ë‹¹ sectionì—ì„œëŠ” ìƒˆë¡œìš´ pre-training ë°©ì‹ì¸ GSGë¥¼ ì†Œê°œí•˜ê³ , ê¸°ì¡´ BERT masked-language modelê³¼ ë¹„êµë¥¼ ìˆ˜í–‰í•œë‹¤.</p>
<p>í•´ë‹¹ ë…¼ë¬¸ì—ì„œ ê°•ì¡°í•˜ëŠ” ê²ƒ ì¤‘ í•˜ë‚˜ëŠ”, <strong>ì¢‹ì€ ì„±ëŠ¥ì„ ì–»ê¸° ìœ„í•´ì„œëŠ” ì ìš©í•˜ê³ ì í•˜ëŠ” downstream taskì˜ ëª©ì ì— ë§ëŠ” pre-training ëª¨ë¸ì„ ì‚¬ìš©í•˜ê³  ì´ë¥¼ fine-tuning í•˜ë¼ëŠ” ê²ƒì´ë‹¤.</strong> ì¦‰, ë‰´ìŠ¤ë¥¼ ìš”ì•½í•˜ê¸° ìœ„í•œ ë°ì´í„°ë¡œ í•™ìŠµëœ ëª¨ë¸ì€ ë‰´ìŠ¤ ìš”ì•½ì— ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ê² ì§€ë§Œ, ì˜í™” ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ìš”ì•½í•˜ëŠ”ë°ëŠ” ì „í˜€ ë§ì§€ ì•Šì„ ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤. ë…¼ë¬¸ì˜ ì‹¤í—˜ë¶€ë¶„ì—ì„œ ë” ì†Œê°œê°€ ë˜ëŠ”ë° Newsê´€ë ¨ ë°ì´í„°ì…‹ìœ¼ë¡œ í•™ìŠµí•œ ëª¨ë¸ì€ Non-news taskì—ì„œëŠ” ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ì§€ ëª»í–ˆë‹¤.</p>
<p>Summarizationì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ì„œëŠ” input documentì™€ ê·¸ì— ë§ëŠ” summary textê°€ ìŒìœ¼ë¡œ í™œìš©ë˜ì–´ì•¼ í•œë‹¤. í•˜ì§€ë§Œ ë‹¨ìˆœíˆ extractive ë°©ì‹ìœ¼ë¡œ summaryë¥¼ ì¶”ì¶œí•˜ê²Œ ë˜ë©´ ëª¨ë¸ì€ ë‹¨ìˆœíˆ sentenceë¥¼ copyí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµì´ ë˜ê¸° ë•Œë¬¸ì—, ì €ìëŠ” ìµœê·¼ masking wordsì™€ contiguous spansì˜ ì„±ê³µì— ì˜ê°ì„ ë°›ì•„ GSGë¥¼ ìˆ˜í–‰í•œë‹¤ê³  ì„¤ëª…í•œë‹¤. </p>
<p>GSGëŠ” ì „ì²´ì ìœ¼ë¡œ ì•„ë˜ì™€ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ìˆ˜í–‰ëœë‹¤.</p>
<blockquote>
<ol>
<li>Select and mask whole sentences form documents.</li>
<li>Concatenate the gap-sentences into a pseudo-summary.</li>
<li>The corresponding position of each selected gap sentence is replaced by a mask token [MASK1] to inform the model</li>
</ol>
</blockquote>
<p>ì—¬ê¸°ì„œ gap sentence ë¹„ìœ¨ì€ GSR(Gap Sentences Ratio)ì— ì˜í•´ ê²°ì •ë˜ëŠ”ë° ì´ëŠ” ë¬¸ì„œì˜ ì „ì²´ sentenceì—ì„œ ì„ íƒëœ gap sentenceì˜ ë¹„ìœ¨ì„ ì˜ë¯¸í•˜ê³ , ë‹¤ë¥¸ Masked Language Modelì—ì„œì˜ mask rateì™€ ìœ ì‚¬í•œ ê°œë…ì´ë¼ê³  ìƒê°í•˜ë©´ ëœë‹¤. í•´ë‹¹ ë…¼ë¬¸ì—ì„œëŠ” GSRì˜ ë¹„ìœ¨ì— ë”°ë¥¸ ì„±ëŠ¥ì„ ì‹¤í—˜í•˜ì˜€ëŠ”ë° ë°ì´í„°ì…‹ì— ë”°ë¼ ì„±ëŠ¥ í¸ì°¨ê°€ ìˆì—ˆì§€ë§Œ, <strong>ìµœì¢…ì ìœ¼ë¡œ GSRì„ 30%ë¡œ ì„ íƒí•˜ì˜€ë‹¤ê³  í•œë‹¤.</strong></p>
<p><br></p>
<h4 id="Three-primary-strategies-for-gap-sentence"><a href="#Three-primary-strategies-for-gap-sentence" class="headerlink" title="Three primary strategies for gap-sentence"></a>Three primary strategies for gap-sentence</h4><p>ê·¸ë ‡ë‹¤ë©´ ì–´ë–¤ ë¬¸ì¥ì´ gap sentenceë¡œ ì„ íƒì´ ë˜ëŠ”ê±¸ê¹Œ?<br>í•´ë‹¹ ë…¼ë¬¸ì€ ì ì ˆí•œ Summarizationì„ ìœ„í•´ì„œ gap sentenceëŠ” documentë‚´ì—ì„œ ë‹¤ë¥¸ ë¬¸ì¥ë“¤(remaining sentence)ì— ë¹„í•´ ì „ì²´ ë¬¸ë§¥ì„ ì„¤ëª…í•  ìˆ˜ ìˆëŠ” ì¤‘ìš”í•œ(important/principal) ë¬¸ì¥ì´ ì„ íƒë˜ì–´ì•¼ í•œë‹¤ê³  í•œë‹¤. ì´ë¥¼ ìœ„í•´ Random, Lead, Principalì´ë¼ëŠ” 3ê°€ì§€ ì „ëµì„ ì‚¬ìš©í•œë‹¤. </p>
<p><img src="/image/gap-sentence-select.PNG" width="500"><br>Randomì€ ë§ê·¸ëŒ€ë¡œ ëœë¤í•˜ê²Œ mê°œì˜ sentenceë¥¼ ì¶”ì¶œí•˜ëŠ” ê²ƒì´ê³ , LeadëŠ” ë¬¸ì„œì˜ ê°€ì¥ ì²« mê°œì˜ ë¬¸ì¥, Principalì€ selected sentenceì™€ remaining sentenceê°„ì˜ ROUGE1-F1 scoreë¥¼ ê¸°ë°˜ìœ¼ë¡œ top-mê°œì˜ sentenceë¥¼ ì„ ì •í•˜ëŠ” ê²ƒì„ ë§í•œë‹¤. (Principal ë°©ë²•ì˜ ê²½ìš°ëŠ” Ind/seqê·¸ë¦¬ê³  Orig/Uniq ì˜µì…˜ìœ¼ë¡œ ì„¸ë¶„í™” ë˜ì–´ ì‹¤í—˜ëœë‹¤.)</p>
<p>ì•„ë˜ëŠ” documentë‚´ì—ì„œ Random, Lead, Principal(Ing-Orig) ê°ê°ì˜ ì „ëµì— ì˜í•´ ì„ íƒëœ sentenceë“¤ì„ ë³´ì—¬ì¤€ë‹¤.<br><img src="/image/gap-sentence-select-example.PNG" width="500"></p>
<h3 id="Masked-Language-Model-MLM"><a href="#Masked-Language-Model-MLM" class="headerlink" title="Masked Language Model(MLM)"></a>Masked Language Model(MLM)</h3><p>BERTì—ì„œëŠ” input textì˜ 15%ì˜ tokenì„ ì„ íƒí•˜ì—¬, ê·¸ ì¤‘ 80%ëŠ” mask tokenìœ¼ë¡œ ë³€í™˜í•˜ê³ , 10%ëŠ” random token, ë‚˜ë¨¸ì§€ 10%ëŠ” ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ê²Œ ëœë‹¤.<br>ìœ„ ì²«ë²ˆì§¸ ê·¸ë¦¼ì¸ PEGASUS ëª¨ë¸ì˜ ì•„í‚¤í…ì²˜ë¥¼ ë³´ë©´ GSGì™€ MLMì´ ë™ì‹œì— ì ìš©ë˜ê³  ìˆëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆì§€ë§Œ, ì‹¤ì œë¡œëŠ” MLMì´ downstream taskì˜ ì„±ëŠ¥ í–¥ìƒì— ì˜í–¥ì„ ì£¼ì§€ ì•Šì•„ ìµœì¢… ëª¨ë¸ì—ì„œëŠ” MLMì„ í¬í•¨í•˜ì§€ ì•Šì•˜ë‹¤ê³  í•œë‹¤. </p>
<p><br></p>
<h3 id="Pre-training-Corpus"><a href="#Pre-training-Corpus" class="headerlink" title="Pre-training Corpus"></a>Pre-training Corpus</h3><p>pre-trainingì„ ìœ„í•´ ì‚¬ìš©ëœ corpusëŠ” C4ì™€ HugeNewsì´ë‹¤.</p>
<ul>
<li>C4(Colossal and Cleaned version of Common Crawl) : consist of text from 350M web-pages(750GB)</li>
<li>HugeNews : a dataset of 1.5B articles (3.8TB) collected from news and news-like websites from 2013-2019</li>
</ul>
<h4 id="Downstream-Tasks-Datasets"><a href="#Downstream-Tasks-Datasets" class="headerlink" title="Downstream Tasks/Datasets"></a>Downstream Tasks/Datasets</h4><p>downstream summarization ë° ì¬í˜„ ê°€ëŠ¥í•œ ì½”ë“œ ì œê³µì„ ìœ„í•´ public datasetsì¸ <em><a href="https://www.tensorflow.org/datasets/catalog/overview" target="_blank" rel="noopener">Tensorflow Summarization Datasets</a></em> ë°ì´í„° ì…‹ì„ í™œìš©í•˜ì˜€ë‹¤. ì‚¬ìš©ëœ ë°ì´í„° ì…‹ì€ ì´ 12ê°œë¡œ ì•„ë˜ì™€ ê°™ë‹¤.<br>-Xsum<br>-CNN/DailyMail<br>-NEWSROOM<br>-Multi-News<br>-Gigaword<br>-arXiv<br>-PubMed<br>-BIGPATENT<br>-WikiHow<br>-Reddit TIFU<br>-AESLC<br>-BillSum</p>
<p><br></p>
<h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p>íš¨ìœ¨ì ì¸ ì‹¤í—˜ì„ ìœ„í•˜ì—¬ ëª¨ë¸ì˜ ì‚¬ì´ì¦ˆë¥¼ ì¤„ì¸ PEAGASUS-baseëª¨ë¸(223M parameters)ê³¼ PEGASUS-largeëª¨ë¸(568M parmeters)ì„ ê°ê° ë¹„êµí•œë‹¤.</p>
<blockquote>
<p><strong>PEAGASUS-base</strong></p>
<ul>
<li>number of layers of encoder and decoder(L) : 12</li>
<li>hidden size(H) : 768</li>
<li>feed-forward layer size(F) : 3,072</li>
<li>number of self-attention heads(A) : 12</li>
</ul>
<p><strong>PEGASUS-large</strong></p>
<ul>
<li>number of layers of encoder and decoder(L) : 16</li>
<li>hidden size(H) : 1024</li>
<li>feed-forward layer size(F) : 4,096</li>
<li>number of self-attention heads(A) : 16</li>
</ul>
</blockquote>
<p><br></p>
<h4 id="Pre-Training-Corpus"><a href="#Pre-Training-Corpus" class="headerlink" title="Pre-Training Corpus"></a>Pre-Training Corpus</h4><p><img src="/image/effect-of-pre-training-corpus.PNG" width="600"><br>ìœ„ ê·¸ë¦¼ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ í•™ìŠµì‹œ ì‚¬ìš©ëœ Corpusê°€ ë¬´ì—‡ì´ëƒì— ë”°ë¼ downstream taskì˜ ì„±ëŠ¥ì— ì˜í–¥ì„ ì£¼ê²Œ ëœë‹¤.<br>HugeNewsë¥¼ í† ëŒ€ë¡œ í•™ìŠµëœ ëª¨ë¸ì€ news ë°ì´í„° ì…‹(XSum, CNN/DailyMail)ì—ì„œëŠ” ë†’ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ê³  ìˆëŠ” ë°˜ë©´, non-news ë°ì´í„°ì…‹(WikiHow, Reddit TIFU)ì—ì„œëŠ” ë‚®ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ê³  ìˆë‹¤.</p>
<p><br></p>
<p><strong>EFFECT OF PRE-TRAINING OBJECTIVES</strong><br>GSGì˜ ì„±ëŠ¥ë¹„êµë¥¼ ìœ„í•´ Lead, Random, Ing-Oig, Ing-Uniq, Seq-Orig, Seq-Uniqë¥¼ ë¹„êµí•˜ì˜€ìœ¼ë©°, GSRì˜ ê²½ìš° ë°ì´í„°ì…‹ë§ˆë‹¤ ì„±ëŠ¥ ì°¨ì´ë¥¼ ë³´ì´ì§€ë§Œ, <strong>ìµœì¢…ì ìœ¼ë¡œ 30%ë¥¼ ì„ íƒí•˜ì˜€ë‹¤.</strong><br><img src="/image/effect-of-gap-sentence.PNG" width="1000"></p>
<p><strong>EFFECT OF VOCABULARY</strong><br>ì‹¤í—˜ì„ ìœ„í•´ BPE(Byte-pair encoding)ì™€ SentencePiece Unigramì„ ë¹„êµí•˜ì˜€ë‹¤.<br>ë¹„êµê²°ê³¼ news ë°ì´í„°ì…‹ì—ì„œëŠ” BPEì™€ Unigramì˜ ì„±ëŠ¥ì´ ìœ ì‚¬í•˜ì˜€ì§€ë§Œ, non-news ë°ì´í„°ì…‹(especially WikiHow)ì—ì„œëŠ” SentencePiece Unigramëª¨ë¸ì´ í›¨ì”¬ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚˜íƒ€ëƒˆë‹¤. </p>
<p>ìœ„ ê·¸ë˜í”„ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´, WikiHowì˜ ê²½ìš° Unigramì´ 128kì¼ ë•Œ, Reddit TIFUëŠ” 64kì¼ ë•Œ best scoreë¥¼ ë‚˜íƒ€ë‚´ì—ˆê¸° ë•Œë¬¸ì— ì´ë¥¼ ê³ ë ¤í•˜ì—¬ <strong>ìµœì¢…ì ìœ¼ë¡œ SentencePiece Unigramì„ ì‚¬ìš©í•˜ê³  vocabulary sizeëŠ” 96kë¡œ ì„ ì •í•˜ì˜€ë‹¤.</strong></p>
<p><br></p>
<h4 id="Larger-Model"><a href="#Larger-Model" class="headerlink" title="Larger Model"></a>Larger Model</h4><p><img src="/image/PEGASUS-result1.PNG" width="1000"><br>ìœ„ tableì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´, PEGASUSëª¨ë¸ì€ ì´ì „ SOTAëª¨ë¸ ëŒ€ë¹„ ëª¨ë“  12ê°œì˜ downstream tasksì—ì„œ ëª¨ë‘ SOTAë¥¼ ë‹¬ì„±í•œ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.</p>
<p><br></p>
<h4 id="Zero-and-Low-Resource-Summarization"><a href="#Zero-and-Low-Resource-Summarization" class="headerlink" title="Zero and Low-Resource Summarization"></a>Zero and Low-Resource Summarization</h4><p><img src="/image/PEGASUS-result2.PNG" width="1000"><br>PAGASUS-large ëª¨ë¸ì„ 2000 steps, 256 batch-size, 0.0005 learning-rateë¡œ fine-tuningí•˜ì˜€ì„ ë•Œ, ë‹¨ì§€ 100ê°œì˜ examplesë§Œìœ¼ë¡œë„ ê¸°ì¡´ 20k~200kê°œë¡œ í•™ìŠµëœ Transformer-baseëª¨ë¸ê³¼ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ì˜€ê³ , 1000ê°œì˜ examplesë¥¼ ì‚¬ìš©í•˜ì˜€ì„ë•Œ 12ê°œ ë°ì´í„° ì…‹ì¤‘ 6ê°œì˜ ë°ì´í„° ì…‹ì—ì„œ SOTAë¥¼ ë‹¬ì„±í•  ë§Œí¼ <strong>ê¸°ì¡´ ëª¨ë¸ ëŒ€ë¹„ ì ì€ ë¹„ìš©ìœ¼ë¡œ ë†’ì€ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ì˜€ë‹¤ëŠ” ê²ƒì´ í° íŠ¹ì§•ì´ë‹¤.</strong></p>
<p><img src="/image/PEGASUS-result3.PNG" width="600"><br>ë˜í•œ, ì‹¤ì œ ì‚¬ëŒì´ ë§Œë“  ìš”ì•½ë³¸ê³¼ PEGASUS-largeëª¨ë¸ì´ ë§Œë“  ìš”ì•½ë³¸ì€ ë¹„êµí•œ ê²°ê³¼ë¥¼ ë³´ë©´, Reddit TIFU ë°ì´í„°ì…‹ì„ ì œì™¸í•œ XSum, CNN/DailyMail ë°ì´í„°ì…‹ì—ì„œëŠ” PEGASUS-largeëª¨ë¸ì´ ë§Œë“  ìš”ì•½ë³¸ì´ ì‚¬ëŒì´ ë§Œë“  ìš”ì•½ë³¸ë³´ë‹¤ ë” ë†’ì€ ì„±ëŠ¥ì„ ë‚˜íƒ€ëƒˆë‹¤ëŠ” ê²ƒì´ íŠ¹ì§•ì´ë‹¤.</p>
<p><br></p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>ì •ë¦¬í•´ë³´ìë©´ í•´ë‹¹ ë…¼ë¬¸ì˜ í° íŠ¹ì§•ì´ë¼ê³  í•  ìˆ˜ ìˆëŠ” ì ì€,</p>
<blockquote>
<p>ì²«ì§¸, Abstractive summmarizationì´ë¼ëŠ” íŠ¹ì • taskë¥¼ ìœ„í•´ GSG(Gap-Sentence Generation)ë¼ëŠ” ìƒˆë¡œìš´ pre-trainingê¸°ë²•ì„ í†µí•´ ì ìš©í•œ ì <br>ë‘˜ì§¸, GSGì—ì„œ principal sentence selectionì„ ìœ„í•´ ë‹¤ì–‘í•œ ë°©ë²•ì„ ì ìš©í•œ ì <br>ì…‹ì§¸, ì ì€ ë¦¬ì†ŒìŠ¤ ë¹„ìš©(ex, 1000 examples)ë§Œìœ¼ë¡œë„ ëŒ€ë¶€ë¶„ì˜ ê²°ê³¼ì—ì„œ SOTAë¥¼ ë‹¬ì„±í•œ ì </p>
</blockquote>
<p>ì¸ ê²ƒ ê°™ë‹¤. ê·¸ëŸ°ë° ì—¬ê¸°ì„œ ì˜ë¬¸ì´ ë“¤ì—ˆë˜ ì ì€ ì‚¬ëŒì˜ ìš”ì•½ë³¸ê³¼ ì„±ëŠ¥ ë¹„êµë¥¼ í•˜ëŠ”ë° ìˆì–´ì„œ PEGASUS-largeëª¨ë¸ì´ ëŒ€ë¶€ë¶„ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì˜€ëŠ”ë°, ê³¼ì—° human evaluationì´ ê°ê´€ì ìœ¼ë¡œ ì´ë£¨ì–´ì¡ŒëŠ”ì§€ ì˜ë¬¸ì´ ë“¤ì—ˆë‹¤. ê° taskë§ˆë‹¤ 3ëª…ì˜ í‰ê°€ìì— ì˜í•´ 1-5ì ìœ¼ë¡œ í‰ê°€ë¥¼ í•˜ì˜€ë‹¤ê³  í•˜ëŠ”ë° ê³¼ì—° ì¼ë°˜í™” í•  ìˆ˜ ìˆì„ê¹Œ?</p>
<p>ì—¬í•˜íŠ¼, ìµœê·¼ text summarizationë¶„ì•¼ë¥¼ ê´€ì‹¬ìˆê²Œ ë³´ê³  ìˆì—ˆëŠ”ë°, summarization taskì— ìµœì í™”ëœ ëª¨ë¸ì´ ë‚˜ì™”ë‹¤ëŠ” ì ì—ì„œ í¥ë¯¸ê°€ ê°”ë˜ ë…¼ë¬¸ì´ì—ˆë‹¤.</p>
<hr>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://arxiv.org/pdf/1912.08777.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1912.08777.pdf</a></li>
<li><a href="https://www.youtube.com/watch?v=JhGmeQBbDdA" target="_blank" rel="noopener">https://www.youtube.com/watch?v=JhGmeQBbDdA</a></li>
</ul>

  <p><a class="classtest-link" href="/tags/gap-sentence-generator/">gap-sentence-generator</a>, <a class="classtest-link" href="/tags/mlm/">mlm</a>, <a class="classtest-link" href="/tags/nlp/">nlp</a>, <a class="classtest-link" href="/tags/summarization/">summarization</a>, <a class="classtest-link" href="/tags/transformer/">transformer</a> â€” Aug 1, 2020</p>
  
  <hr>
<section id="comments" class="mt-2 mb-3">

  <div id="disqus_thread">
    <a href="#" class="button button-primary" onclick="loadDisqus();return false;">View / Make Comments</a>
  </div>

  <script>
    var disqus_config = function() {
      this.page.url = 'https://jaehyeongan.github.io/2020/08/01/PEGASUS/index.html';
      this.page.identifier = '2020/08/01/PEGASUS/index.html';
      this.page.title = '[Paper Review] PEGASUS:Pre-training with Extracted Gap-sentences for Abstractive Summarization';
    };

    var is_disqus_loaded = false;

    function loadDisqus() {
      if (!is_disqus_loaded) {
        is_disqus_loaded = true;

        var d = document,
          s = d.createElement('script');
        s.src = 'https://jaehyeongan.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
      }
    }

  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</section>



          <div class="row mt-2">
  
    <div class="eight columns">
      <p id="madewith">Made with â¤ and
        <a class="footer-link icon" href="https://hexo.io" target="_blank" style="text-decoration: none;" rel="noreferrer" aria-label="Hexo.io">
        <svg class="hexo svg-hov" width="14" role="img" xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24"><title>Hexo.js</title><path d="M12 .007L1.57 6.056V18.05L12 23.995l10.43-6.049V5.952L12 .007zm4.798 17.105l-.939.521-.939-.521V12.94H9.08v4.172l-.94.521-.938-.521V6.89l.939-.521.939.521v4.172h5.84V6.89l.94-.521.938.521v10.222z"/></svg>
        </a>
        
        at <a href="https://en.wikipedia.org/wiki/Earth" target="_blank" rel="noreferrer">Earth</a>.</p>
        
    </div>

    <!-- Sepcial thanks to https://simpleicons.org/ for the icons -->
    <div class="four columns mb-3 posisi">
      
      <a class="ml-0 footer-link icon" href="https://github.com/jaehyeongAN" target="_blank" style="text-decoration: none" rel="noreferrer" aria-label="GitHub">
        <svg class="github svg-hov" width="18" role="img" viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg>
      </a>
      

      
      <a class="ml-0 footer-link icon" href="https://www.linkedin.com/in/jaehyeong-an-005603160/" target="_blank" style="text-decoration: none" rel="noreferrer" aria-label="LinkedIn">
        <svg class="linkedin svg-hov" width="18" role="img" viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>LinkedIn</title><path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"/></svg>
      </a>
      

      

      
      <a class="ml-0 footer-link icon" href="https://www.instagram.com/an_jh.ds/" target="_blank" style="text-decoration: none" rel="noreferrer" aria-label="Instagram">
        <svg class="instagram svg-hov" width="18" role="img" viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>Instagram</title><path d="M12 0C8.74 0 8.333.015 7.053.072 5.775.132 4.905.333 4.14.63c-.789.306-1.459.717-2.126 1.384S.935 3.35.63 4.14C.333 4.905.131 5.775.072 7.053.012 8.333 0 8.74 0 12s.015 3.667.072 4.947c.06 1.277.261 2.148.558 2.913.306.788.717 1.459 1.384 2.126.667.666 1.336 1.079 2.126 1.384.766.296 1.636.499 2.913.558C8.333 23.988 8.74 24 12 24s3.667-.015 4.947-.072c1.277-.06 2.148-.262 2.913-.558.788-.306 1.459-.718 2.126-1.384.666-.667 1.079-1.335 1.384-2.126.296-.765.499-1.636.558-2.913.06-1.28.072-1.687.072-4.947s-.015-3.667-.072-4.947c-.06-1.277-.262-2.149-.558-2.913-.306-.789-.718-1.459-1.384-2.126C21.319 1.347 20.651.935 19.86.63c-.765-.297-1.636-.499-2.913-.558C15.667.012 15.26 0 12 0zm0 2.16c3.203 0 3.585.016 4.85.071 1.17.055 1.805.249 2.227.415.562.217.96.477 1.382.896.419.42.679.819.896 1.381.164.422.36 1.057.413 2.227.057 1.266.07 1.646.07 4.85s-.015 3.585-.074 4.85c-.061 1.17-.256 1.805-.421 2.227-.224.562-.479.96-.899 1.382-.419.419-.824.679-1.38.896-.42.164-1.065.36-2.235.413-1.274.057-1.649.07-4.859.07-3.211 0-3.586-.015-4.859-.074-1.171-.061-1.816-.256-2.236-.421-.569-.224-.96-.479-1.379-.899-.421-.419-.69-.824-.9-1.38-.165-.42-.359-1.065-.42-2.235-.045-1.26-.061-1.649-.061-4.844 0-3.196.016-3.586.061-4.861.061-1.17.255-1.814.42-2.234.21-.57.479-.96.9-1.381.419-.419.81-.689 1.379-.898.42-.166 1.051-.361 2.221-.421 1.275-.045 1.65-.06 4.859-.06l.045.03zm0 3.678c-3.405 0-6.162 2.76-6.162 6.162 0 3.405 2.76 6.162 6.162 6.162 3.405 0 6.162-2.76 6.162-6.162 0-3.405-2.76-6.162-6.162-6.162zM12 16c-2.21 0-4-1.79-4-4s1.79-4 4-4 4 1.79 4 4-1.79 4-4 4zm7.846-10.405c0 .795-.646 1.44-1.44 1.44-.795 0-1.44-.646-1.44-1.44 0-.794.646-1.439 1.44-1.439.793-.001 1.44.645 1.44 1.439z"/></svg>
      </a>
      

    </div>
  
</div>

        </div>
      </div>

    </div>

  </div>
  <script src="/js/nanobar.min.js"></script>
  <script>
    var options = {
      classname: 'nanobar',
      id: 'myNanobar'
    };
    var nanobar = new Nanobar(options);
    nanobar.go(30);
    nanobar.go(76);
    nanobar.go(100);
  </script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>

</html>
