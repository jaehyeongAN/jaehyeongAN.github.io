{"meta":{"title":"jaehyeong's ds","subtitle":"for data scientist","description":null,"author":null,"url":"https://jaehyeongan.github.io","root":"/"},"pages":[{"title":"Call me ğŸ¤—","date":"2018-06-29T01:49:26.000Z","updated":"2020-12-05T15:13:27.000Z","comments":true,"path":"about/index.html","permalink":"https://jaehyeongan.github.io/about/index.html","excerpt":"","text":"e-mail : nonamed000000@gmail.comphone : 010 3566 3150 bachelorâ€™s degree at Ajou univ.masterâ€™s degree at Gachon univ."},{"title":"ê·¸ ë™ì•ˆ ê³ êµ°ë¶„íˆ¬í•˜ë©° ë„ì›€ì´ ë˜ì—ˆë˜ ê¸€ë“¤.","date":"2018-06-29T04:11:18.000Z","updated":"2020-10-28T15:14:34.000Z","comments":true,"path":"links/index.html","permalink":"https://jaehyeongan.github.io/links/index.html","excerpt":"","text":"# For Data ScienceGAN ì‰½ê²Œ ì”Œì—¬ì§„ GAN ìƒì„±ì  ì ëŒ€ ì‹ ê²½ë§(GANs)ì— ëŒ€í•œ ì´ˆë³´ììš© ê°€ì´ë“œ (GANs) AnoGAN in tensorflow AnoGANì„ ì´ìš©í•œ ì² ê°• ì†Œì¬ ê²°í•¨ ê²€ì¶œ AI RNN/LSTM Understanding LSTM Networks Long Short-Term Memory (LSTM) ì´í•´í•˜ê¸° RNN/LSTM ë…¼ë¬¸ë¦¬ë·° Kerasë¥¼ í†µí•œ LSTM êµ¬í˜„ Deep Learning for Time Series by Jason Brownlee Stateful LSTM in Keras How to connect LSTM layers in Keras, RepeatVector or return_sequence=True? ê¸°ìƒ ë‚ ì”¨ ì˜ˆì¸¡ Autoencoder ì˜¤í† ì¸ì½”ë”(Autoencoder) ê°œë… AutoEncoder - LSTM AutoEncoder LSTM Autoencoder for Extreme Rare Event Classification in Keras ConvNet CNN, Convolutional Neural Network ìš”ì•½ CNNì„ í™œìš©í•œ ì£¼ìš” Model - Modern CNN CNNì„ í™œìš©í•œ ì£¼ìš” Model - Image Detection Convolutional Feature Maps Class Activation Map(Learning Deep Features for Discriminative Localization) CAM - Class Activation Map Quality inspection in manufacturing using deep learning based computer vision Object Detection R-CNN, Fast R-CNN, Faster R-CNN, YOLO â€” Object Detection Algorithms R-CNNs Tutorial You Only Look Once - Paper Review Darkflowë¥¼ í™œìš©í•˜ì—¬ YOLO ëª¨ë¸ë¡œ ì´ë¯¸ì§€ ë””í…ì…˜ êµ¬í˜„ ìœˆë„ìš° í™˜ê²½ì—ì„œ YOLOë¡œ ì‹¤ì‹œê°„ ê°ì²´íƒì§€ ìë™ì°¨ ë²ˆí˜¸íŒ ì¸ì‹ (OCR) with YOLO v2 imgaugë¥¼ ì´ìš©í•˜ì—¬ ë°”ìš´ë”©ë°•ìŠ¤ ì •ë³´ë¥¼ í¬í•¨í•œ ì´ë¯¸ì§€ ì¦í­ì‹œí‚¤ê¸° OpenCV OpenCV-Python Tutorials NAMPë‹˜ì˜ OpenCV-Python Tutorials OpenCV ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ, ìœ¤ê³½ì— ê¸°ë°˜í•œ ìë™ì°¨ ë²ˆí˜¸íŒ ì˜ì—­ ì¶”ì¶œ(License Plates Recognition) Python+OpenCVë¡œ ìë™ì°¨ ë²ˆí˜¸íŒ ì˜ì—­ ì¶”ì¶œ ë° ì¸ì‹í•˜ê¸° Recommender System ì¿ íŒ¡ ì¶”ì²œ ì‹œìŠ¤í…œ 2ë…„ê°„ì˜ ë³€ì²œì‚¬ (ìƒí’ˆì¶”ì²œì—ì„œ ì‹¤ì‹œê°„ ê°œì¸í™”ë¡œ) ë‹¹ê·¼ë§ˆì¼“ ë”¥ëŸ¬ë‹ ê°œì¸í™” ì¶”ì²œ Text ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ì…ë¬¸ Word2Vecìœ¼ë¡œ ë¬¸ì¥ ë¶„ë¥˜í•˜ê¸° keras word embedding Boosting Model Baggingê³¼ Boosting ê·¸ë¦¬ê³  Stacking What is LightGBM, How to implement it? How to fine tune the parameters? LIGHTGBM ì£¼ìš” íŒŒë¼ë¯¸í„° ì •ë¦¬ # For DevelopmentLinux Ubuntu ìš°ë¶„íˆ¬ ë¦¬ëˆ…ìŠ¤ ë“€ì–¼ë¶€íŒ… ì„¤ì¹˜ë°©ë²• ì •ë¦¬ Install Docker on Ubuntu 18.04 How to install TensorFlow GPU on UBUNTU 18.04 ubuntu 18.04.1 theme ì„¤ì • Flask Flask doc í•œê¸€ ë²ˆì—­ ê°„ë‹¨í•œ Flask ì–´í”Œë¦¬ì¼€ì´ì…˜ ë§Œë“¤ê¸° íŒŒì´ì¬ Flaskë¡œ ê°„ë‹¨ ì›¹ì„œë²„ êµ¬ë™í•˜ê¸° AWS EC2ì—ì„œ í”Œë¼ìŠ¤í¬(Flask) ì›¹ ì„œë²„ êµ¬ë™ì‹œí‚¤ê¸° Django Django/ì¥ê³  MVT íŒ¨í„´ì´ë€ Deploy Machine Learning Models with Django Real-Time System ë£¨ë¹…ìŠ¤(RUBICS) â€“ kakaoì˜ ì‹¤ì‹œê°„ ì¶”ì²œ ì‹œìŠ¤í…œ ë°ì´íƒ€ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ì— ëŒ€í•œ ì´í•´ Apache Kafka ì†Œê°œ ë° ì •ë¦¬ Real Time Credit Card Fraud Detection with Apache Spark and Event Streaming Etc ëˆ„êµ¬ë‚˜ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆëŠ” Git ì…ë¬¸ ë²¡ì—”ë“œê°€ ì´ì •ë„ëŠ” í•´ì¤˜ì•¼ í•¨ ì‹œë¦¬ì¦ˆ"}],"posts":[{"title":"[Basic NLP_2] Transformer model","slug":"Transformer","date":"2021-02-07T14:45:19.000Z","updated":"2021-02-08T16:10:00.975Z","comments":true,"path":"2021/02/07/Transformer/","link":"","permalink":"https://jaehyeongan.github.io/2021/02/07/Transformer/","excerpt":"","text":"Introì§€ë‚œ í¬ìŠ¤íŠ¸ì¸ Sequence-to-Sequence with Attentionì—ì„œ sequence-to-sequence ëª¨ë¸ì˜ ê²½ìš° RNN ê³„ì—´ì˜ ìˆœí™˜ ì‹ ê²½ë§ì„ ì‚¬ìš©í•¨ìœ¼ë¡œ ì¸í•´ ì…ë ¥ ì‹œí€€ìŠ¤ê°€ ê¸¸ì–´ì§ˆ ìˆ˜ ë¡ í•˜ë‚˜ì˜ Context Vectorì— ëª¨ë“  ì •ë³´ë¥¼ ë‹´ê¸° ë¶€ì¡±í•˜ë‹¤ëŠ” í•œê³„ê°€ ìˆìŒì„ í™•ì¸í•˜ì˜€ë‹¤. ê·¸ë¡œ ì¸í•´ Attention mechanismì´ ì ìš©ë˜ì—ˆì§€ë§Œ ì´ ë˜í•œ ê²°êµ­ ë¬¸ì¥ì— ê°€ì¤‘ì¹˜ë§Œ ì¤„ ë¿ í•˜ë‚˜ì˜ Context Vectorì— ë¬¸ë§¥ ì •ë³´ë¥¼ ì••ì¶•í•œë‹¤ëŠ” ì ì—ì„œ ê°™ì€ ë¬¸ì œê°€ ìˆì—ˆê³ , ì´ëŸ¬í•œ ì‹œí€€ìŠ¤ ìˆœì„œë¥¼ ìœ ì§€í•˜ë©° í•™ìŠµí•˜ëŠ” RNNì˜ í•œê³„ê°€ ì§€ì ë˜ì—ˆë‹¤.ì´í›„ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ RNNì…€ì´ ì „ë¶€ ì œê±°ë˜ê³  Attentionê¸°ë²•ì„ ì¤‘ì ìœ¼ë¡œ í•™ìŠµí•˜ëŠ” ëª¨ë¸ì´ ë“±ì¥í•˜ì˜€ìœ¼ë‹ˆ ê·¸ê²ƒì´ ë°”ë¡œ íŠ¸ëœìŠ¤í¬ë¨¸(Transformer) ëª¨ë¸ì´ë‹¤. (ìš°ë¦¬ê°€ ì•„ëŠ” ê·¸ ì˜í™”ëŠ” ë’¤ì— 's'ê°€ ë¶™ëŠ”ë‹¤. Transformers...) TransformeríŠ¸ëœìŠ¤í¬ë¨¸(Transformer)ëª¨ë¸ì€ 2017ë…„ êµ¬ê¸€ì— ì˜í•´ ì†Œê°œëœ ë…¼ë¬¸ì¸ â€œAttention is all you needâ€ì—ì„œ ë“±ì¥í•œ ëª¨ë¸ì´ë‹¤. ê¸°ë³¸ì ìœ¼ë¡œ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì€ ì•ì„œ ì‚´í´ë³´ì•˜ë˜ Sequence-to-Sequenceì˜ ì¸ì½”ë”(Encoder), ë””ì½”ë”(Decoder)ì˜ êµ¬ì¡°ë¥¼ ê°€ì§€ê³  ìˆì§€ë§Œ RNNì…€ì„ ì‚¬ìš©í•˜ì§€ ì•Šê³  ë‹¨ìˆœíˆ ì–´í…ì…˜(Attention)êµ¬ì¡°ë§Œìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆë‹¤ëŠ” ê²ƒ, ê·¸ë¦¬ê³  ì´ëŸ¬í•œ ì¸ì½”ë”-ë””ì½”ë” êµ¬ì¡°ê°€ nê°œ(ë…¼ë¬¸ì—ì„œëŠ” 6ê°œ) ì¡´ì¬í•œë‹¤ëŠ” ê²ƒì´ í° íŠ¹ì§•ì´ë‹¤. ìœ„ ê·¸ë¦¼ì€ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì˜ ì „ì²´ì ì¸ ì•„í‚¤í…ì²˜ì´ë‹¤. í¬ê²Œ ì™¼ìª½ ë¶€ë¶„ì„ ì¸ì½”ë”(Encoder)ë¡œ êµ¬ë¶„í•˜ê³ , ì˜¤ë¥¸ìª½ì„ ë””ì½”ë”(Decoder)ë¡œ êµ¬ë¶„í•˜ë©° ì´ëŸ¬í•œ êµ¬ì¡°ê°€ nê°œ ì¡´ì¬í•œë‹¤. í•™ìŠµ ë°©ë²•ì€ seq2seqëª¨ë¸ê³¼ ê°™ì´ ì¸ì½”ë”ì—ì„œ ì…ë ¥ ì‹œí€€ìŠ¤ì˜ íŠ¹ì§•ì„ í•™ìŠµí•˜ê³  ì´ë¥¼ ë””ì½”ë”ì˜ ì…ë ¥ ë²¡í„°ë¡œ ë„˜ê²¨ì£¼ì–´ í•˜ë‚˜ì˜ í† í°ì”© ì¶œë ¥í•˜ê²Œ ëœë‹¤. í•˜ì§€ë§Œ seq2seqì™€ í¬ê²Œ ë‹¤ë¥¸ ì ì€ ì‹œí€€ìŠ¤ ìˆœì„œë¥¼ í•™ìŠµí•˜ê¸° ìœ„í•œ RNNì…€ì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ê²ƒì¸ë° ê·¸ê²ƒì„ ëŒ€ì²´í•˜ê¸° ìœ„í•´ Positinal Encodingì´ë¼ëŠ” ê¸°ë²•ì„ ì‚¬ìš©í•˜ì˜€ìœ¼ë©°, ê¸°ì¡´ Attentionê³¼ ë¹„ìŠ·í•˜ë©´ì„œë„ ì¡°ê¸ˆ ë‹¤ë¥¸ Self-Attentionê³¼ Multi-Head Attentionì´ë¼ëŠ” ê¸°ë²•ì„ ì ìš©í•˜ì˜€ë‹¤. ì•„ë˜ì—ì„œ ê° ë¶€ë¶„ì— ëŒ€í•´ ìì„¸íˆ ì‚´í´ë³´ë„ë¡ í•˜ì. Positional Encodingìœ„ì—ì„œ ì–¸ê¸‰í–ˆë‹¤ì‹œí”¼ RNNì…€ì„ í†µí•´ ìˆœì°¨ì ìœ¼ë¡œ ì‹œí€€ìŠ¤ë¥¼ ì…ë ¥ë°›ëŠ” seq2seqì™€ ë‹¬ë¦¬ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì€ ì…ë ¥ ì‹œí€€ìŠ¤ê°€ ìˆœì°¨ì ìœ¼ë¡œ ì…ë ¥ë˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ í•œë²ˆì— ë³‘ë ¬ì ìœ¼ë¡œ ì¸ì½”ë”ë¡œ ì…ë ¥ëœë‹¤. ì´ë ‡ê²Œ ëª¨ë“  ë‹¨ì–´ê°€ í•œë²ˆì— ì…ë ¥ë˜ë©´ ì…ë ¥ ë‹¨ì–´ë“¤ì˜ ìˆœì„œì •ë³´ë¥¼ ë³´ì¡´í•  ìˆ˜ ì—†ê²Œ ë˜ëŠ”ë°, ì´ëŸ° ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ë°©ë²•ì´ ë°”ë¡œ Positional Encodingì´ë‹¤. ìˆœì„œì •ë³´ë¥¼ ìœ ì§€í•˜ê¸° ìœ„í•œ Positional Encodingí•¨ìˆ˜ì˜ ìˆ˜ì‹ì€ ì•„ë˜ì™€ ê°™ë‹¤. ìœ„ ìˆ˜ì‹ì˜ â€˜posâ€™ëŠ” ì„ë² ë”© ë²¡í„°ì˜ ìœ„ì¹˜ë¥¼ ë‚˜íƒ€ë‚´ê³  â€˜iâ€™ëŠ” ì¸ë±ìŠ¤ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.ìœ„ ìˆ˜ì‹ì„ ì‚´í´ë³´ë©´ ì¸ë±ìŠ¤ê°€ ì§ìˆ˜(pos, 2i)ì¸ ê²½ìš°ëŠ” ì‚¬ì¸(sin)í•¨ìˆ˜ë¥¼ ì ìš©í•˜ê³ , í™€ìˆ˜(pos, 2i+1)ì¸ ê²½ìš°ëŠ” ì½”ì‚¬ì¸(cos)í•¨ìˆ˜ë¥¼ ì ìš©í•˜ì—¬ ìˆœì„œ ì •ë³´ë¥¼ ë°˜ì˜í•´ì£¼ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ì´ë ‡ê²Œ ê³„ì‚°ëœ í¬ì§€ì…˜ ì„ë² ë”© í–‰ë ¬(Positional Embedding Matrix)ì€ ì…ë ¥ ì›ë³¸ ë¬¸ì¥ì˜ ì„ë² ë”© í–‰ë ¬(Input Embedding Matrix)ì— ë‹¨ìˆœ ë§ì…ˆì—°ì‚°ì„ í†µí•´ ë”í•´ì ¸ ì¸ì½”ë”ì˜ inputìœ¼ë¡œ ì‚¬ìš©ë˜ê²Œ ëœë‹¤. Self-Attention &amp; Multi-Head Attention Masked Multi-Head Attention Residual Connection &amp; Layer Nomalization Position-Wise Feed-Forward Networks ì‘ì„± ä¸­.. Conclusion Reference Attention Is All You Need ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ì…ë¬¸ - íŠ¸ëœìŠ¤í¬ë¨¸ What Exactly Is Happening Inside the Transformer","categories":[],"tags":[{"name":"nlp","slug":"nlp","permalink":"https://jaehyeongan.github.io/tags/nlp/"},{"name":"transformer","slug":"transformer","permalink":"https://jaehyeongan.github.io/tags/transformer/"},{"name":"attention","slug":"attention","permalink":"https://jaehyeongan.github.io/tags/attention/"},{"name":"self-attention","slug":"self-attention","permalink":"https://jaehyeongan.github.io/tags/self-attention/"},{"name":"positional-encoding","slug":"positional-encoding","permalink":"https://jaehyeongan.github.io/tags/positional-encoding/"}]},{"title":"[Basic NLP_1] Sequence-to-Sequence with Attention","slug":"Sequence-to-Sequence-with-Attention","date":"2021-02-06T12:46:46.000Z","updated":"2021-02-07T14:59:23.867Z","comments":true,"path":"2021/02/06/Sequence-to-Sequence-with-Attention/","link":"","permalink":"https://jaehyeongan.github.io/2021/02/06/Sequence-to-Sequence-with-Attention/","excerpt":"","text":"Introìµœê·¼ ëª‡ ë…„ê°„ Transformer ëª¨ë¸ì˜ ë“±ì¥ ì´í›„ BERT, GPT, RoBERTa, XLNet, ELECTRA, BART ë“±ê³¼ ê°™ì€ ì–¸ì–´ ëª¨ë¸(Language Model)ì´ ë§¤í•´ ìƒˆë¡œìš´ SOTAë¥¼ ë‹¬ì„±í•˜ë©° ë“±ì¥í•˜ê³  ìˆë‹¤.íŠ¹íˆ ì–¸ì–´ëª¨ë¸ì˜ ê²½ìš° self-supervised learningìœ¼ë¡œ ì˜ì–´ ë¿ë§Œ ì•„ë‹ˆë¼ ìµœê·¼ ë‹¤ì–‘í•œ ì–¸ì–´ë¡œ í•™ìŠµëœ ëª¨ë¸ì´ ë“±ì¥í•˜ê³  ìˆê³ , ê·¸ ë•ì— ë‹¤ì–‘í•œ ìì—°ì–´ ì²˜ë¦¬ íƒœìŠ¤í¬ë“¤ì—ì„œ fine-tuningì‹œ ë°ì´í„°ê°€ ë§ì§€ ì•Šë”ë¼ë„ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ê³  ìˆë‹¤. ì´ëŸ¬í•œ íŠ¸ë Œë“œë¥¼ ì´ëˆ ê²ƒì€ Transformerì˜ ì—­í• ì´ í¬ì§€ë§Œ ê·¸ ì „ì— Transformerì˜ ì „ì‹ ì¸ Sequence-to-Sequnceëª¨ë¸ê³¼ Attention mechanismì— ëŒ€í•´ ë¨¼ì € ê°„ë‹¨í•˜ê²Œ ì‚´í´ë³´ë ¤ê³  í•œë‹¤. Sequnece-to-SequenceSequence-to-Sequence(ì´í•˜ seq2seq)ëª¨ë¸ì€ 2014ë…„ êµ¬ê¸€ì— ì˜í•´ ì œì•ˆëœ ëª¨ë¸ë¡œì„œ ì´ë¦„ ê·¸ëŒ€ë¡œ ì‹œí€€ìŠ¤ í˜•íƒœì˜ ì…ë ¥ê°’ì„ ë°›ì•„ ì‹œí€€ìŠ¤ í˜•íƒœì˜ ì¶œë ¥ê°’ì„ ë§Œë“œëŠ” ëª¨ë¸ì´ë©°, ê¸°ì¡´ DNNëª¨ë¸ì´ ì…ë ¥ê³¼ ì¶œë ¥ ë²¡í„°ì˜ ì°¨ì›ì´ ê³ ì •ë˜ì–´ìˆë‹¤ëŠ” í•œê³„ë¥¼ ê·¹ë³µí•˜ì—¬ ê°€ë³€ ê¸¸ì´ì˜ ì¶œë ¥ì„ ê°€ëŠ¥í•˜ê²Œ í•œ ëª¨ë¸ì´ë‹¤.seq2seqëª¨ë¸ì€ ê¸°ë³¸ì ìœ¼ë¡œ RNN ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ë©°, í¬ê²Œ ì¸ì½”ë”(Encoder)ì™€ ë””ì½”ë”(Decoder)ë¡œ êµ¬ë¶„ëœë‹¤. 1. EncoderEncoderì—ì„œëŠ” ê° ì‹œí€€ìŠ¤ë§ˆë‹¤ embedding vectorë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ë§ˆì§€ë§‰ê¹Œì§€ ìˆœì°¨ì ìœ¼ë¡œ weightì„ ì—…ë°ì´íŠ¸í•œë‹¤.(RNN í•™ìŠµ ë°©ë²•ê³¼ ë™ì¼) ê·¸ë ‡ê²Œ ë˜ë©´ ë§ˆì§€ë§‰ ì‹œí€€ìŠ¤ì˜ hidden statesëŠ” ì´ì „ ì…ë ¥ ì‹œí€€ìŠ¤ë“¤ì„ ì •ë³´ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ë°˜ì˜í•˜ì—¬ ì—…ë°ì´íŠ¸ ëœ ìƒíƒœì´ë©°, ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ì „ë°˜ì ì¸ ë¬¸ë§¥ì„ ë°˜ì˜í•˜ê³  ìˆë‹¤ê³  í•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ ë²¡í„°(Context vector)ë¼ê³  ë¶€ë¥¸ë‹¤. 2. DecoderDecoderëŠ” ìš°ì„  Encoderì˜ ì „ì²´ì ì¸ ë¬¸ë§¥ì´ í•™ìŠµëœ context vectorì™€ &lt; SOS &gt; (Start of Sentence) í† í°ì„ ì²« ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ì¶œë ¥ í† í°ì„ ì˜ˆì¸¡í•œë‹¤. Decoderì˜ í˜„ì¬ ì‹œì (t)ì˜ ì¶œë ¥ê²°ê³¼ëŠ” ì´ì „ ì‹œì (t1,â€¦,t-1) ì¶œë ¥ ê²°ê³¼ì˜ ì¡°ê±´ë¶€ í™•ë¥ ë¡œì„œ, ì´ì „ ì‹œì ì˜ ê²°ê³¼ì— ë”°ë¼ í˜„ì¬ ì‹œì ì˜ ì¶œë ¥ ê²°ê°€ ì˜í–¥ì„ ë°›ê²Œ ë˜ëŠ” êµ¬ì¡°ì´ë©°, ì´ë ‡ê²Œ ì˜ˆì¸¡ëœ ì¶œë ¥ê°’ì€ ë‹¤ì‹œ ë‹¤ìŒ ì‹œí€€ìŠ¤ì˜ ì˜ˆì¸¡ì„ ìœ„í•´ ì…ë ¥ê°’ìœ¼ë¡œ ì‚¬ìš©ë˜ê³  ì´ëŸ¬í•œ ê³¼ì •ì´ &lt; EOS &gt; (End of Sentence) í† í°ì´ ë“±ì¥í•  ë•Œ ê¹Œì§€ ë°˜ë³µëœë‹¤. Example (Machine Translation)seq2seqë¥¼ ê¸°ê³„ë²ˆì—­ì— ì ìš©í•  ì‹œ ìœ„ì™€ ê°™ì´ í”„ë‘ìŠ¤ì–´ì— í•´ë‹¹í•˜ëŠ” ì…ë ¥ ì‹œí€€ìŠ¤ë“¤ì´ ìˆœì°¨ì ìœ¼ë¡œ Encoderë¡œ ì…ë ¥ë˜ì–´ ë§ˆì§€ë§‰ ì‹œí€€ìŠ¤ê¹Œì§€ weightì„ ì—…ë°ì´íŠ¸í•˜ê³  ê·¸ë ‡ê²Œ ì—…ë°ì´íŠ¸ ëœ ë§ˆì§€ë§‰ ì…ë ¥ ì‹œí€€ìŠ¤ ì¦‰, Context Vectorë¥¼ Decoderì˜ ì…ë ¥ìœ¼ë¡œ ë„˜ê²¨ì£¼ì–´ ì˜ì–´ë¡œ ì¶œë ¥í•˜ê²Œ ëœë‹¤. seq2seqì˜ í•œê³„seq2seqëŠ” ì¶œë ¥ ì‹œí€€ìŠ¤ì˜ ê°€ë³€ ê¸¸ì´ ì¶œë ¥ì´ ê°€ëŠ¥í•´ì§ìœ¼ë¡œì¨ ì–¸ì–´ ëª¨ë¸ì˜ ë°œì „ì„ ê°€ì ¸ì™”ì§€ë§Œ ì…ë ¥ ì‹œí€€ìŠ¤ê°€ ê¸¸ì–´ì§ˆ ìˆ˜ ë¡ ì´ˆê¸° ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ì •ë³´ë¥¼ ìƒê²Œ ë˜ëŠ” gradient vanishing ë¬¸ì œê°€ ì œê¸°ë˜ì—ˆë‹¤. ì•„ë¬´ë˜ë„ í•˜ë‚˜ì˜ context vectorì— ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ëª¨ë“  ì •ë³´ë¥¼ ë‹´ë‹¤ë³´ë‹ˆ ì „ì²´ ë¬¸ë§¥ ì •ë³´ê°€ í¬ë¯¸í•´ì§ˆ ìˆ˜ ë°–ì— ì—†ê³  ì´ëŠ” RNN ê³„ì—´ì˜ ëª¨ë¸(RNN, LSTM, GRU ë“±)ì—ì„œ ê³ ì§ˆì ìœ¼ë¡œ ë°œìƒí•˜ëŠ” ë¬¸ì œì´ë‹¤. Attention MechanismAttention mechanismì€ ìœ„ì—ì„œ ì–¸ê¸‰í•œ seq2seqì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ ì œì•ˆëœ ê°œë…ì´ë‹¤. Attentionì˜ ê¸°ë³¸ì ì¸ ì•„ì´ë””ì–´ëŠ” Decoderì—ì„œ ì¶œë ¥ í† í° ì˜ˆì¸¡ ì‹œ ë§¤ ì‹œì (time step)ë§ˆë‹¤ ì…ë ¥ ì‹œí€€ìŠ¤ì˜ í† í°ì„ ì°¸ì¡°í•˜ì—¬ ì—°ê´€ì„±ì´ ë†’ì€ í† í°ì— ê°€ì¤‘ì¹˜ë¥¼ ë†’ì—¬ í•™ìŠµí•œë‹¤ëŠ” ê²ƒì´ë‹¤. 1. Query, Key, ValueAttention ê³„ì‚°ì€ Decoder ì¶œë ¥ í† í° ì˜ˆì¸¡ ì‹œ ìˆ˜í–‰ë˜ë©° ì•„ë˜ì™€ ê°™ì´ Query, Key, Valueë¼ëŠ” ê°œë…ì´ ì‚¬ìš©ëœë‹¤. Q(Query) : t ì‹œì ì˜ decoderì…€ì˜ hidden statesK(Key) : ëª¨ë“  ì‹œì ì˜ encoderì…€ì˜ hidden statesV(Value) : ëª¨ë“  ì‹œì ì˜ encoderì…€ì˜ hidden states 2. Attention ScoreAttention Scoreë€ Decoderì—ì„œ ì¶œë ¥ í† í° ì˜ˆì¸¡ ì‹œ Encoderì˜ ëª¨ë“  ì‹œí€€ìŠ¤ ì •ë³´ë¥¼ ì°¸ì¡°(attention)í•˜ì—¬ ê°ê°ì˜ ì‹œí€€ìŠ¤ê°€ ì–¼ë§ˆë‚˜ ì¶œë ¥ í† í°ê³¼ ìœ ì‚¬í•œì§€ë¥¼ íŒë‹¨í•œ ìœ ì‚¬ë„ ê°’ì´ë‹¤.ì´ ê³¼ì •ì—ì„œ Decoderì˜ í˜„ì¬ tì‹œì ì€ Queryê°€ ë˜ê³ , ì°¸ì¡°í•˜ê³ ì í•˜ëŠ” Encoderì˜ ëª¨ë“  hidden statesëŠ” Keyê°€ ëœë‹¤. ì´ë•Œ QueryëŠ” ì „ì¹˜(transpose) í›„ ëª¨ë“  keyì— ëŒ€í•´ ê°ê° ë‚´ì (dot-product)ì—°ì‚°ì„ ìˆ˜í–‰í•˜ì—¬ Encoderì˜ Key ê°¯ìˆ˜ë§Œí¼ì˜ Attention scoreë¥¼ ê³„ì‚°í•œë‹¤. 3. Attention Distributionì…ë ¥ ì‹œí€€ìŠ¤ ê°¯ìˆ˜ë§Œí¼ ë‚˜ì˜¨ Attention Score ë¦¬ìŠ¤íŠ¸ì— Softmax í•¨ìˆ˜ë¥¼ ì ìš©í•œë‹¤. Softmaxë¥¼ ì ìš©í•˜ê²Œ ë˜ë©´ í•©ì´ 1ì´ë˜ëŠ” í™•ë¥ ë¶„í¬ê°€ ë˜ëŠ”ë° ì—¬ê¸°ì„œ ê°ê°ì˜ ê°’ë“¤ì„ Attention weightë¼ê³  í•œë‹¤. 4. Attention Valueìœ„ì—ì„œ êµ¬í•œ Attention weightì„ ë‹¤ì‹œ ê°ê°ì˜ Encoderì˜ hidden stateì™€ ê³±ì…ˆì—°ì‚°ì„ í•˜ê³ , ì´í›„ ëª¨ë“  ê°’ë“¤ì„ ë”í•´ì£¼ëŠ” ê°€ì¤‘í•©(weighted sum)ì„ í•˜ì—¬ ìµœì¢… Attention Value(í˜¹ì€ Context Value)ë¥¼ êµ¬í•˜ì—¬ ì´ë¥¼ Decoderì˜ í˜„ì¬ tì‹œì ì˜ ì…ë ¥ê°’ìœ¼ë¡œ ì‚¬ìš©í•œë‹¤. ìœ„ì˜ ê³¼ì •ì„ ê±°ì³ ë‚˜ì˜¨ ìµœì¢… Attention valueëŠ” Decoderì˜ ì˜ˆì¸¡í•˜ë ¤ëŠ” t ì‹œì ì˜ ì…ë ¥ê°’ìœ¼ë¡œ ì‚¬ìš©ë˜ê³  ë§¤ ì‹œì  ì˜ˆì¸¡ ì‹œ ë§ˆë‹¤ ìœ„ì™€ ê°™ì€ ê³¼ì •ì´ ë°˜ë³µëœë‹¤. ì•„ë˜ëŠ” Attention ê³¼ì •ì„ ì• ë‹ˆë©”ì´ì…˜ìœ¼ë¡œ í‘œí˜„í•œ ê²ƒì´ë‹¤. Conclusionì‚¬ì‹¤ TransformerëŠ” Sequence-to-Sequenceì™€ ê°™ì€ Encoder-Decoder êµ¬ì¡°ë¥¼ ì—¬ëŸ¬ê°œ ì‚¬ìš©í•œ ê²ƒì´ê³ , Transformerì—ì„œ ì‚¬ìš©ë˜ëŠ” self-attention ë° multi-head attention ë˜í•œ ê¸°ì¡´ Attention mechanismì„ ì‘ìš©í•œ ê²ƒì´ê¸° ë•Œë¬¸ì— Sequence-to-Sequenceëª¨ë¸ê³¼ Attention ê°œë…ë§Œ ì•Œì•„ë„ Transformer ì•„í‚¤í…ì²˜ë¥¼ ì´í•´í•˜ëŠ”ë° ì–´ë µì§€ ì•Šì„ ê²ƒì´ë‹¤. ë˜í•œ ì´í›„ ë“±ì¥í•œ ëª¨ë¸ë“¤ë„ ëŒ€ë¶€ë¶„ ì´ì™€ ê°™ì€ êµ¬ì¡°ë¥¼ ì‘ìš©í•œ ëª¨ë¸ì´ë¼ê³  í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— í™•ì‹¤íˆ ì´í•´í•˜ê³  ë„˜ì–´ê°€ëŠ” ê²ƒì´ ì¢‹ì„ ë“¯ í•˜ë‹¤. Tensorflow, Pytorch ê³µì‹ docì—ì„œ seq-to-seq with attentionëª¨ë¸ êµ¬í˜„ tutorialì´ ì¤€ë¹„ë˜ì–´ ìˆìœ¼ë‹ˆ ì°¸ê³ ! NLP FROM SCRATCH: TRANSLATION WITH A SEQUENCE TO SEQUENCE NETWORK AND ATTENTION ì–´í…ì…˜ì„ ì‚¬ìš©í•œ ì¸ê³µ ì‹ ê²½ë§ ê¸°ê³„ ë²ˆì—­ Reference Sequence to Sequence Learning with Neural Networks Neural Machine Translation by Jointly Learning to Align and Translate https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/","categories":[],"tags":[{"name":"nlp","slug":"nlp","permalink":"https://jaehyeongan.github.io/tags/nlp/"},{"name":"transformer","slug":"transformer","permalink":"https://jaehyeongan.github.io/tags/transformer/"},{"name":"attention","slug":"attention","permalink":"https://jaehyeongan.github.io/tags/attention/"},{"name":"seq2seq","slug":"seq2seq","permalink":"https://jaehyeongan.github.io/tags/seq2seq/"},{"name":"context","slug":"context","permalink":"https://jaehyeongan.github.io/tags/context/"}]},{"title":"[Paper Review] PEGASUS:Pre-training with Extracted Gap-sentences for Abstractive Summarization","slug":"PEGASUS","date":"2020-08-01T11:05:03.000Z","updated":"2021-02-07T14:56:06.987Z","comments":true,"path":"2020/08/01/PEGASUS/","link":"","permalink":"https://jaehyeongan.github.io/2020/08/01/PEGASUS/","excerpt":"","text":"Introìµœê·¼ NLPì˜ downstream tasks ì¤‘ í•˜ë‚˜ì¸ Summarizationë¶„ì•¼ì— â€œPEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarizationâ€ì´ë¼ëŠ” ìƒˆë¡œìš´ ë…¼ë¬¸(ë©‹ì§„ ì´ë¦„ì´ë‹¤..)ì´ ë“±ì¥í•˜ì—¬ ê°„ëµí•˜ê²Œ ì†Œê°œí•´ë³´ë ¤ê³  í•œë‹¤. What is Text Summarization?Text Summarizationì€ ìì—°ì–´ ì²˜ë¦¬ ë¶„ì•¼ì˜ ì—¬ëŸ¬ ê°œì˜ Downstram tasksì¤‘ í•˜ë‚˜ì´ë‹¤.ì´ë¦„ì—ì„œë¶€í„° ì‰½ê²Œ ì•Œ ìˆ˜ ìˆë“¯ì´ Text Summarizationì€ ë¬¸ì„œë¥¼ ìš”ì•½í•˜ëŠ” ê¸°ìˆ ì„ ì˜ë¯¸í•œë‹¤. Text Summarizationì€ í¬ê²Œ ì•„ë˜ì™€ ê°™ì´ ë‘ ê°€ì§€ë¡œ ë¶„ë¥˜ê°€ ëœë‹¤. 1. Extractive Summarization2. Abstractive Summarization ìœ„ ë‘ ë°©ì‹ì€ ìš”ì•½(summarization)ì„ í•œë‹¤ëŠ” ì¸¡ë©´ì—ì„œëŠ” ë™ì¼í•˜ë‚˜, ê·¸ ë°©ë²•ì— ì°¨ì´ê°€ ìˆë‹¤.ìœ„ ì˜ˆì‹œì™€ ê°™ì´ ExtractiveëŠ” ì›ë¬¸ í…ìŠ¤íŠ¸ë¡œë¶€í„° ì£¼ìš” Sentenceë¥¼ ì›ë¬¸ ê·¸ëŒ€ë¡œ ì¶”ì¶œí•´ë‚´ëŠ” ë°©ì‹ì´ë¼ë©´, AbstractiveëŠ” ìš°ë¦¬ê°€ ì›ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ë³´ê³  ìƒê°ê³¼ ëŠë‚Œì„ í•œ ì¤„ ìš”ì•½í•˜ë“¯ì´ í‘œí˜„í•˜ëŠ” ë°©ì‹ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤. Extractive Summarizationì—ì„œ ê°€ì¥ ë§ì´ ì•Œë ¤ì§„ ì•Œê³ ë¦¬ì¦˜ì€ ì•„ë¬´ë˜ë„ Text-Rankì¼ ê²ƒì´ë‹¤. ì´ˆê¸° êµ¬ê¸€ì˜ ê²€ìƒ‰ì—”ì§„ë­í‚¹ ì•Œê³ ë¦¬ì¦˜ì¸ Page-Rankë¥¼ Textì— ì ìš©í•œ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ, ì ì€ ì—°ì‚°ëŸ‰ìœ¼ë¡œë„ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ê³  ìˆë‹¤. Text-Rankì•Œê³ ë¦¬ì¦˜ì€ Document ë‚´ì—ì„œ Term-Frequencyê°€ ë†’ê³ , Co-occurenceê°€ ë†’ì€ ë‹¨ì–´ë¥¼ keywordë¡œ íŒë‹¨í•˜ë©°, ê·¸ëŸ¬í•œ keywordë¥¼ ë§ì´ ê°–ëŠ” Sentenceë¥¼ Key-Sentenceì¼ ê²ƒì´ë¼ ê°€ì •í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤.Text-Rankì˜ ìì„¸í•œ ì„¤ëª…ì€ í•´ë‹¹ ë§í¬(TextRank ë¥¼ ì´ìš©í•œ í‚¤ì›Œë“œ ì¶”ì¶œê³¼ í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ) ì°¸ì¡°í•˜ë©´ ì¢‹ì„ ê²ƒ ê°™ë‹¤. Extractive Summarization vs Abstractive Summarizationë‘˜ ì¤‘ì— ìµœê·¼ ê°€ì¥ í™œë°œíˆ ì—°êµ¬ë˜ëŠ” ë¶„ì•¼ëŠ” ì•„ë¬´ë˜ë„ Abstractive Summarizationì´ë‹¤.Abstractiveë°©ì‹ì´ Extractiveë°©ì‹ë³´ë‹¤ í›¨ì”¬ ì–´ë ¤ìš´ ë‚œì´ë„ì˜ taskì¼ ë¿ë§Œ ì•„ë‹ˆë¼ ì›ë¬¸ì„ ê·¸ëŒ€ë¡œ ì¶”ì¶œí•´ë‚´ëŠ” ê²ƒì´ ì•„ë‹Œ ë‹¤ì–‘í•œ í‘œí˜„ë°©ì‹ìœ¼ë¡œ Generateí•˜ê¸° ë•Œë¬¸ì— í›¨ì”¬ ë” ë‹¤ì–‘í•œ ë¶„ì•¼ì— ì‚¬ìš©ë  ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤.ìµœê·¼ ëª‡ ë…„ ì‚¬ì´ Seqence-to-Sequence, Attention mechanism, Transformer ë“±ê³¼ ê°™ì€ ì•„í‚¤í…ì²˜ê°€ ë“±ì¥í•˜ê³  Bertì™€ ê°™ì€ ëŒ€ëŸ‰ì˜ corpusë¡œ í•™ìŠµëœ pre-training ëª¨ë¸ì´ ë“±ì¥í•˜ë©° ì´ëŸ¬í•œ generatorëª¨ë¸ì˜ ì„±ëŠ¥ë„ ë‚˜ë‚ ì´ í–¥ìƒë˜ëŠ” ì¶”ì„¸ì´ë‹¤.ì´ì œ ì•„ë˜ì—ì„œ ê°€ì¥ ìµœê·¼ Abstractive Summarizaion ë…¼ë¬¸ìœ¼ë¡œ ë“±ì¥í•œ PEGASUSì— ëŒ€í•´ ì•Œì•„ë³´ì. PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive SummarizationAbstractìµœê·¼ ëŒ€ëŸ‰ì˜ text corporaë¡œ self-supervisedëœ pre-training Transfomers ëª¨ë¸ë“¤ì´ text summarizationì„ í¬í•¨í•œ fine-tuning downstream NLP taskì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ê³  ìˆë‹¤. í•˜ì§€ë§Œ, Abstractive Text Summarizationì˜ ëª©ì ì— ë§ê²Œ pre-trainingëœ ëª¨ë¸ì€ ì°¾ì•„ë³´ê¸° í˜ë“¤ê³ , ë”ìš±ì´ ë‹¤ì–‘í•œ domainì„ ì»¤ë²„í• ë§Œ í•œ ì²´ê³„ì ì¸ í‰ê°€ ë°©ë²•ë„ ë¶€ì¡±í•œ ìƒí™©ì´ë‹¤. ë”°ë¼ì„œ, í•´ë‹¹ ë…¼ë¬¸ì—ì„œëŠ” ëŒ€ëŸ‰ì˜ text coporaë¡œ self-supervisedëœ encoder-decoder ê¸°ë°˜ì˜ pre-training Transformer ëª¨ë¸ì¸ PEGASUSë¥¼ ì†Œê°œí•œë‹¤. PEGASUSì˜ ì£¼ìš” íŠ¹ì§•ì€ GSG(Gap sentence generation)ì„ ì‚¬ìš©í•œë‹¤ëŠ” ê²ƒì¸ë°, ê°„ë‹¨íˆ ì„¤ëª…í•˜ë©´ MLMë°©ì‹ì—ì„œëŠ” token ë‹¨ìœ„ë¡œ maskingí•˜ì—¬ masked tokenì„ ì˜ˆì¸¡í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµí–ˆë˜ ê²ƒê³¼ ìœ ì‚¬í•˜ê²Œ, GSGëŠ” token ë‹¨ìœ„ê°€ ì•„ë‹Œ Importance Sentence ë‹¨ìœ„ë¡œ maskingì„ í•˜ì—¬ í•™ìŠµì„ ìˆ˜í–‰í•œë‹¤. ì—¬ê¸°ì„œ ë§í•˜ëŠ” Importance Sentenceë€ document ë‚´ì—ì„œ ë‹¤ë¥¸ ë¬¸ì¥ì— ë¹„í•´ ì „ì²´ì ì¸ contextë¥¼ ì˜ ì„¤ëª…í•  ìˆ˜ ìˆëŠ” ë¬¸ì¥ì„ ë§í•œë‹¤. PEGASUS ëª¨ë¸ì€ 12ê°œì˜ downstream summarization tasksë¡œë¶€í„° ROUGE scoreë¥¼ ê¸°ë°˜ìœ¼ë¡œ SOTAë¥¼ ë‹¬ì„±í•˜ì˜€ê³ , ê·¸ ì¤‘ 6ê°œì˜ ë°ì´í„° ì…‹ì—ì„œ ì˜¤ì§ 1,000ê°œì˜ examplesë§Œìœ¼ë¡œë„ SOTAë¥¼ ë‹¬ì„±í•  ë§Œí¼ ì ì€ ë¦¬ì†ŒìŠ¤ ë¹„ìš©ìœ¼ë¡œ ë†€ë¼ìš´ ì„±ëŠ¥ì„ ë‚˜íƒ€ë‚´ì—ˆë‹¤. The Basic architecture of PEGASUS PEGASUSëŠ” ê¸°ë³¸ì ìœ¼ë¡œ encoder-decoderê¸°ë°˜ì˜ Transformerêµ¬ì¡°ë¥¼ í•˜ê³  ìˆìœ¼ë©°, ê¸°ì¡´ MLM(Masked Language Model)ê³¼ ìœ ì‚¬í•˜ê²Œ Input textì˜ ì¼ë¶€ë¥¼ maskingí•˜ì—¬ Encoderì˜ inputìœ¼ë¡œ ë³´ë‚´ê²Œ ëœë‹¤. í•˜ì§€ë§Œ ê¸°ì¡´ MLMê³¼ ë‹¤ë¥¸ ì ì€ ë°”ë¡œ Sentence ìì²´ë¥¼ maskingí•œë‹¤ëŠ” ì ì´ë‹¤.ê¸°ì¡´ MLM ëª¨ë¸ë“¤ì€ token ë‹¨ìœ„ë¡œ maskingí•˜ì—¬ masked tokenì„ ì˜ˆì¸¡í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµì„ ì§„í–‰í•˜ì˜€ì§€ë§Œ, PEGASUSëŠ” Input Documentë¡œë¶€í„° Sentence ë‹¨ìœ„ë¡œ Maskingì„ í•œ í›„ ë‚¨ì€ Sentenceë¥¼ ê¸°ë°˜ìœ¼ë¡œ masked sentenceë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµëœë‹¤. ë…¼ë¬¸ì—ì„œëŠ” ì´ëŸ¬í•œ ë°©ì‹ì„ Gap-Sentences-Generation(GSG)ë¼ê³  ë§í•˜ê³  ìˆë‹¤. Gap Sentences Generation (GSG)í•´ë‹¹ sectionì—ì„œëŠ” ìƒˆë¡œìš´ pre-training ë°©ì‹ì¸ GSGë¥¼ ì†Œê°œí•˜ê³ , ê¸°ì¡´ BERT masked-language modelê³¼ ë¹„êµë¥¼ ìˆ˜í–‰í•œë‹¤. í•´ë‹¹ ë…¼ë¬¸ì—ì„œ ê°•ì¡°í•˜ëŠ” ê²ƒ ì¤‘ í•˜ë‚˜ëŠ”, ì¢‹ì€ ì„±ëŠ¥ì„ ì–»ê¸° ìœ„í•´ì„œëŠ” ì ìš©í•˜ê³ ì í•˜ëŠ” downstream taskì˜ ëª©ì ì— ë§ëŠ” pre-training ëª¨ë¸ì„ ì‚¬ìš©í•˜ê³  ì´ë¥¼ fine-tuning í•˜ë¼ëŠ” ê²ƒì´ë‹¤. ì¦‰, ë‰´ìŠ¤ë¥¼ ìš”ì•½í•˜ê¸° ìœ„í•œ ë°ì´í„°ë¡œ í•™ìŠµëœ ëª¨ë¸ì€ ë‰´ìŠ¤ ìš”ì•½ì— ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ê² ì§€ë§Œ, ì˜í™” ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ìš”ì•½í•˜ëŠ”ë°ëŠ” ì „í˜€ ë§ì§€ ì•Šì„ ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤. ë…¼ë¬¸ì˜ ì‹¤í—˜ë¶€ë¶„ì—ì„œ ë” ì†Œê°œê°€ ë˜ëŠ”ë° Newsê´€ë ¨ ë°ì´í„°ì…‹ìœ¼ë¡œ í•™ìŠµí•œ ëª¨ë¸ì€ Non-news taskì—ì„œëŠ” ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ì§€ ëª»í–ˆë‹¤. Summarizationì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ì„œëŠ” input documentì™€ ê·¸ì— ë§ëŠ” summary textê°€ ìŒìœ¼ë¡œ í™œìš©ë˜ì–´ì•¼ í•œë‹¤. í•˜ì§€ë§Œ ë‹¨ìˆœíˆ extractive ë°©ì‹ìœ¼ë¡œ summaryë¥¼ ì¶”ì¶œí•˜ê²Œ ë˜ë©´ ëª¨ë¸ì€ ë‹¨ìˆœíˆ sentenceë¥¼ copyí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµì´ ë˜ê¸° ë•Œë¬¸ì—, ì €ìëŠ” ìµœê·¼ masking wordsì™€ contiguous spansì˜ ì„±ê³µì— ì˜ê°ì„ ë°›ì•„ GSGë¥¼ ìˆ˜í–‰í•œë‹¤ê³  ì„¤ëª…í•œë‹¤. GSGëŠ” ì „ì²´ì ìœ¼ë¡œ ì•„ë˜ì™€ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ìˆ˜í–‰ëœë‹¤. Select and mask whole sentences form documents. Concatenate the gap-sentences into a pseudo-summary. The corresponding position of each selected gap sentence is replaced by a mask token [MASK1] to inform the model ì—¬ê¸°ì„œ gap sentence ë¹„ìœ¨ì€ GSR(Gap Sentences Ratio)ì— ì˜í•´ ê²°ì •ë˜ëŠ”ë° ì´ëŠ” ë¬¸ì„œì˜ ì „ì²´ sentenceì—ì„œ ì„ íƒëœ gap sentenceì˜ ë¹„ìœ¨ì„ ì˜ë¯¸í•˜ê³ , ë‹¤ë¥¸ Masked Language Modelì—ì„œì˜ mask rateì™€ ìœ ì‚¬í•œ ê°œë…ì´ë¼ê³  ìƒê°í•˜ë©´ ëœë‹¤. í•´ë‹¹ ë…¼ë¬¸ì—ì„œëŠ” GSRì˜ ë¹„ìœ¨ì— ë”°ë¥¸ ì„±ëŠ¥ì„ ì‹¤í—˜í•˜ì˜€ëŠ”ë° ë°ì´í„°ì…‹ì— ë”°ë¼ ì„±ëŠ¥ í¸ì°¨ê°€ ìˆì—ˆì§€ë§Œ, ìµœì¢…ì ìœ¼ë¡œ GSRì„ 30%ë¡œ ì„ íƒí•˜ì˜€ë‹¤ê³  í•œë‹¤. Three primary strategies for gap-sentenceê·¸ë ‡ë‹¤ë©´ ì–´ë–¤ ë¬¸ì¥ì´ gap sentenceë¡œ ì„ íƒì´ ë˜ëŠ”ê±¸ê¹Œ?í•´ë‹¹ ë…¼ë¬¸ì€ ì ì ˆí•œ Summarizationì„ ìœ„í•´ì„œ gap sentenceëŠ” documentë‚´ì—ì„œ ë‹¤ë¥¸ ë¬¸ì¥ë“¤(remaining sentence)ì— ë¹„í•´ ì „ì²´ ë¬¸ë§¥ì„ ì„¤ëª…í•  ìˆ˜ ìˆëŠ” ì¤‘ìš”í•œ(important/principal) ë¬¸ì¥ì´ ì„ íƒë˜ì–´ì•¼ í•œë‹¤ê³  í•œë‹¤. ì´ë¥¼ ìœ„í•´ Random, Lead, Principalì´ë¼ëŠ” 3ê°€ì§€ ì „ëµì„ ì‚¬ìš©í•œë‹¤. Randomì€ ë§ê·¸ëŒ€ë¡œ ëœë¤í•˜ê²Œ mê°œì˜ sentenceë¥¼ ì¶”ì¶œí•˜ëŠ” ê²ƒì´ê³ , LeadëŠ” ë¬¸ì„œì˜ ê°€ì¥ ì²« mê°œì˜ ë¬¸ì¥, Principalì€ selected sentenceì™€ remaining sentenceê°„ì˜ ROUGE1-F1 scoreë¥¼ ê¸°ë°˜ìœ¼ë¡œ top-mê°œì˜ sentenceë¥¼ ì„ ì •í•˜ëŠ” ê²ƒì„ ë§í•œë‹¤. (Principal ë°©ë²•ì˜ ê²½ìš°ëŠ” Ind/seqê·¸ë¦¬ê³  Orig/Uniq ì˜µì…˜ìœ¼ë¡œ ì„¸ë¶„í™” ë˜ì–´ ì‹¤í—˜ëœë‹¤.) ì•„ë˜ëŠ” documentë‚´ì—ì„œ Random, Lead, Principal(Ing-Orig) ê°ê°ì˜ ì „ëµì— ì˜í•´ ì„ íƒëœ sentenceë“¤ì„ ë³´ì—¬ì¤€ë‹¤. Masked Language Model(MLM)BERTì—ì„œëŠ” input textì˜ 15%ì˜ tokenì„ ì„ íƒí•˜ì—¬, ê·¸ ì¤‘ 80%ëŠ” mask tokenìœ¼ë¡œ ë³€í™˜í•˜ê³ , 10%ëŠ” random token, ë‚˜ë¨¸ì§€ 10%ëŠ” ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ê²Œ ëœë‹¤.ìœ„ ì²«ë²ˆì§¸ ê·¸ë¦¼ì¸ PEGASUS ëª¨ë¸ì˜ ì•„í‚¤í…ì²˜ë¥¼ ë³´ë©´ GSGì™€ MLMì´ ë™ì‹œì— ì ìš©ë˜ê³  ìˆëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆì§€ë§Œ, ì‹¤ì œë¡œëŠ” MLMì´ downstream taskì˜ ì„±ëŠ¥ í–¥ìƒì— ì˜í–¥ì„ ì£¼ì§€ ì•Šì•„ ìµœì¢… ëª¨ë¸ì—ì„œëŠ” MLMì„ í¬í•¨í•˜ì§€ ì•Šì•˜ë‹¤ê³  í•œë‹¤. Pre-training Corpuspre-trainingì„ ìœ„í•´ ì‚¬ìš©ëœ corpusëŠ” C4ì™€ HugeNewsì´ë‹¤. C4(Colossal and Cleaned version of Common Crawl) : consist of text from 350M web-pages(750GB) HugeNews : a dataset of 1.5B articles (3.8TB) collected from news and news-like websites from 2013-2019 Downstream Tasks/Datasetsdownstream summarization ë° ì¬í˜„ ê°€ëŠ¥í•œ ì½”ë“œ ì œê³µì„ ìœ„í•´ public datasetsì¸ Tensorflow Summarization Datasets ë°ì´í„° ì…‹ì„ í™œìš©í•˜ì˜€ë‹¤. ì‚¬ìš©ëœ ë°ì´í„° ì…‹ì€ ì´ 12ê°œë¡œ ì•„ë˜ì™€ ê°™ë‹¤.-Xsum-CNN/DailyMail-NEWSROOM-Multi-News-Gigaword-arXiv-PubMed-BIGPATENT-WikiHow-Reddit TIFU-AESLC-BillSum Experimentsíš¨ìœ¨ì ì¸ ì‹¤í—˜ì„ ìœ„í•˜ì—¬ ëª¨ë¸ì˜ ì‚¬ì´ì¦ˆë¥¼ ì¤„ì¸ PEAGASUS-baseëª¨ë¸(223M parameters)ê³¼ PEGASUS-largeëª¨ë¸(568M parmeters)ì„ ê°ê° ë¹„êµí•œë‹¤. PEAGASUS-base number of layers of encoder and decoder(L) : 12 hidden size(H) : 768 feed-forward layer size(F) : 3,072 number of self-attention heads(A) : 12 PEGASUS-large number of layers of encoder and decoder(L) : 16 hidden size(H) : 1024 feed-forward layer size(F) : 4,096 number of self-attention heads(A) : 16 Pre-Training Corpusìœ„ ê·¸ë¦¼ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ í•™ìŠµì‹œ ì‚¬ìš©ëœ Corpusê°€ ë¬´ì—‡ì´ëƒì— ë”°ë¼ downstream taskì˜ ì„±ëŠ¥ì— ì˜í–¥ì„ ì£¼ê²Œ ëœë‹¤.HugeNewsë¥¼ í† ëŒ€ë¡œ í•™ìŠµëœ ëª¨ë¸ì€ news ë°ì´í„° ì…‹(XSum, CNN/DailyMail)ì—ì„œëŠ” ë†’ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ê³  ìˆëŠ” ë°˜ë©´, non-news ë°ì´í„°ì…‹(WikiHow, Reddit TIFU)ì—ì„œëŠ” ë‚®ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ê³  ìˆë‹¤. EFFECT OF PRE-TRAINING OBJECTIVESGSGì˜ ì„±ëŠ¥ë¹„êµë¥¼ ìœ„í•´ Lead, Random, Ing-Oig, Ing-Uniq, Seq-Orig, Seq-Uniqë¥¼ ë¹„êµí•˜ì˜€ìœ¼ë©°, GSRì˜ ê²½ìš° ë°ì´í„°ì…‹ë§ˆë‹¤ ì„±ëŠ¥ ì°¨ì´ë¥¼ ë³´ì´ì§€ë§Œ, ìµœì¢…ì ìœ¼ë¡œ 30%ë¥¼ ì„ íƒí•˜ì˜€ë‹¤. EFFECT OF VOCABULARYì‹¤í—˜ì„ ìœ„í•´ BPE(Byte-pair encoding)ì™€ SentencePiece Unigramì„ ë¹„êµí•˜ì˜€ë‹¤.ë¹„êµê²°ê³¼ news ë°ì´í„°ì…‹ì—ì„œëŠ” BPEì™€ Unigramì˜ ì„±ëŠ¥ì´ ìœ ì‚¬í•˜ì˜€ì§€ë§Œ, non-news ë°ì´í„°ì…‹(especially WikiHow)ì—ì„œëŠ” SentencePiece Unigramëª¨ë¸ì´ í›¨ì”¬ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚˜íƒ€ëƒˆë‹¤. ìœ„ ê·¸ë˜í”„ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´, WikiHowì˜ ê²½ìš° Unigramì´ 128kì¼ ë•Œ, Reddit TIFUëŠ” 64kì¼ ë•Œ best scoreë¥¼ ë‚˜íƒ€ë‚´ì—ˆê¸° ë•Œë¬¸ì— ì´ë¥¼ ê³ ë ¤í•˜ì—¬ ìµœì¢…ì ìœ¼ë¡œ SentencePiece Unigramì„ ì‚¬ìš©í•˜ê³  vocabulary sizeëŠ” 96kë¡œ ì„ ì •í•˜ì˜€ë‹¤. Larger Modelìœ„ tableì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´, PEGASUSëª¨ë¸ì€ ì´ì „ SOTAëª¨ë¸ ëŒ€ë¹„ ëª¨ë“  12ê°œì˜ downstream tasksì—ì„œ ëª¨ë‘ SOTAë¥¼ ë‹¬ì„±í•œ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. Zero and Low-Resource SummarizationPAGASUS-large ëª¨ë¸ì„ 2000 steps, 256 batch-size, 0.0005 learning-rateë¡œ fine-tuningí•˜ì˜€ì„ ë•Œ, ë‹¨ì§€ 100ê°œì˜ examplesë§Œìœ¼ë¡œë„ ê¸°ì¡´ 20k~200kê°œë¡œ í•™ìŠµëœ Transformer-baseëª¨ë¸ê³¼ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ì˜€ê³ , 1000ê°œì˜ examplesë¥¼ ì‚¬ìš©í•˜ì˜€ì„ë•Œ 12ê°œ ë°ì´í„° ì…‹ì¤‘ 6ê°œì˜ ë°ì´í„° ì…‹ì—ì„œ SOTAë¥¼ ë‹¬ì„±í•  ë§Œí¼ ê¸°ì¡´ ëª¨ë¸ ëŒ€ë¹„ ì ì€ ë¹„ìš©ìœ¼ë¡œ ë†’ì€ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ì˜€ë‹¤ëŠ” ê²ƒì´ í° íŠ¹ì§•ì´ë‹¤. ë˜í•œ, ì‹¤ì œ ì‚¬ëŒì´ ë§Œë“  ìš”ì•½ë³¸ê³¼ PEGASUS-largeëª¨ë¸ì´ ë§Œë“  ìš”ì•½ë³¸ì€ ë¹„êµí•œ ê²°ê³¼ë¥¼ ë³´ë©´, Reddit TIFU ë°ì´í„°ì…‹ì„ ì œì™¸í•œ XSum, CNN/DailyMail ë°ì´í„°ì…‹ì—ì„œëŠ” PEGASUS-largeëª¨ë¸ì´ ë§Œë“  ìš”ì•½ë³¸ì´ ì‚¬ëŒì´ ë§Œë“  ìš”ì•½ë³¸ë³´ë‹¤ ë” ë†’ì€ ì„±ëŠ¥ì„ ë‚˜íƒ€ëƒˆë‹¤ëŠ” ê²ƒì´ íŠ¹ì§•ì´ë‹¤. Conclusionì •ë¦¬í•´ë³´ìë©´ í•´ë‹¹ ë…¼ë¬¸ì˜ í° íŠ¹ì§•ì´ë¼ê³  í•  ìˆ˜ ìˆëŠ” ì ì€, ì²«ì§¸, Abstractive summmarizationì´ë¼ëŠ” íŠ¹ì • taskë¥¼ ìœ„í•´ GSG(Gap-Sentence Generation)ë¼ëŠ” ìƒˆë¡œìš´ pre-trainingê¸°ë²•ì„ í†µí•´ ì ìš©í•œ ì ë‘˜ì§¸, GSGì—ì„œ principal sentence selectionì„ ìœ„í•´ ë‹¤ì–‘í•œ ë°©ë²•ì„ ì ìš©í•œ ì ì…‹ì§¸, ì ì€ ë¦¬ì†ŒìŠ¤ ë¹„ìš©(ex, 1000 examples)ë§Œìœ¼ë¡œë„ ëŒ€ë¶€ë¶„ì˜ ê²°ê³¼ì—ì„œ SOTAë¥¼ ë‹¬ì„±í•œ ì  ì¸ ê²ƒ ê°™ë‹¤. ê·¸ëŸ°ë° ì—¬ê¸°ì„œ ì˜ë¬¸ì´ ë“¤ì—ˆë˜ ì ì€ ì‚¬ëŒì˜ ìš”ì•½ë³¸ê³¼ ì„±ëŠ¥ ë¹„êµë¥¼ í•˜ëŠ”ë° ìˆì–´ì„œ PEGASUS-largeëª¨ë¸ì´ ëŒ€ë¶€ë¶„ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì˜€ëŠ”ë°, ê³¼ì—° human evaluationì´ ê°ê´€ì ìœ¼ë¡œ ì´ë£¨ì–´ì¡ŒëŠ”ì§€ ì˜ë¬¸ì´ ë“¤ì—ˆë‹¤. ê° taskë§ˆë‹¤ 3ëª…ì˜ í‰ê°€ìì— ì˜í•´ 1-5ì ìœ¼ë¡œ í‰ê°€ë¥¼ í•˜ì˜€ë‹¤ê³  í•˜ëŠ”ë° ê³¼ì—° ì¼ë°˜í™” í•  ìˆ˜ ìˆì„ê¹Œ? ì—¬í•˜íŠ¼, ìµœê·¼ text summarizationë¶„ì•¼ë¥¼ ê´€ì‹¬ìˆê²Œ ë³´ê³  ìˆì—ˆëŠ”ë°, summarization taskì— ìµœì í™”ëœ ëª¨ë¸ì´ ë‚˜ì™”ë‹¤ëŠ” ì ì—ì„œ í¥ë¯¸ê°€ ê°”ë˜ ë…¼ë¬¸ì´ì—ˆë‹¤. Reference https://arxiv.org/pdf/1912.08777.pdf https://www.youtube.com/watch?v=JhGmeQBbDdA","categories":[],"tags":[{"name":"nlp","slug":"nlp","permalink":"https://jaehyeongan.github.io/tags/nlp/"},{"name":"summarization","slug":"summarization","permalink":"https://jaehyeongan.github.io/tags/summarization/"},{"name":"transformer","slug":"transformer","permalink":"https://jaehyeongan.github.io/tags/transformer/"},{"name":"gap-sentence-generator","slug":"gap-sentence-generator","permalink":"https://jaehyeongan.github.io/tags/gap-sentence-generator/"},{"name":"mlm","slug":"mlm","permalink":"https://jaehyeongan.github.io/tags/mlm/"}]},{"title":"Basic Object-Detection","slug":"Basic-Object-Detection","date":"2020-04-15T09:34:17.000Z","updated":"2020-10-28T15:14:34.000Z","comments":true,"path":"2020/04/15/Basic-Object-Detection/","link":"","permalink":"https://jaehyeongan.github.io/2020/04/15/Basic-Object-Detection/","excerpt":"","text":"IntroInflearnì˜ ë”¥ëŸ¬ë‹ ì»´í“¨í„° ë¹„ì „ ì™„ë²½ ê°€ì´ë“œë¥¼ ìˆ˜ê°•í•˜ë©° ê³µë¶€ ëª©ì ìœ¼ë¡œ ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤. Computer Vision Techniques Classification(ë¶„ë¥˜) : ì´ë¯¸ì§€ì— ìˆëŠ” objectê°€ ë¬´ì—‡ì¸ì§€ë§Œ íŒë³„, ìœ„ì¹˜ ê³ ë ¤ x Localization(ë°œê²¬) : object íŒë³„ ë° ë‹¨ í•˜ë‚˜ì˜ object ìœ„ì¹˜ë¥¼ bounding boxë¡œ ì§€ì •í•˜ì—¬ ì°¾ìŒ Detection(ë°œê²¬) : object íŒë³„ ë° ì—¬ëŸ¬ ê°œì˜ objectë“¤ì— ëŒ€í•œ ìœ„ì¹˜ë¥¼ bounding boxë¡œ ì§€ì •í•˜ì—¬ ì°¾ìŒ Segmentation(ë¶„í• ) : object íŒë³„ ë° Pixel ë ˆë²¨ì˜ detectionì„ í†µí•´ ëª¨ë“  í”½ì…€ì˜ ë ˆì´ë¸”ì„ ì˜ˆì¸¡ Object DetectionHistory í˜„ì¬ YOLO ëª¨ë¸ì´ real-time ì˜ˆì¸¡ ì¸¡ë©´ì—ì„œ ì„±ëŠ¥ì´ ë‚˜ì˜ì§€ ì•Šì•„ ì‹¤ë¬´ì—ì„œ ê°€ì¥ ë§ì´ í™œìš©ë˜ê³  ìˆìŒ real-timeì—ëŠ” í•œê³„ê°€ ìˆìœ¼ë‚˜ ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì€ ëª¨ë¸ì€ RetinaNet Sliding Window ë°©ì‹ì„ í™œìš©í•œ ì´ˆê¸° object detection object detectionì˜ ì´ˆê¸° ê¸°ë²• windowë¥¼ ì™¼ìª½ ìƒë‹¨ì—ì„œë¶€í„° ì˜¤ë¥¸ìª½ í•˜ë‹¨ìœ¼ë¡œ ì´ë™ì‹œí‚¤ë©´ì„œ objectë¥¼ detectioní•˜ëŠ” ë°©ì‹ ì˜¤ë¸Œì íŠ¸ê°€ ì—†ëŠ” ì—­ì—­ë„ ë¬´ì¡°ê±´ ìŠ¬ë¼ì´ë”©í•˜ë©° ì—¬ëŸ¬ í˜•íƒœì˜ windowì™€ scaleì„ ìŠ¤ìº”í•´ì•¼ í•˜ë¯€ë¡œ ìˆ˜í–‰ì‹œê°„ ë° ì„±ëŠ¥ì´ íš¨ìœ¨ì ì´ì§€ ì•ŠìŒ Region Proposal ê¸°ë²•ì˜ ë“±ì¥ ì´í›„ í™œìš©ë„ê°€ ë–¨ì–´ì¡Œì§€ë§Œ object detectionì˜ ê¸°ìˆ ì  í† ëŒ€ ì œê³µ Obejct Detectionì˜ ì£¼ìš” êµ¬ì„± ìš”ì†Œ ë° ë¬¸ì œì£¼ìš” êµ¬ì„±ìš”ì†Œ Region Proposal Detectionì„ ìœ„í•œ Network êµ¬ì„±(feature extraction, network prediction) detectionì„ ìœ„í•œ ìš”ì†Œë“¤(IoU, NMS, mAP, Anchor Box ë“±) ì£¼ìš” ë¬¸ì œ ë¬¼ì²´ íŒë³„(Classification) + ë¬¼ì²´ ìœ„ì¹˜ ì°¾ê¸°(Regression)ì„ ë™ì‹œì— ìˆ˜í–‰í•´ì•¼ í•¨ í•œ ì´ë¯¸ì§€ ë‚´ì— í¬ê¸°, ìƒ‰, ìƒê¹€ìƒˆê°€ ë‹¤ì–‘í•œ objectê°€ ì„ì—¬ ìˆìŒ ì‹¤ì‹œê°„ detectionì„ ìœ„í•´ ì‹œê°„ ì„±ëŠ¥ì´ ì¤‘ìš” ëª…í™•í•˜ì§€ ì•Šì€ ì´ë¯¸ì§€ê°€ ë§ìŒ(ë…¸ì´ì¦ˆ í˜¹ì€ ë°°ê²½ì´ ì „ë¶€ì¸ ì‚¬ì§„ ë“±) ì´ë¯¸ì§€ ë°ì´í„° ì…‹ì˜ ë¶€ì¡± Region Proposal(ì˜ì—­ ì¶”ì •) ëª©í‘œ : Objectê°€ ìˆì„ ë§Œí•œ í›„ë³´ ì˜ì—­ì„ ì°¾ì! ëŒ€í‘œì ì¸ ê¸°ë²•ì´ Selective Search Selective Search Region Proposalì˜ ëŒ€í‘œì ì¸ ê¸°ë²• ì»¬ëŸ¬(color), ë¬´ëŠ¬(texture), í¬ê¸°(size), í˜•íƒœ(shape) ë“±ì— ë”°ë¼ ìœ ì‚¬í•œ regionë“¤ì„ ê³„ì¸µì ìœ¼ë¡œ ê·¸ë£¹í•‘ í•˜ëŠ” ë°©ë²• Selective Search ìˆ˜í–‰ í”„ë¡œì„¸ìŠ¤ ì´ˆê¸° ìˆ˜ ì²œê°œì˜ ê°œë³„ Over segmentationëœ ëª¨ë“  ë¶€ë¶„ë“¤ì„ bounding boxë¡œ ë§Œë“¤ì–´ region proposal ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€ ì»¬ëŸ¬(color), ë¬´ëŠ¬(texture), í¬ê¸°(size), í˜•íƒœ(shape) ë“±ì— ë”°ë¼ ìœ ì‚¬í•œ segmentë“¤ì„ ê·¸ë£¹í•‘ ìœ„ ê³¼ì •ì„ ë°˜ë³µí•˜ë©° ìµœì¢… ê·¸ë£¹í•‘ ëœ segmentë“¤ì„ ì œì•ˆ Pythonì„ í†µí•œ Selective Search êµ¬í˜„ pip install selectivesearch ë¥¼ í†µí•´ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ 12345678import selectivesearchimport cv2img = cv2.imread('./image/test.jpg') # ì´ë¯¸ì§€ ë¡œë“œ img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)plt.figure(figsize=(8, 8))plt.imshow(img_rgb)plt.show() 123456#selectivesearch.selective_search()ëŠ” ì´ë¯¸ì§€ì˜ Region Proposalì •ë³´ë¥¼ ë°˜í™˜ _, regions = selectivesearch.selective_search(img_rgb, scale=100, # bounding box scale min_size=2000) # rectì˜ ìµœì†Œ ì‚¬ì´ì¦ˆregions[:5] 12345[&#123;&apos;rect&apos;: (0, 0, 58, 257), &apos;size&apos;: 7918, &apos;labels&apos;: [0.0]&#125;, &#123;&apos;rect&apos;: (16, 0, 270, 50), &apos;size&apos;: 5110, &apos;labels&apos;: [1.0]&#125;, &#123;&apos;rect&apos;: (284, 0, 90, 420), &apos;size&apos;: 6986, &apos;labels&apos;: [2.0]&#125;, &#123;&apos;rect&apos;: (59, 14, 262, 407), &apos;size&apos;: 3986, &apos;labels&apos;: [3.0]&#125;, &#123;&apos;rect&apos;: (62, 17, 256, 401), &apos;size&apos;: 5282, &apos;labels&apos;: [4.0]&#125;] ë°˜í™˜ëœ regions ë³€ìˆ˜ëŠ” ë¦¬ìŠ¤íŠ¸ íƒ€ì…ìœ¼ë¡œ ì„¸ë¶€ ì›ì†Œë¡œ ë”•ì…”ë„ˆë¦¬ë¥¼ ê°€ì§€ê³  ìˆìŒ. rect í‚¤ê°’ì€ x,y ì‹œì‘ ì¢Œí‘œì™€ ë„ˆë¹„, ë†’ì´ ê°’ì„ ê°€ì§€ë©° ì´ ê°’ì´ Detected Object í›„ë³´ë¥¼ ë‚˜íƒ€ë‚´ëŠ” Bounding boxì„. sizeëŠ” Bounding boxì˜ í¬ê¸° labelsëŠ” í•´ë‹¹ rectë¡œ ì§€ì •ëœ Bounding Boxë‚´ì— ìˆëŠ” ì˜¤ë¸Œì íŠ¸ë“¤ì˜ ê³ ìœ  ID ì•„ë˜ë¡œ ë‚´ë ¤ê°ˆ ìˆ˜ë¡ íŠ¹ì„±ì´ ë¹„ìŠ·í•œ ê²ƒë“¤ì´ í•©ì³ì§€ê³ , ë„ˆë¹„ì™€ ë†’ì´ ê°’ì´ í° Bounding boxì´ë©° í•˜ë‚˜ì˜ Bounding boxì— ì—¬ëŸ¬ê°œì˜ ì˜¤ë¸Œì íŠ¸ê°€ ìˆì„ í™•ë¥ ì´ ì»¤ì§. 12345678910111213141516# Bounding Box ì‹œê°í™” green_rgb = (125, 255, 51)img_rgb_copy = img_rgb.copy()for rect in cand_rects: left = rect[0] top = rect[1] # rect[2], rect[3]ì€ ë„ˆë¹„ì™€ ë†’ì´ì´ë¯€ë¡œ ìš°í•˜ë‹¨ ì¢Œí‘œë¥¼ êµ¬í•˜ê¸° ìœ„í•´ ì¢Œìƒë‹¨ ì¢Œí‘œì— ê°ê°ì„ ë”í•¨. right = left + rect[2] bottom = top + rect[3] img_rgb_copy = cv2.rectangle(img_rgb_copy, (left, top), (right, bottom), color=green_rgb, thickness=2) plt.figure(figsize=(8, 8))plt.imshow(img_rgb_copy)plt.show() IoU(Intersection over Union)ëª¨ë¸ì´ ì˜ˆì¸¡í•œ bounding boxì™€ ì‹¤ì œ ground truth boxê°€ ì–¼ë§ˆë‚˜ ì •í™•í•˜ê²Œ ê²¹ì¹˜ëŠ”ì§€ë¥¼ ì¸¡ì •í•˜ëŠ” ì§€í‘œ ì•„ë˜ì™€ ê°™ì€ ì§€í‘œë¡œ ê³„ì‚° ë˜ë©° 100%ë¡œ ì •í™•í•˜ê²Œ ê²¹ì³ì§ˆ ë•Œì˜ ê°’ì€ 1ì´ ë¨ IoU ê°’ì— ë”°ë¼ detection ì˜ˆì¸¡ ì„±ê³µ ê²°ì • object detectionì—ì„œ ê°œë³„ objectì— ëŒ€í•œ ê²€ì¶œ ì˜ˆì¸¡ì´ ì„±ê³µí•˜ì˜€ëŠ”ì§€ì— ëŒ€í•œ ì—¬ë¶€ë¥¼ IoUë¥¼ í†µí•´ ê²°ì • ì¼ë°˜ì ìœ¼ë¡œ PASCAL VOC Challengeì—ì„œ ëŠ” IoUê°€ 0.5ì´ìƒì´ë©´ ì˜ˆì¸¡ ì„±ê³µí–ˆë‹¤ê³  íŒë‹¨ Pythonì„ í†µí•œ IoU ê³„ì‚°123456789101112131415def compute_iou(cand_box, gt_box): # Calculate intersection areas x1 = np.maximum(cand_box[0], gt_box[0]) y1 = np.maximum(cand_box[1], gt_box[1]) x2 = np.minimum(cand_box[2], gt_box[2]) y2 = np.minimum(cand_box[3], gt_box[3]) intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0) # width * height (x2ì—ì„œ x1ì„ ëº€ ê°’ì´ width, y2ì—ì„œ y1ì„ ëº€ ê°’ì´ height ì´ë¯€ë¡œ) cand_box_area = (cand_box[2] - cand_box[0]) * (cand_box[3] - cand_box[1]) # width * height gt_box_area = (gt_box[2] - gt_box[0]) * (gt_box[3] - gt_box[1]) # width * height union = cand_box_area + gt_box_area - intersection # ì‹¤ì œboxì™€ ì˜ˆì¸¡boxì˜ í•©ì—ì„œ intersectionì„ ëºŒ iou = intersection / union return iou ì‹¤ì œ bounding boxì™€ í›„ë³´ bounding boxê°€ ìˆì„ ë•Œ, ë‘˜ ì¤‘ì—ì„œ x1ê³¼ y1ì¢Œí‘œëŠ” maxê°’, x2ì™€ y2ì¢Œí‘œëŠ” minê°’ì„ ì„ íƒí•˜ê²Œ ë˜ë©´ ê·¸ ì¢Œí‘œê°€ Intersection areaê°€ ë˜ë©° ë‘ ê°œì˜ boxë¥¼ ë”í•œ í›„ intersectionì„ ë¹¼ì¤€ ê°’ì´ Union area ë§ˆì§€ë§‰ìœ¼ë¡œ intersectionì„ unionìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì£¼ë©´ IoUê°’ì„ ì–»ì„ ìˆ˜ ìˆìŒ NMS(Non Max Suppression)object detection ì‹œ ìµœëŒ€í•œ objectë¥¼ ë†“ì¹˜ì§€ ì•Šê¸° ìœ„í•´ ë§ì€ bounding boxë¥¼ ì°¾ê²Œ ë˜ëŠ”ë°, ì´ë ‡ê²Œ detected ëœ ìˆ˜ë§ì€ bounding box ì¤‘ ë¹„ìŠ·í•œ ìœ„ì¹˜ì— ìˆëŠ” boxë¥¼ ì œê±°í•˜ê³  ê°€ì¥ ì í•©í•œ boxë¥¼ ì„ íƒí•˜ëŠ” ê¸°ë²• NMS ìˆ˜í–‰ í”„ë¡œì„¸ìŠ¤ Detected ëœ Bounding boxë³„ë¡œ íŠ¹ì • Confidence score threshold ì´í•˜ bounding boxëŠ” ë¨¼ì € ì œê±° (ex. confidence score threshold &lt; 0.5) ê°€ì¥ ë†’ì€ confidence scoreë¥¼ ê°€ì§„ box ìˆœìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬í•˜ê³  ì•„ë˜ ë¡œì§ì„ ëª¨ë“  boxì— ìˆœì°¨ì ìœ¼ë¡œ ì ìš© ë†’ì€ confidence scoreë¥¼ ê°€ì§„ boxì™€ ê²¹ì¹˜ëŠ” ë‹¤ë¥¸ boxë¥¼ ëª¨ë‘ ì¡°ì‚¬í•˜ì—¬ IoUê°€ íŠ¹ì • threshold ì´ìƒì¸ boxë¥¼ ëª¨ë‘ ì œê±° (ex. IoU Threshold &gt; 0.4) ë‚¨ì•„ìˆëŠ” boxë§Œ ì„ íƒ Confidence score thresholdê°€ ë†’ì„ ìˆ˜ë¡, IoU Thresholdê°€ ë‚®ì„ ìˆ˜ë¡ ë§ì€ boxê°€ ì œê±° ë¨ Object Detection ì„±ëŠ¥ í‰ê°€mAP(mean Average Precision) ì‹¤ì œ Objectê°€ detectedëœ ì¬í˜„ìœ¨(recall)ì˜ ë³€í™”ì— ë”°ë¥¸ ì •ë°€ë„(precision)ì˜ ê°’ì„ í‰ê· í•œ ì„±ëŠ¥ ìˆ˜ì¹˜ ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ ì •ë°€ë„ëŠ” ëª¨ë¸ì´ positiveë¼ê³  ì˜ˆì¸¡í•œ ëŒ€ìƒ ì¤‘ ì˜ˆì¸¡ ê°’ì´ ì‹¤ì œ positive ê°’ê³¼ ì–¼ë§ˆë‚˜ ì¼ì¹˜í•˜ëŠ”ì§€ì— ëŒ€í•œ ë¹„ìœ¨(ì¦‰, ì˜ˆì¸¡í•œ objectê°€ ì‹¤ì œ objectë“¤ê³¼ ì–¼ë§ˆë‚˜ ì¼ì¹˜í•˜ëŠ”ì§€) ì¬í˜„ìœ¨ì€ ì‹¤ì œ positive ê°’ ì¤‘ ëª¨ë¸ì´ ì–¼ë§ˆë‚˜ ì‹¤ì œ ê°’ì„ positiveë¼ê³  ì˜ˆì¸¡í–ˆëŠ”ì§€ì— ëŒ€í•œ ë¹„ìœ¨(ì¦‰, ì‹¤ì œ objectë¥¼ ì–¼ë§ˆë‚˜ ë¹ ë“œë¦¬ì§€ ì•Šê³  ì˜ ì˜ˆì¸¡í–ˆëŠ”ì§€) Precision Recall Trade-off : ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ì€ ìƒí˜¸ ë³´ì™„ì ì¸ ê´€ê³„ì´ë¯€ë¡œ ì–´ëŠ í•œìª½ì´ ë†’ì•„ì§€ë©´ ë‹¤ë¥¸ ìª½ì´ ë‚®ì•„ì§€ê²Œ ë¨ Precision-Recall Curve : confidence thresholdì˜ ë³€í™”ì— ë”°ë¥¸ ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ì˜ ë³€í™” ê³¡ì„ , ì´ ê³¡ì„ ì˜ ì•„ë«ë¶€ë¶„ ë©´ì ì„ AP(Averge Precision, í‰ê·  ì •ë°€ë„)ë¼ê³  í•¨ APëŠ” í•˜ë‚˜ì˜ objectì— ëŒ€í•œ ì„±ëŠ¥ ìˆ˜ì¹˜ì´ë©°, mAPëŠ” ì—¬ëŸ¬ objectë“¤ì˜ APë¥¼ í‰ê· í•œ ê°’ì„ ì˜ë¯¸ Image Resolution / FPS / Detection ì„±ëŠ¥ ìƒê´€ ê´€ê³„ì¼ë°˜ì ìœ¼ë¡œ ì´ë¯¸ì§€ í•´ìƒë„(Image Resolution)ê°€ ë†’ì„ ìˆ˜ë¡ Detectionì„±ëŠ¥ì´ ì¢‹ì•„ì§€ì§€ë§Œ ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì‹œê°„(FPS)ì´ ì˜¤ë˜ê±¸ë¦¼ High Resolution -&gt; High Detection Score -&gt; Low FPS Low Resolution -&gt; Low Detection Score -&gt; High FPS Object Detectionì„ ìœ„í•œ ì£¼ìš” ë°ì´í„° ì…‹ Pascal-VOC - XML format, 20ê°œì˜ ì˜¤ë¸Œì íŠ¸ ì¹´í…Œê³ ë¦¬ MS-COCO - json format, 80ê°œì˜ ì˜¤ë¸Œì íŠ¸ ì¹´í…Œê³ ë¦¬ Google Open Images - csv format, 600ê°œì˜ ì˜¤ë¸Œì íŠ¸ ì¹´í…Œê³ ë¦¬","categories":[],"tags":[{"name":"objectdetection","slug":"objectdetection","permalink":"https://jaehyeongan.github.io/tags/objectdetection/"},{"name":"region-proposal","slug":"region-proposal","permalink":"https://jaehyeongan.github.io/tags/region-proposal/"},{"name":"selectivesearch","slug":"selectivesearch","permalink":"https://jaehyeongan.github.io/tags/selectivesearch/"},{"name":"IoU","slug":"IoU","permalink":"https://jaehyeongan.github.io/tags/IoU/"},{"name":"NMS","slug":"NMS","permalink":"https://jaehyeongan.github.io/tags/NMS/"},{"name":"mAP","slug":"mAP","permalink":"https://jaehyeongan.github.io/tags/mAP/"}]},{"title":"LSTM Autoencoder for Anomaly Detection","slug":"LSTM-Autoencoder-for-Anomaly-Detection","date":"2020-02-29T09:19:16.000Z","updated":"2020-10-28T15:14:34.000Z","comments":true,"path":"2020/02/29/LSTM-Autoencoder-for-Anomaly-Detection/","link":"","permalink":"https://jaehyeongan.github.io/2020/02/29/LSTM-Autoencoder-for-Anomaly-Detection/","excerpt":"","text":"Introì§€ë‚œ í¬ìŠ¤íŒ…(Autoencoderì™€ LSTM Autoencoder)ì— ì´ì–´ LSTM Autoencoderë¥¼ í†µí•´ Anomaly Detectioní•˜ëŠ” ë°©ì•ˆì— ëŒ€í•´ ì†Œê°œí•˜ê³ ì í•œë‹¤. Autoencoderì˜ ê²½ìš° ë³´í†µ ì´ë¯¸ì§€ì˜ ìƒì„±ì´ë‚˜ ë³µì›ì— ë§ì´ ì‚¬ìš©ë˜ë©° ì´ëŸ¬í•œ êµ¬ì¡°ë¥¼ ì´ì–´ë°›ì•„ ëŒ€í‘œì ì¸ ë”¥ëŸ¬ë‹ ìƒì„± ëª¨ë¸ì¸ GAN(Generative Adversarial Network)ìœ¼ë¡œ ê¹Œì§€ ì´ì–´ì¡ŒëŠ”ë° ì´ëŸ¬í•œ ìê¸° í•™ìŠµ ëª¨ë¸ì€ Anomaly Detection ë¶„ì•¼ì—ì„œë„ ë„ë¦¬ ì‚¬ìš©ë˜ê³  ìˆë‹¤.ëŒ€í‘œì ìœ¼ë¡œ ì´ë¯¸ì§€ ë¶„ì•¼ì—ì„œë„ ì •ìƒì ì¸ ì´ë¯¸ì§€ë¡œ ëª¨ë¸ í•™ìŠµ í›„ ë¹„ì •ìƒì ì¸ ì´ë¯¸ì§€ë¥¼ ë„£ì–´ ì´ë¥¼ ë””ì½”ë”© í•˜ê²Œ ë˜ë©´ ì •ìƒ ì´ë¯¸ì§€ íŠ¹ì„±ê³¼ ë””ì½”ë”© ëœ ì´ë¯¸ì§€ ê°„ì˜ ì°¨ì´ì¸ ì¬êµ¬ì„± ì†ì‹¤(Reconstruction Error)ë¥¼ ê³„ì‚°í•˜ê²Œ ë˜ëŠ”ë° ì´ ì¬êµ¬ì„± ì†ì‹¤ì´ ë‚®ì€ ë¶€ë¶„ì€ ì •ìƒ(normal), ì¬êµ¬ì„± ì†ì‹¤ì´ ë†’ì€ ë¶€ë¶„ì€ ì´ìƒ(Abnormal)ë¡œ íŒë‹¨í•  ìˆ˜ ìˆë‹¤. ì´ëŸ¬í•œ Anomaly Detectionì€ ì´ë¯¸ì§€ ë¿ë§Œ ì•„ë‹ˆë¼ ì´ì œë¶€í„° ì‚´í´ë³´ê³ ì í•˜ëŠ” ì‹œê³„ì—´ ë°ì´í„°ì—ë„ ì ìš©ì´ ê°€ëŠ¥í•˜ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ íŠ¹ì • ì„¤ë¹„ì˜ ì„¼ì„œë¥¼ í†µí•´ ë¹„ì •ìƒ ì‹ í˜¸ë¥¼ íƒì§€í•˜ê³ ì í•œë‹¤ë©´ Autoencoderë¥¼ LSTM ë ˆì´ì–´ë¡œ êµ¬ì„±í•œë‹¤ë©´ ì´ëŸ¬í•œ ì‹œí€€ìŠ¤ í•™ìŠµì´ ê°€ëŠ¥í•˜ê²Œ ëœë‹¤. ì´ë¥¼ í†µí•´ ì •ìƒ ì‹ í˜¸ë§Œì„ ì´ìš©í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµì‹œì¼œ ì¶”í›„ ë¹„ì •ìƒ ì‹ í˜¸ê°€ ëª¨ë¸ì— ì…ë ¥ë˜ë©´ ë†’ì€ reconstruction errorë¥¼ ë‚˜íƒ€ë‚¼ ê²ƒì´ë¯€ë¡œ ì´ë¥¼ ë¹„ì •ìƒ ì‹ í˜¸ë¡œ íŒë‹¨í•  ìˆ˜ ìˆê²Œ ëœë‹¤. LSTM Autoencoder LSTM AutoencoderëŠ” ì‹œí€€ìŠ¤(sequence) ë°ì´í„°ì— Encoder-Decoder LSTM ì•„í‚¤í…ì²˜ë¥¼ ì ìš©í•˜ì—¬ êµ¬í˜„í•œ ì˜¤í† ì¸ì½”ë”ì´ë‹¤. ëª¨ë¸ì— ì…ë ¥ ì‹œí€€ìŠ¤ê°€ ìˆœì°¨ì ìœ¼ë¡œ ë“¤ì–´ì˜¤ê²Œ ë˜ê³ , ë§ˆì§€ë§‰ ì…ë ¥ ì‹œí€€ìŠ¤ê°€ ë“¤ì–´ì˜¨ í›„ ë””ì½”ë”ëŠ” ì…ë ¥ ì‹œí€€ìŠ¤ë¥¼ ì¬ìƒì„±í•˜ê±°ë‚˜ í˜¹ì€ ëª©í‘œ ì‹œí€€ìŠ¤ì— ëŒ€í•œ ì˜ˆì¸¡ì„ ì¶œë ¥í•œë‹¤.ìœ„ì—ì„œ ì„¤ëª…í•œ ê²ƒê³¼ ë§ˆì°¬ê°€ì§€ë¡œ LSTM Autoencoder í•™ìŠµ ì‹œì—ëŠ” ì •ìƒ(normal) ì‹ í˜¸ì˜ ë°ì´í„°ë¡œë§Œ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê²Œ ëœë‹¤. encoderì™€ decoderëŠ” í•™ìŠµì´ ì§„í–‰ë  ìˆ˜ ë¡ ì •ìƒ ì‹ í˜¸ë¥¼ ë” ì •ìƒ ì‹ í˜¸ ë‹µê²Œ í‘œí˜„í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•˜ê²Œ ë  ê²ƒì´ë©° ìµœì¢…ì ìœ¼ë¡œ ì¬êµ¬ì„± í•œ ê²°ê³¼ë„ ì •ìƒ ì‹ í˜¸ì™€ ë§¤ìš° ìœ ì‚¬í•œ ë¶„í¬ë¥¼ ê°€ì§€ëŠ” ë°ì´í„°ì¼ ê²ƒì´ë‹¤. ê·¸ë ‡ê¸° ë•Œë¬¸ì— ì´ ëª¨ë¸ì— ë¹„ì •ìƒ ì‹ í˜¸ë¥¼ ì…ë ¥ìœ¼ë¡œ ë„£ê²Œ ë˜ë©´ ì •ìƒ ë¶„í¬ì™€ ë‹¤ë¥¸ íŠ¹ì„±ì˜ ë¶„í¬ë¥¼ ë‚˜íƒ€ë‚¼ ê²ƒì´ê¸° ë•Œë¬¸ì— ë†’ì€ reconstruction errorë¥¼ ë³´ì´ê²Œ ë  ê²ƒì´ë‹¤. Curve Shiftingì„ ì ìš©í•œ LSTM Autoencoder ì „ì²´ í”„ë¡œì„¸ìŠ¤ëŠ” ìœ„ ì•„í‚¤í…ì²˜ì™€ ê°™ë‹¤. ë¨¼ì € Curve Shiftingì„ í†µí•´ ë°ì´í„°ì˜ ì‹œì ì„ ë³€í™˜í•´ì£¼ê³  normal ë°ì´í„°ë§Œì„ í†µí•´ LSTM Autoencoder ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê²Œ ëœë‹¤. ê·¸ í›„ ì¬êµ¬ì„± ì†ì‹¤ì„ ê³„ì‚° í›„ Precision Recall Curveë¥¼ í†µí•´ normal/abnormalì„ êµ¬ë¶„í•˜ê¸° ìœ„í•œ thresholdë¥¼ ì§€ì •í•˜ê²Œ ë˜ê³  ì´ thresholdë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë§ˆì§€ë§‰ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ì…‹ì˜ ì¬êµ¬ì„± ì†ì‹¤ì„ ë¶„ë¥˜í•˜ì—¬ t+n ì‹œì ì„ ì˜ˆì¸¡í•˜ê²Œ ëœë‹¤.ê° ë¶€ë¶„ì— ëŒ€í•´ ì•„ë˜ì—ì„œ ì¢€ ë” ìƒì„¸íˆ ì‚´í´ë³´ì. 1. Curve Shiftingë¹„ì •ìƒ ì‹ í˜¸ë¥¼ íƒì§€í•˜ê¸° ìœ„í•´ì„œëŠ” ë¹„ì •ìƒ ì‹ í˜¸ê°€ ë“¤ì–´ì˜¤ê¸° ì „ì— ì¦‰, ë­”ê°€ ê³ ì¥ í˜¹ì€ ê²°í•¨ì´ ë°œìƒí•˜ê¸° ì „ì— ë¯¸ë¦¬ ì˜ˆì¸¡ì„ í•´ì•¼ë§Œ í•œë‹¤. ê·¸ë ‡ê¸° ë•Œë¬¸ì— ë‹¨ìˆœíˆ í˜„ì¬ ì‹œì ì˜ errorë¥¼ ê³„ì‚°í•˜ì—¬ ë¹„ì •ìƒ ì‹ í˜¸ë¥¼ íƒì§€í•˜ëŠ” ê²ƒì€ ì´ë¯¸ ê³ ì¥ì´ ë°œìƒí•œ í›„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒê³¼ ë‹¤ë¦„ì´ ì—†ê¸° ë•Œë¬¸ì— ë°ì´í„°ì— ëŒ€í•œ ì‹œì  ë³€í™˜ì´ ê¼­ í•„ìš”í•˜ë‹¤. ì´ëŸ¬í•œ future value ì˜ˆì¸¡ì„ ìœ„í•´ ë‹¤ì–‘í•œ ë°©ë²•ì´ ìˆëŠ”ë° ì—¬ê¸°ì„œëŠ” Curve Shiftingì´ë¼ëŠ” ê¸°ë²•ì„ ì ìš©í•  ê²ƒì´ë‹¤. Curve Shiftingì€ ì‚¬ì „ ì˜ˆì¸¡ ê°œë…ì„ ì ìš©í•˜ê¸° ìœ„í•œ Shifting ë°©ë²•ì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ìœ„ ê·¸ë¦¼ê³¼ ê°™ì´ ë¹„ì •ìƒ ì‹ í˜¸(1)ë¥¼ 2ì¼ ì „ì— ì¡°ê¸° ì˜ˆì¸¡ í•˜ê³ ì í•œë‹¤ë©´ ë‹¨ìˆœíˆ Yê°’ì„ ë‘ ì¹¸ì”© ë‚´ë¦¬ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ë¹„ì •ìƒ ì‹ í˜¸(1)ê°€ ìˆëŠ” ë‚ ì§œë¡œë¶€í„° 2ì¼ ì „ê¹Œì§€ì˜ ë°ì´í„°ë¥¼ ë¹„ì •ìƒ ì‹ í˜¸(1)ë¡œ ë°”ê¾¸ì–´ì£¼ëŠ” ê²ƒì´ë‹¤. ì´ëŠ” ë¹„ì •ìƒ ì‹ í˜¸ê°€ ë°œìƒí•˜ê¸° ì „ ì–´ë– í•œ ì¡°ì§ì´ ìˆì„ ê²ƒì´ë©° ì´ëŸ¬í•œ ì¡°ì§ì´ ë°ì´í„° íŠ¹ì„±ì— ë‚˜íƒ€ë‚  ê²ƒì´ë¼ëŠ” ê°€ì •ì„ ê°€ì§€ê³  í•™ìŠµí•˜ëŠ” ë°©ë²•ì´ë‹¤.ê·¸ë¦¬ê³  ë‚˜ì„œ ë³¸ë˜ ë¹„ì •ìƒ ì‹ í˜¸(1) ë°ì´í„°ë¥¼ ì œê±°í•´ì£¼ëŠ”ë° ì´ë ‡ê²Œ í•˜ëŠ” ì´ìœ ëŠ” ë¼ë²¨ì„ ë°”ê¿”ì£¼ëŠ” ìˆœê°„ ì´ëŠ” ë¹„ì •ìƒ ì‹ í˜¸ ì˜ˆì¸¡ ë¬¸ì œê°€ ì•„ë‹Œ ë¹„ì •ìƒ ì‹ í˜¸ ì¡°ì§ ì˜ˆì¸¡ ë¬¸ì œê°€ ë˜ëŠ” ê²ƒì´ ë•Œë¬¸ì— ë°ì´í„°ì˜ í•™ìŠµ í˜¼ë™ì„ ì—†ì• ì£¼ê¸° ìœ„í•´ ì œê±°í•˜ëŠ” ê²ƒì´ë¼ ë³´ë©´ ë  ê²ƒì´ë‹¤. 2. Threshold by Precision-Recall-CurveAutoencoderëŠ” ì¬êµ¬ì„± ëœ ê²°ê³¼ë¥¼ intputê³¼ ë¹„êµí•˜ì—¬ ì¬êµ¬ì„± ì†ì‹¤(Reconstruction Error)ë¥¼ ê³„ì‚°í•œë‹¤ê³  ë§í–ˆë‹¤. ê·¸ë¦¬ê³  ì´ ì¬êµ¬ì„± ì†ì‹¤ê°’ì„ í†µí•´ ì†ì‹¤ê°’ì´ ë‚®ìœ¼ë©´ ì •ìƒìœ¼ë¡œ, ì†ì‹¤ê°’ì´ ë†’ìœ¼ë©´ ì´ìƒìœ¼ë¡œ íŒë‹¨í•œë‹¤ê³  í•˜ì˜€ëŠ”ë°, ì´ ì •ìƒê³¼ ì´ìƒì„ ë‚˜ëˆ„ëŠ” ê¸°ì¤€ì€ ê³¼ì—° ë¬´ì—‡ì¼ê¹Œ?ì¼ë°˜ì ìœ¼ë¡œ ëª¨ë¸ì´ ì •ìƒ ë°ì´í„°ë§Œìœ¼ë¡œ í•™ìŠµì„ í•˜ì—¬ ì •ìƒ ë°ì´í„°ë¥¼ ì¬êµ¬ì„±í•˜ì˜€ì„ ë•Œ í•™ìŠµì´ ì˜ ë˜ì—ˆë‹¤ê³  ê°€ì •í•˜ë©´ ì†ì‹¤ê°’ì€ 0ì— ê°€ê¹Œìš¸ ê²ƒì´ê³ , í•™ìŠµì´ ì˜ ì•ˆë˜ì—ˆë‹¤ê³  í•˜ë©´ ì†ì‹¤ê°’ì€ 1ì— ê°€ê¹Œìš¸ ê²ƒì´ë‹¤. ë³´í†µ ë¶„ë¥˜(Classification)ë¬¸ì œì—ì„œëŠ” ì˜ˆì¸¡ í™•ë¥ ê°’(0% ~ 100%)ì„ í†µí•´ 50%ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¥˜ë¥¼ í•˜ê²Œ ë˜ëŠ”ë°, ì´ recontruction errorì˜ ê²½ìš° ê·¸ë ‡ê²Œ ê·¹ë‹¨ì ìœ¼ë¡œ ê°’ì´ íŠ€ê¸°ëŠ” í˜ë“¤ê¸° ë•Œë¬¸ì— ì •ìƒê³¼ ì´ìƒì„ ë¶„ë¦¬í•˜ëŠ” íƒ€ë‹¹í•œ thresholdê°’ì„ ì •í•˜ëŠ” ê²ƒì´ í•„ìš”í•˜ë‹¤. Precision Recall Curve ìœ„ì™€ ê°™ì€ ë¬¸ì œì˜ ì ì ˆí•œ thresholdê°’ì„ ì ìš©í•˜ê¸° ìœ„í•œ ë°©ë²• ì¤‘ í•˜ë‚˜ë¡œ precision recall curveê°€ ìˆë‹¤. ì´ëŠ” Recall(ì¬í˜„ìœ¨)ê³¼ Precision(ì •ë°€ë„)ê°€ ì„œë¡œ Trade off ê´€ê³„ë¥¼ ê°€ì§€ê¸° ë•Œë¬¸ì— ì–´ëŠ í•œìª½ì— ì¹˜ìš°ì§€ì§€ ì•ŠëŠ” ìµœì ì˜ thresholdë¥¼ êµ¬í•˜ê¸° ìœ„í•œ ë°©ë²•ì´ë‹¤.ì¶”í›„ ì´ ê²€ì¦ ê¸°ë²•ì„ ì ìš©í•˜ì—¬ LSTM Autoencoderë¥¼ í†µí•´ ì¬êµ¬ì„± ëœ ì •ìƒ ì‹ í˜¸ì™€ ë¹„ì •ìƒ ì‹ í˜¸ë¥¼ êµ¬ë¶„í•˜ê¸° ìœ„í•œ ì ì ˆí•œ thresholdë¥¼ ì°¾ì•„ë‚¼ ê²ƒì´ë‹¤. Implementationì ìš©í•´ ë³¼ ë°ì´í„°ëŠ” í„í”„ ì œì§€ ê³µì¥ì˜ Sheet breaks(ì¢…ì´ ì”¹í˜)ì— ê´€í•œ ì´ì§„ ë¼ë²¨ ë°ì´í„°ì´ë‹¤. ë°ì´í„° ì„¤ëª…ì— ë”°ë¥´ë©´ í•´ë‹¹ ê³µì¥ì—ì„œ í•œë²ˆ sheet breakê°€ ë°œìƒí•˜ë©´ ìˆ˜ì²œ ë‹¬ëŸ¬ì˜ ì†í•´ê°€ ë°œìƒí•œë‹¤ê³  í•˜ë©°, ì´ëŸ¬í•œ ì‚¬ê³ ê°€ ì ì–´ë„ ë§¤ì¼ í•œ ë²ˆ ì´ìƒ ë°œìƒí•œë‹¤ê³  í•œë‹¤.í•´ë‹¹ ë°ì´í„°ëŠ” 15ì¼ì¹˜ì— í•´ë‹¹í•˜ëŠ” 18,268 rowsë¥¼ ê°€ì§€ê³  ìˆìœ¼ë©° ì´ ì¤‘ sheet breakì— í•´ë‹¹í•˜ëŠ” positive labelì˜ ë¹„ìœ¨ì€ 124ê°œë¡œ ì „ì²´ ë°ì´í„°ì˜ 0.6%ë¥¼ ì°¨ì§€í•˜ê³  ìˆë‹¤.ë°ì´í„°ëŠ” ì—¬ê¸°ì—ì„œ ì‹ ì²­ í›„ ë°›ì„ ìˆ˜ ìˆë‹¤. 1. Import Libraries12345678910111213import pandas as pd import numpy as npimport matplotlib.pyplot as pltimport seaborn as snsfrom pylab import rcParamsfrom collections import Counterimport tensorflow as tffrom tensorflow.keras import Model ,models, layers, optimizers, regularizersfrom tensorflow.keras.callbacks import ModelCheckpointfrom sklearn.preprocessing import StandardScalerfrom sklearn.model_selection import train_test_splitfrom sklearn import metrics 2. Load Datatimeê³¼ ë¼ë²¨ yê°’ì„ ë¹¼ë©´ ì´ 61ê°œì˜ ì¹¼ëŸ¼ì„ ê°€ì§€ê³  ìˆë‹¤.1234LABELS = ['Normal', 'Break']df = pd.read_csv('./data/processminer-rare-event-mts-csv.csv')df.head() normal(0)ì´ 18,274ê±´, break(1)ê°€ 124ê±´ìœ¼ë¡œ êµ¬ì„± ë˜ì–´ìˆë‹¤.1Counter(df['y']) # Counter(&#123;0: 18274, 1: 124&#125;) 3. Curve Shiftingtime ì¹¼ëŸ¼ì„ ë³´ë©´ 2ë¶„ ë‹¨ìœ„ë¡œ ë°ì´í„°ê°€ ë‚˜ëˆ„ì–´ì ¸ ìˆëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ì—¬ê¸°ì„œì˜ ëª©í‘œëŠ” breakê°€ ë°œìƒí•˜ê¸° 4ë¶„ ì „ì— ì¡°ê¸° ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ë‹¤. ê·¸ëŸ¬ë¯€ë¡œ 4ë¶„ ì „ê¹Œì§€ì˜ ë°ì´í„°ë¥¼ break ë°ì´í„°ë¡œ ë§Œë“¤ê¸° ìœ„í•´ì„œëŠ” curve shiftingì„ 2ê°œì˜ rowë§Œí¼ë§Œ ì ìš©í•˜ë©´ ëœë‹¤. ì´í›„, ë³¸ë˜ break ë°ì´í„°ëŠ” ì œê±°í•œë‹¤.12345678910111213141516171819202122sign = lambda x: (1, -1)[x &lt; 0]def curve_shift(df, shift_by): vector = df['y'].copy() for s in range(abs(shift_by)): tmp = vector.shift(sign(shift_by)) tmp = tmp.fillna(0) vector += tmp labelcol = 'y' # Add vector to the df df.insert(loc=0, column=labelcol+'tmp', value=vector) # Remove the rows with labelcol == 1. df = df.drop(df[df[labelcol] == 1].index) # Drop labelcol and rename the tmp col as labelcol df = df.drop(labelcol, axis=1) df = df.rename(columns=&#123;labelcol+'tmp': labelcol&#125;) # Make the labelcol binary df.loc[df[labelcol] &gt; 0, labelcol] = 1 return df 123# shift the response column y by 2 rows to do a 4-min ahead predictionshifted_df = curve_shift(df, shift_by=-5)shifted_df.head() ëª‡ ê°€ì§€ ë¶ˆí•„ìš”í•œ ë°ì´í„°ëŠ” ì œê±°í•œ í›„, ë°ì´í„°ì™€ ë¼ë²¨ì„ ë¶„ë¦¬í•´ì¤€ë‹¤.12345678# drop remove columnsshifted_df = shifted_df.drop(['time','x28','x61'], axis=1)# x, yinput_x = shifted_df.drop('y', axis=1).valuesinput_y = shifted_df['y'].valuesn_features = input_x.shape[1] 4. Transform to Series DataLSTM ëª¨ë¸ì€ (samples, timesteps, feature)ì— í•´ë‹¹í•˜ëŠ” 3d ì°¨ì›ì˜ shapeì„ ê°€ì§€ë¯€ë¡œ, ë°ì´í„°ë¥¼ ì‹œí€€ìŠ¤ í˜•íƒœë¡œ ë³€í™˜í•œë‹¤. timestepsì€ 5(ì¦‰, 10ë¶„)ë§Œí¼ ì¡ì•˜ë‹¤.1234567891011def temporalize(X, y, timesteps): output_X = [] output_y = [] for i in range(len(X) - timesteps - 1): t = [] for j in range(1, timesteps + 1): # Gather the past records upto the lookback period t.append(X[[(i + j + 1)], :]) output_X.append(t) output_y.append(y[i + timesteps + 1]) return np.squeeze(np.array(output_X)), np.array(output_y) 12345timesteps = 5# Temporalizex, y = temporalize(input_x, input_y, timesteps)print(x.shape) # (18268, 5, 59) 5. Split Train / Valid / Testì´í›„, í›ˆë ¨, ê²€ì¦, í…ŒìŠ¤íŠ¸ ìš© ë°ì´í„°ë¡œ ë¶„ë¦¬í•œë‹¤. ê°ê° 11,691, 2,923, 3,654ê°œë¡œ ë‚˜ëˆ„ì–´ì£¼ì—ˆë‹¤.1234567# Split into train, valid, and test x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2)print(len(x_train)) # 11691print(len(x_valid)) # 2923print(len(x_test)) # 3654 LSTM Autoencoder í•™ìŠµ ì‹œì—ëŠ” Normal(0) ë°ì´í„°ë§Œìœ¼ë¡œ í•™ìŠµí•  ê²ƒì´ê¸° ë•Œë¬¸ì— ë°ì´í„°ë¡œ ë¶€í„° Normal(0)ê³¼ Break(1) ë°ì´í„°ë¥¼ ë¶„ë¦¬í•œë‹¤.123456# For training the autoencoder, split 0 / 1x_train_y0 = x_train[y_train == 0]x_train_y1 = x_train[y_train == 1]x_valid_y0 = x_valid[y_valid == 0]x_valid_y1 = x_valid[y_valid == 1] 6. Standardizeê°ê¸° ë‹¤ë¥¸ ë°ì´í„° íŠ¹ì„±ì˜ í‘œì¤€í™”ë¥¼ ìœ„í•´ z-score ì •ê·œí™”ì¸ scikit-learnì˜ StandardScaler()ë¥¼ ì ìš©í•˜ì˜€ë‹¤. í•´ë‹¹ í•¨ìˆ˜ë¥¼ ì ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” 2d í˜•íƒœì—¬ì•¼ í•˜ë¯€ë¡œ Flatten ê³¼ì •ì„ ê±°ì¹œ í›„ ìŠ¤ì¼€ì¼ì„ ì ìš©í•˜ì˜€ìœ¼ë©° ì´í›„ ë‹¤ì‹œ 3d í˜•íƒœë¡œ ë³€í™˜í•˜ì˜€ë‹¤.1234567891011def flatten(X): flattened_X = np.empty((X.shape[0], X.shape[2])) # sample x features array. for i in range(X.shape[0]): flattened_X[i] = X[i, (X.shape[1]-1), :] return(flattened_X)def scale(X, scaler): for i in range(X.shape[0]): X[i, :, :] = scaler.transform(X[i, :, :]) return X 123456scaler = StandardScaler().fit(flatten(x_train_y0))x_train_y0_scaled = scale(x_train_y0, scaler)x_valid_scaled = scale(x_valid, scaler)x_valid_y0_scaled = scale(x_valid_y0, scaler)x_test_scaled = scale(x_test, scaler) 7. Training LSTM AutoencoderëŒ€ì¹­ êµ¬ì¡°ì˜ Staked Autoencoder í˜•íƒœë¡œ LSTM Autoencoderë¥¼ êµ¬ì„±í•˜ì—¬ ì •ìƒ ë°ì´í„°ë¡œë§Œ êµ¬ì„± ëœ ë°ì´í„°ë¥¼ í†µí•´ ì´ 200 epoch í•™ìŠµì‹œì¼°ë‹¤.123epochs = 200batch = 128lr = 0.001 1234567891011lstm_ae = models.Sequential()# Encoderlstm_ae.add(layers.LSTM(32, activation='relu', input_shape=(timesteps, n_features), return_sequences=True))lstm_ae.add(layers.LSTM(16, activation='relu', return_sequences=False))lstm_ae.add(layers.RepeatVector(timesteps))# Decoderlstm_ae.add(layers.LSTM(16, activation='relu', return_sequences=True))lstm_ae.add(layers.LSTM(32, activation='relu', return_sequences=True))lstm_ae.add(layers.TimeDistributed(layers.Dense(n_features)))lstm_ae.summary() 1234567# compilelstm_ae.compile(loss='mse', optimizer=optimizers.Adam(lr))# fithistory = lstm_ae.fit(x_train_y0_scaled, x_train_y0_scaled, epochs=epochs, batch_size=batch, validation_data=(x_valid_y0_scaled, x_valid_y0_scaled)) 12345678910111213141516Train on 11314 samples, validate on 2830 samplesEpoch 1/20011314/11314 [==============================] - 4s 393us/sample - loss: 0.8505 - val_loss: 0.6345Epoch 2/20011314/11314 [==============================] - 1s 86us/sample - loss: 0.5249 - val_loss: 0.4738Epoch 3/20011314/11314 [==============================] - 1s 83us/sample - loss: 0.4049 - val_loss: 0.3784 : : : : Epoch 198/20011314/11314 [==============================] - 1s 94us/sample - loss: 0.1256 - val_loss: 0.1308Epoch 199/20011314/11314 [==============================] - 1s 97us/sample - loss: 0.1209 - val_loss: 0.1282Epoch 200/20011314/11314 [==============================] - 1s 96us/sample - loss: 0.1212 - val_loss: 0.1308 train lossì™€ valid loss ëª¨ë‘ 0.1ê·¼ì²˜ë¡œ ìˆ˜ë ´í•˜ê³  ìˆë‹¤.12345plt.plot(history.history['loss'], label='train loss')plt.plot(history.history['val_loss'], label='valid loss')plt.legend()plt.xlabel('Epoch'); plt.ylabel('loss')plt.show() 8. Threshold by Precision Recall Curvenormalê³¼ breakë¥¼ êµ¬ë¶„í•˜ê¸° ìœ„í•œ thresholdë¥¼ ì§€ì •í•˜ê¸° ìœ„í•´ precision recall curveë¥¼ ì ìš©í•œë‹¤. ì£¼ì˜í•´ì•¼í•  ê²ƒì€ ë””ì½”ë”© ëœ ì¬êµ¬ì„± ê²°ê³¼ê°€ ì•„ë‹Œ ì¬êµ¬ì„± ì†ì‹¤(reconstruction error)ì™€ ì‹¤ì œ ë¼ë²¨ ê°’ì„ ë¹„êµí•œë‹¤ëŠ” ê²ƒì´ë‹¤. 12345678910111213valid_x_predictions = lstm_ae.predict(x_valid_scaled)mse = np.mean(np.power(flatten(x_valid_scaled) - flatten(valid_x_predictions), 2), axis=1)error_df = pd.DataFrame(&#123;'Reconstruction_error':mse, 'True_class':list(y_valid)&#125;)precision_rt, recall_rt, threshold_rt = metrics.precision_recall_curve(error_df['True_class'], error_df['Reconstruction_error'])plt.figure(figsize=(8,5))plt.plot(threshold_rt, precision_rt[1:], label='Precision')plt.plot(threshold_rt, recall_rt[1:], label='Recall')plt.xlabel('Threshold'); plt.ylabel('Precision/Recall')plt.legend()plt.show() ì—¬ê¸°ì„œ thresholdì˜ ê²½ìš° Recallê³¼ Precisionì˜ ê°’ì´ êµì°¨ë˜ëŠ” ì§€ì ì„ ìµœì ì˜ threshold ì§€ì ìœ¼ë¡œ ì¡ì•˜ë‹¤.ì—¬ê¸°ì„œ ìµœì ì˜ thresholdëŠ” 0.407ì´ë‹¤.1234567# best position of thresholdindex_cnt = [cnt for cnt, (p, r) in enumerate(zip(precision_rt, recall_rt)) if p==r][0]print('precision: ',precision_rt[index_cnt],', recall: ',recall_rt[index_cnt])# fixed Thresholdthreshold_fixed = threshold_rt[index_cnt]print('threshold: ',threshold_fixed) 12precision: 0.10752688172043011 , recall: 0.10752688172043011threshold: 0.40777142413843237 9. Predict Testì´ì œ í…ŒìŠ¤íŠ¸ ì…‹ì— ì ìš©í•´ë³¼ ì°¨ë¡€ì´ë‹¤. í•™ìŠµí•˜ì˜€ë˜ LSTM Autoencoder ëª¨ë¸ì„ í†µí•´ í…ŒìŠ¤íŠ¸ ì…‹ì„ ì˜ˆì¸¡ í›„ ì¬êµ¬ì„± ì†ì‹¤ì„ ê³„ì‚°í•œë‹¤. ê·¸ í›„ ìœ„ì—ì„œ ì°¾ì€ thresholdë¥¼ ì ìš©í•˜ì—¬ Normalê³¼ Breakë¥¼ êµ¬ë¶„í•œë‹¤.123456789101112131415161718test_x_predictions = lstm_ae.predict(x_test_scaled)mse = np.mean(np.power(flatten(x_test_scaled) - flatten(test_x_predictions), 2), axis=1)error_df = pd.DataFrame(&#123;'Reconstruction_error': mse, 'True_class': y_test.tolist()&#125;)groups = error_df.groupby('True_class')fig, ax = plt.subplots()for name, group in groups: ax.plot(group.index, group.Reconstruction_error, marker='o', ms=3.5, linestyle='', label= \"Break\" if name == 1 else \"Normal\")ax.hlines(threshold_fixed, ax.get_xlim()[0], ax.get_xlim()[1], colors=\"r\", zorder=100, label='Threshold')ax.legend()plt.title(\"Reconstruction error for different classes\")plt.ylabel(\"Reconstruction error\")plt.xlabel(\"Data point index\")plt.show(); 10. Evaluationconfusion matrixí…ŒìŠ¤íŠ¸ ì…‹ì— ëŒ€í•œ ì¬êµ¬ì„± ì†ì‹¤ì„ thresholdë¥¼ ê¸°ì¤€ìœ¼ë¡œ 0/1ë¡œ ë‚˜ëˆ„ê³  ì´ë¥¼ confusion matrixë¡œ í‘œí˜„í•˜ì˜€ë‹¤.Breakì— ëŒ€í•œ ì˜ˆì¸¡ ê²°ê³¼ê°€ ì‹¤ë§ìŠ¤ëŸ¬ìš¸ ìˆ˜ ìˆì§€ë§Œ ì´ë ‡ê²Œ Sheet Breakì˜ 10%ë§Œ ì¤„ì—¬ë„ ì—„ì²­ë‚œ ì†ì‹¤ì„ ì¤„ì¼ ìˆ˜ ìˆë‹¤ê³  í•œë‹¤.123456789# classification by thresholdpred_y = [1 if e &gt; threshold_fixed else 0 for e in error_df['Reconstruction_error'].values]conf_matrix = metrics.confusion_matrix(error_df['True_class'], pred_y)plt.figure(figsize=(7, 7))sns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt='d')plt.title('Confusion Matrix')plt.xlabel('Predicted Class'); plt.ylabel('True Class')plt.show() ROC Curve and AUC123456789101112false_pos_rate, true_pos_rate, thresholds = metrics.roc_curve(error_df['True_class'], error_df['Reconstruction_error'])roc_auc = metrics.auc(false_pos_rate, true_pos_rate,)plt.plot(false_pos_rate, true_pos_rate, linewidth=5, label='AUC = %0.3f'% roc_auc)plt.plot([0,1],[0,1], linewidth=5)plt.xlim([-0.01, 1])plt.ylim([0, 1.01])plt.legend(loc='lower right')plt.title('Receiver operating characteristic curve (ROC)')plt.ylabel('True Positive Rate'); plt.xlabel('False Positive Rate')plt.show() 11. Resultìµœì¢…ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ì…‹ì— ëŒ€í•œ ì¬êµ¬ì„± ì†ì‹¤ì„ thresholdë¥¼ í†µí•´ êµ¬ë¶„í•œ pred_yì˜ ë§ˆì§€ë§‰ 5ë²ˆì§¸(timestepë§Œí¼)ì„ ì¶œë ¥í•˜ì—¬ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ìˆë‹¤.1pred_y[-5:] # [0, 0, 1, 0, 0] ìœ„ ê²°ê³¼ë¥¼ í•´ì„í•˜ê¸°ê°€ ì• ë§¤ëª¨í˜¸í•œ ë¶€ë¶„ì´ ìˆì§€ë§Œ, ëŒ€ëµ ë„“ê²Œ ì¡ì•˜ì„ ë•Œ ìµœì†Œ 10ë¶„ ì´ë‚´ì—ëŠ” Breakê°€ ë°œìƒí•  ê²ƒ ê°™ë‹¤ê³  í•´ì„í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. References Extreme Rare Event Classification using Autoencoders in Keras LSTM Autoencoder for Extreme Rare Event Classification in Keras Understanding LSTM Networks LSTM TimeDistributed layer How to connect LSTM layers in Keras, RepeatVector or return_sequence=True?","categories":[],"tags":[{"name":"deeplearning","slug":"deeplearning","permalink":"https://jaehyeongan.github.io/tags/deeplearning/"},{"name":"autoencoder","slug":"autoencoder","permalink":"https://jaehyeongan.github.io/tags/autoencoder/"},{"name":"lstm","slug":"lstm","permalink":"https://jaehyeongan.github.io/tags/lstm/"},{"name":"machinelearning","slug":"machinelearning","permalink":"https://jaehyeongan.github.io/tags/machinelearning/"},{"name":"lstmautoencoder","slug":"lstmautoencoder","permalink":"https://jaehyeongan.github.io/tags/lstmautoencoder/"},{"name":"prediction","slug":"prediction","permalink":"https://jaehyeongan.github.io/tags/prediction/"},{"name":"shifting","slug":"shifting","permalink":"https://jaehyeongan.github.io/tags/shifting/"},{"name":"windowing","slug":"windowing","permalink":"https://jaehyeongan.github.io/tags/windowing/"}]},{"title":"Autoencoderì™€ LSTM Autoencoder","slug":"Autoencoder-LSTMautoencoder","date":"2020-02-28T14:26:57.000Z","updated":"2020-10-28T15:14:34.000Z","comments":true,"path":"2020/02/28/Autoencoder-LSTMautoencoder/","link":"","permalink":"https://jaehyeongan.github.io/2020/02/28/Autoencoder-LSTMautoencoder/","excerpt":"","text":"IntroëŒ€í‘œì ì¸ ìê¸° ì§€ë„ í•™ìŠµì¸ Autoencoderì™€ Autoencoderì— LSTM cellì„ ì ìš©í•´ ì‹œí€€ìŠ¤ í•™ìŠµì´ ê°€ëŠ¥í•œ LSTM Autoencoderì— ëŒ€í•´ ì†Œê°œí•œë‹¤. ì´í›„ ë‹¤ìŒ í¬ìŠ¤íŒ…ì—ëŠ” LSTM Autoencoderë¥¼ í†µí•´ ë¯¸ë˜ì— ë°œìƒ í•  ê³ ì¥ì´ë‚˜ ì´ìƒì‹ í˜¸ë¥¼ ì¡°ê¸° ì˜ˆì¸¡í•˜ê¸° ìœ„í•œ Anomaly Detection ë°©ì•ˆì— ëŒ€í•´ ì†Œê°œí•  ê²ƒì´ë‹¤. 1. Autoencoder?ì˜¤í† ì¸ì½”ë”ëŠ”(autoencoder)ëŠ” ë¼ë²¨ì´ ì—†ëŠ” í›ˆë ¨ ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ í•™ìŠµ(ì¦‰, ì§€ë„ í•™ìŠµ) ì—†ì´ë„ ì…ë ¥ ë°ì´í„°ì˜ í‘œí˜„ì„ íš¨ìœ¨ì ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ì¸ê³µì‹ ê²½ë§ì´ë‹¤. ì˜¤í† ì¸ì½”ë”ëŠ” ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ input ë ˆì´ì–´, hidden ë ˆì´ì–´, output ë ˆì´ì–´ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©° ì¼ë°˜ì ìœ¼ë¡œ Input ìœ ë‹›ë³´ë‹¤ í›¨ì”¬ ë‚®ì€ ì°¨ì›ì˜ hidden ìœ ë‹›ì„ ê°€ì§€ë¯€ë¡œ ì£¼ë¡œ ì°¨ì› ì¶•ì†Œ(Dimensionality Reduction) ëª©ì ìœ¼ë¡œ ì‚¬ìš©ëœë‹¤. ë˜í•œ ì˜¤í† ì¸ì½”ë”ëŠ” ê°•ë ¥í•œ feature extractorë¡œ ì‘ë™í•˜ê¸° ë•Œë¬¸ì— ë¹„ì§€ë„ ì‚¬ì „í›ˆë ¨ì— ì‚¬ìš©ë  ìˆ˜ ìˆê³ , í›ˆë ¨ ë°ì´í„°ì™€ ë§¤ìš° ë¹„ìŠ·í•œ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ìƒì„± ëª¨ë¸(generative model)ë¡œì„œ ì‚¬ìš©ë  ìˆ˜ ìˆë‹¤. ì˜¤í† ì¸ì½”ë”ê°€ í•™ìŠµí•˜ëŠ” ê²ƒì€ ë‹¨ìˆœíˆ ì…ë ¥ì„ ì¶œë ¥ìœ¼ë¡œ ë³µì‚¬í•˜ëŠ” ê²ƒì´ë‹¤. í•˜ì§€ë§Œ ê·¸ ê³¼ì •ì—ì„œ ì—¬ëŸ¬ ë°©ë²•ì˜ ì œì•½(ë‚´ë¶€ í‘œí˜„ í¬ê¸° ì œí•œ, ì…ë ¥ ì¡ìŒ ì¶”ê°€ ë“±)ì„ í†µí•´ ì˜¤í† ì¸ì½”ë”ê°€ ë‹¨ìˆœíˆ ì…ë ¥ì„ ë°”ë¡œ ì¶œë ¥ìœ¼ë¡œ ë³µì‚¬í•˜ì§€ ëª»í•˜ë„ë¡ ë§‰ê³ , ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì¬í‘œí˜„(representation)í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•˜ë„ë¡ ì œì–´í•œë‹¤. ì˜¤í† ì¸ì½”ë”ëŠ” ì¸ì½”ë”(encoder)ì™€ ë””ì½”ë”(decoder)ë¡œ êµ¬ë¶„ëœë‹¤. ì¸ì½”ë”(encoder) : ì¸ì§€ ë„¤íŠ¸ì›Œí¬(recognition network)ë¼ê³ ë„ í•˜ë©°, ì…ë ¥ì„ ë‚´ë¶€ í‘œí˜„ìœ¼ë¡œ ë³€í™˜ ë””ì½”ë”(decoder) : ìƒì„± ë„¤íŠ¸ì›Œí¬(generative network)ë¼ê³ ë„ í•˜ë©°, ë‚´ë¶€ í‘œí˜„ì„ ì¶œë ¥ìœ¼ë¡œ ë³€í™˜ ì˜¤í† ì¸ì½”ë”ê°€ ì…ë ¥ì„ ì¬êµ¬ì„±í•˜ê¸° ë•Œë¬¸ì— ì¶œë ¥ì„ ì¬êµ¬ì„±(reconstruction)ì´ë¼ê³  ë¶€ë¥´ë©°, ì…ë ¥ê³¼ ì¬êµ¬ì„±ëœ ì¶œë ¥ê³¼ì˜ ì°¨ì´ë¥¼ ê³„ì‚°í•˜ì—¬ ì¬êµ¬ì„± ì†ì‹¤(reconstruction loss)ì´ë¼ê³  í•œë‹¤. ì—¬ê¸°ì„œ íŒŒë¼ë¯¸í„° (Î¸,Ï†)ëŠ” encoderì— ì…ë ¥ë˜ëŠ” original input (x)ê³¼ ë””ì½”ë”ë¥¼ í†µí•´ ì¶œë ¥ ëœ reconstruced input (f(g(x))ì´ ê°™ì•„ì§€ë„ë¡ í•™ìŠµí•˜ë©° ì—…ë°ì´íŠ¸ ëœë‹¤. 1.1. Stacked Autoencoderì—¬ëŸ¬ ê°œì˜ hidden ë ˆì´ì–´ë¥¼ ê°€ì§„ ê²½ìš°ë¥¼ ì ì¸µ ì˜¤í† ì¸ì½”ë”(stacked autoencoder)ë¼ê³  í•œë‹¤. ì•„ë˜ ê·¸ë¦¼ì™€ ê°™ì´ ë ˆì´ì–´ë¥¼ ë” ì¶”ê°€í•  ê²½ìš° ì˜¤í† ì¸ì½”ë”ëŠ” ë” ë³µì‘í•œ í‘œí˜„ì„ í•™ìŠµí•  ìˆ˜ ìˆê²Œ ë˜ë©° ì¼ë°˜ì ìœ¼ë¡œ ì ì¸µ ì˜¤í† ì¸ì½”ë”ëŠ” ì¶”ê°€ëœ hideen ë ˆì´ì–´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì¸ì½”ë”ì™€ ë””ì½”ë”ëŠ” ëŒ€ì¹­ êµ¬ì¡°ë¥¼ ì´ë£¬ë‹¤. ìœ„ì™€ ê°™ì´ ì˜¤í† ì¸ì½”ë”ê°€ ì™„ë²½í•˜ê²Œ ëŒ€ì¹­ êµ¬ì¡°ë¥¼ ì´ë£° ë•ŒëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì¸ì½”ë”ì™€ ë””ì½”ë”ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë¬¶ê²Œ ë˜ëŠ”ë° ì´ë ‡ê²Œ í•  ê²½ìš° ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ ìˆ˜ë¥¼ ì ˆë°˜ìœ¼ë¡œ ì¤„ì—¬ í›ˆë ¨ì†ë„ë¥¼ ë†’ì´ê³  overfitting ìœ„í—˜ì„ ì¤„ì¼ ìˆ˜ ìˆë‹¤ê³  í•œë‹¤. 1.2. Denoising Autoencoderìœ„ì—ì„œ ì‚´í´ë³´ì•˜ë˜ Stacked Autoencoderì˜ ê²½ìš° ë‹¤ìˆ˜ì˜ hidden ë ˆì´ì–´ì™€ ë…¸ë“œê°€ ì¶”ê°€ ë  ê²½ìš° overfitting ìì‹ ì— ëŒ€í•œ í‘œí˜„ì„ ì„¸ë°€í•˜ê²Œ í•™ìŠµí•˜ê²Œ ë˜ëŠ” overfitting ë¬¸ì œì— ì§ë©´í•  ìˆ˜ ìˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ í•œ ê°€ì§€ ë°©ë²•ìœ¼ë¡œ ì œì•ˆëœ ê²ƒì´ Denoising Autoencoder(Vincent et al. 2008)ì´ë‹¤. ì´ ëª¨ë¸ì€ ë§ ê·¸ëŒ€ë¡œ ëª¨ë¸ì— í•™ìŠµë˜ê¸° ì „ Input ë°ì´í„°ì— ì¡ìŒ(noise)ì„ ì£¼ì–´ ëª¨ë¸ì´ ë°ì´í„° í‘œí˜„ì„ í•™ìŠµí•˜ê¸° í˜ë“¤ê²Œ ë§Œë“ ë‹¤. ì´ë ‡ê²Œ í•˜ëŠ” ì´ìœ ëŠ” ëª¨ë¸ì„ ì¼ë°˜í™”í•˜ê¸° ìœ„í•œ ëª©ì ì´ë©°, ë…¸ì´ì¦ˆ ì¦‰, ì œì•½ì´ ìˆëŠ” ìƒí™©ì—ì„œë„ ë°ì´í„°ë¥¼ íš¨ìš¸ì ìœ¼ë¡œ ë³µì›í•˜ê¸° ìœ„í•¨ì´ë‹¤. ì´ë•Œ ì¡ìŒì„ ì£¼ê¸° ìœ„í•œ ë°©ë²•ì€ ì—¬ëŸ¬ê°€ì§€ê°€ ìˆì§€ë§Œ í•´ë‹¹ ë…¼ë¬¸ì—ì„œëŠ” ì•„ë˜ì™€ ê°™ì´ ë°ì´í„°ì˜ ì¼ë¶€ê°€ ì‚­ì œëœ input(x~) ë¥¼ ë„£ì–´ ì´ x~ê°€ ì¶œë ¥ ëœ reconstruced input(xâ€™)ê³¼ ìœ ì‚¬í•´ì§€ë„ë¡ í•™ìŠµí•˜ëŠ” ê²ƒì´ë‹¤. 2. LSTM AutoencoerLSTM AutoencoderëŠ” ì‹œí€€ìŠ¤(sequence) ë°ì´í„°ì— Encoder-Decoder LSTM ì•„í‚¤í…ì²˜ë¥¼ ì ìš©í•˜ì—¬ êµ¬í˜„í•œ ì˜¤í† ì¸ì½”ë”ì´ë‹¤. ì•„ë˜ ê·¸ë¦¼ì€ LSTM ì˜¤í† ì¸ì½”ë”ì˜ êµ¬ì¡°ì´ë©° ì…ë ¥ ì‹œí€€ìŠ¤ê°€ ìˆœì°¨ì ìœ¼ë¡œ ë“¤ì–´ì˜¤ê²Œ ë˜ê³ , ë§ˆì§€ë§‰ ì…ë ¥ ì‹œí€€ìŠ¤ê°€ ë“¤ì–´ì˜¨ í›„ ë””ì½”ë”ëŠ” ì…ë ¥ ì‹œí€€ìŠ¤ë¥¼ ì¬ìƒì„±í•˜ê±°ë‚˜ í˜¹ì€ ëª©í‘œ ì‹œí€€ìŠ¤ì— ëŒ€í•œ ì˜ˆì¸¡ì„ ì¶œë ¥í•œë‹¤. 2.1 Reconstruction LSTM Autoencoderì¬êµ¬ì„±(reconstruction)ì„ ìœ„í•œ LSTM Autoencoder êµ¬ì¡°ì´ë‹¤. ì¦‰, inputê³¼ ìµœëŒ€í•œ ìœ ì‚¬í•˜ê²Œ outputì„ ë””ì½”ë”©í•˜ë©°, LSTM í•™ìŠµì„ ìœ„í•´ ë°ì´í„°ë¥¼ ìš°ì„  (samples, timesteps, feature)ì™€ ê°™ì€ 3dí˜•íƒœë¡œ ë³€í™˜í•œë‹¤. input ë ˆì´ì–´ì˜ featureëŠ” 1ì°¨ì›ìœ¼ë¯€ë¡œ output ë ˆì´ì–´ë„ ë™ì¼í•œ ì°¨ì›ìœ¼ë¡œ êµ¬ì„±í•˜ì—¬ ì¶œë ¥ë˜ë„ë¡ í•œë‹¤. 1234import pandas as pd import numpy as npimport tensorflow as tffrom tensorflow.keras import Model ,models, layers, optimizers, utils 123456789101112131415161718192021# define input sequencesequence = np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])# reshape input into [samples, timesteps, features]n_in = len(sequence)sequence = sequence.reshape((1, n_in, 1))# define modelmodel = models.Sequential()model.add(layers.LSTM(100, activation='relu', input_shape=(n_in, 1)))model.add(layers.RepeatVector(n_in))model.add(layers.LSTM(100, activation='relu', return_sequences=True))model.add(layers.TimeDistributed(layers.Dense(1)))model.compile(optimizer='adam', loss='mse')# fit modelmodel.fit(sequence, sequence, epochs=300, verbose=0)# predictyhat = model.predict(sequence)yhat 123456789array([[[0.10559099], [0.20217314], [0.30041453], [0.39952287], [0.49908453], [0.5987617 ], [0.69832975], [0.7991052 ], [0.9024458 ]]], dtype=float32) 2.2 Prediction LSTM Autoencoderì‹œê³„ì—´ì  ì˜ˆì¸¡ì„ ìœ„í•œ LSTM êµ¬ì¡°ì´ë©° input ì‹œí€€ìŠ¤ëŠ” í˜„ì¬ ì‹œì (t) output ì‹œì ì€ (t+1)ë¡œ ë‘ì–´ í•œ ì‹œì  ì•ì„ í•™ìŠµí•˜ë„ë¡ ë°ì´í„°ë¥¼ êµ¬ì„±í•œë‹¤. ì—¬ê¸°ì„œ autoencoderëŠ” í•™ìŠµ ì‹œ encoderì—ëŠ” t ì‹œì ì´ ì…ë ¥ë˜ì§€ë§Œ decoding í›„ì—ëŠ” (t+1)ì‹œì ê³¼ reconstruction errorë¥¼ ê³„ì‚°í•˜ë©° ê²°êµ­ t ì‹œì ì´ t+1 ì‹œì ì„ í•™ìŠµí•˜ê²Œ ëœë‹¤.ê²°ê³¼ì ìœ¼ë¡œ ì˜ˆì¸¡ ê²°ê³¼ëŠ” 1ì´ ì…ë ¥ë˜ë©´ 2ì™€ ê°€ê¹Œìš´ ìˆ˜ë¥¼, 2ê°€ ì…ë ¥ë˜ë©´ 3ê³¼ ê°€ê¹Œìš´ ìˆ˜ë¥¼ ì˜ˆì¸¡í•˜ê²Œ ëœë‹¤.12345678910111213141516171819202122232425# define input sequenceseq_in = np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])# reshape input into [samples, timesteps, features]n_in = len(seq_in)seq_in = seq_in.reshape((1, n_in, 1))# prepare output sequenceseq_out = seq_in[:, 1:, :]n_out = n_in - 1# define model model = models.Sequential()model.add(layers.LSTM(100, activation='relu', input_shape=(n_in, 1)))model.add(layers.RepeatVector(n_out))model.add(layers.LSTM(100, activation='relu', return_sequences=True))model.add(layers.TimeDistributed(layers.Dense(1)))model.compile(optimizer='adam', loss='mse')# fit modelmodel.fit(seq_in, seq_out, epochs=300, verbose=0)# predictyhat = model.predict(seq_in)yhat 12345678array([[[0.16683361], [0.2898971 ], [0.403169 ], [0.5089176 ], [0.6094323 ], [0.7060289 ], [0.7997408 ], [0.89148134]]], dtype=float32) 2.3 Composite LSTM AutoencoderReconstructionê³¼ Prediction ëª¨ë¸ì„ í†µí•©í•œ ëª¨ë¸ì´ë‹¤. ëª¨ë¸ì˜ í†µí•©ì„ ìœ„í•´ ì˜ˆì œì—ì„œëŠ” keras functional apië¥¼ í™œìš©í•˜ì˜€ìœ¼ë©°, ê²°ê³¼ì ìœ¼ë¡œ ì¶œë ¥ ì‹œ reconstructionê²°ê³¼ì™€ predictionê²°ê³¼ê°€ í•¨ê»˜ ì¶œë ¥ëœë‹¤.123456789101112131415161718192021222324252627282930313233# define input sequenceseq_in = np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])# reshape input into [samples, timesteps, features]n_in = len(seq_in)seq_in = seq_in.reshape((1, n_in, 1))# prepare output sequenceseq_out = seq_in[:, 1:, :]n_out = n_in - 1# define encodervisible = layers.Input(shape=(n_in, 1))encoder = layers.LSTM(100, activation='relu')(visible)# define reconstruct decoderdecoder1 = layers.RepeatVector(n_in)(encoder)decoder1 = layers.LSTM(100, activation='relu', return_sequences=True)(decoder1)decoder1 = layers.TimeDistributed(layers.Dense(1))(decoder1)# define predict decoderdecoder2 = layers.RepeatVector(n_out)(encoder)decoder2 = layers.LSTM(100, activation='relu', return_sequences=True)(decoder2)decoder2 = layers.TimeDistributed(layers.Dense(1))(decoder2)# concat modelmodel = Model(inputs=visible, outputs=[decoder1, decoder2])model.compile(optimizer='adam', loss='mse')# utils.plot_model(model, show_shapes=True, to_file='composite_lstm_autoencoder.png')# fit model model.fit(seq_in, [seq_in, seq_out], epochs=300, verbose=0)# predictyhat = model.predict(seq_in)yhat 1234567891011121314151617[array([[[0.10127164], [0.19949059], [0.29943317], [0.39987874], [0.50023794], [0.60028654], [0.7000689 ], [0.79983366], [0.89999163]]], dtype=float32), array([[[0.19868489], [0.30206183], [0.3981459 ], [0.4989811 ], [0.600592 ], [0.7013527 ], [0.80077535], [0.8988221 ]]], dtype=float32)] References Hands-On Machine Learning with Scikit-Learn and TensorFlow Unsupervised Learning of Video Representations using LSTMs https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html https://machinelearningmastery.com/lstm-autoencoders/","categories":[],"tags":[{"name":"autoencoder","slug":"autoencoder","permalink":"https://jaehyeongan.github.io/tags/autoencoder/"},{"name":"rnn","slug":"rnn","permalink":"https://jaehyeongan.github.io/tags/rnn/"},{"name":"lstm","slug":"lstm","permalink":"https://jaehyeongan.github.io/tags/lstm/"},{"name":"reconstruction","slug":"reconstruction","permalink":"https://jaehyeongan.github.io/tags/reconstruction/"},{"name":"encoder","slug":"encoder","permalink":"https://jaehyeongan.github.io/tags/encoder/"},{"name":"decoder","slug":"decoder","permalink":"https://jaehyeongan.github.io/tags/decoder/"}]},{"title":"OpenCVë¥¼ í™œìš©í•œ ê¸°ì´ˆ ì´ë¯¸ì§€ ì²˜ë¦¬ with Python","slug":"OpenCVë¥¼-í™œìš©í•œ-ê¸°ì´ˆ-ì´ë¯¸ì§€-ì²˜ë¦¬","date":"2020-02-15T09:01:24.000Z","updated":"2020-10-28T15:14:34.000Z","comments":true,"path":"2020/02/15/OpenCVë¥¼-í™œìš©í•œ-ê¸°ì´ˆ-ì´ë¯¸ì§€-ì²˜ë¦¬/","link":"","permalink":"https://jaehyeongan.github.io/2020/02/15/OpenCVë¥¼-í™œìš©í•œ-ê¸°ì´ˆ-ì´ë¯¸ì§€-ì²˜ë¦¬/","excerpt":"","text":"Introë¨¸ì‹ ëŸ¬ë‹ ë¶„ì•¼ì—ì„œ ê°€ì¥ í™œë°œí•˜ê²Œ ì—°êµ¬ ë˜ê³  ìˆëŠ” ë¶„ì•¼ëŠ” ì•„ë¬´ë˜ë„ ì»´í“¨í„° ë¹„ì „(computer vision)ë¶„ì•¼ ì¸ ê²ƒ ê°™ë‹¤.ìµœê·¼ ì»¨ë³¼ë£¨ì…˜ ë„¤íŠ¸ì›Œí¬ ëª¨ë¸ë“¤ì€ feature extraction ëŠ¥ë ¥ì´ ë§¤ìš° ë›°ì–´ë‚˜ì„œ ì´ë¯¸ì§€ì— ì¶”ê°€ì ì¸ ì „ì²˜ë¦¬ ì‘ì—…ì„ í•˜ì§€ ì•Šë”ë¼ë„ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë‚´ê³  ìˆë‹¤. í•˜ì§€ë§Œ ê·¸ë ‡ë”ë¼ë„ ë” íš¨ê³¼ì ì¸ ëª¨ë¸ì„ ìœ„í•´ì„œëŠ” ì ìš©í•˜ê³ ì í•˜ëŠ” ëª©ì ì— ë§ê²Œ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì‘ì—…ì„ ê±°ì³ì•¼ í•˜ëŠ” ê²½ìš°ê°€ ìˆë‹¤. ì´ë²ˆ ê¸€ì€ â€˜íŒŒì´ì¬ì„ í™œìš©í•œ ë¨¸ì‹ ëŸ¬ë‹ ì¿¡ë¶ - CHAPTER 8 ì´ë¯¸ì§€ ë‹¤ë£¨ê¸°â€™ë¥¼ ì½ê³  ì •ë¦¬í•œ ê¸€ì´ë©°, OpenCVë¥¼ í™œìš©í•œ ë‹¤ì–‘í•œ ì´ë¯¸ì§€ ì²˜ë¦¬ ê¸°ìˆ ì— ëŒ€í•´ ì†Œê°œí•œë‹¤. 1. OpenCV ì„¤ì¹˜OpenCV(Open Source Computer Vision Libary)ëŠ” ì´ë¯¸ì§€ë¥¼ ë‹¤ë£¨ëŠ” ë¶„ì•¼ì—ì„œ ê°€ì¥ ë„ë¦¬ ì´ìš©ë˜ê³  ì¸ê¸° ìˆëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì´ë©°, ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ í¸ë¦¬í•œ ê¸°ëŠ¥ì„ ëŒ€ë¶€ë¶„ ë‹´ê³  ìˆë‹¤. ì•„ë˜ì˜ ëª…ë ¹ì–´ë¥¼ í†µí•´ ì„¤ì¹˜ê°€ ê°€ëŠ¥í•˜ë‹¤.1pip install opencv-python ì„¤ì¹˜ê°€ ì œëŒ€ë¡œ ë˜ì—ˆëŠ”ì§€ OpenCVë¥¼ importí•˜ì—¬ ë²„ì „ì„ í™•ì¸í•œë‹¤.12import cv2cv2.__version__ # 4.1.2 2. ì´ë¯¸ì§€ ë¡œë“œì—¬ê¸°ì„œ í™œìš©í•˜ëŠ” ìƒ˜í”Œ ì´ë¯¸ì§€ëŠ” í•´ë‹¹ ì±…ì˜ github ì—ì„œ ë‹¤ìš´ë°›ì„ ìˆ˜ ìˆë‹¤. ë¨¼ì € ì•ìœ¼ë¡œ ê³µí†µì ìœ¼ë¡œ ê³„ì† ì‚¬ìš© ë  ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„í¬íŠ¸í•œë‹¤.123import cv2import numpy as npimport matplotlib.pyplot as plt imread() ë©”ì†Œë“œë¥¼ í†µí•´ ì´ë¯¸ì§€ë¥¼ ë¡œë“œ í›„ matplotlibì„ í†µí•´ ì¶œë ¥í•´ë³¸ë‹¤.123image = cv2.imread('images/plane.jpg', cv2.IMREAD_GRAYSCALE)plt.imshow(); plt.show() ìœ„ ì´ë¯¸ì§€ì˜ type ë° shapeì„ ì¶œë ¥í•´ë³´ë©´ ì•„ë˜ì™€ ê°™ë‹¤.12image.type # numpy.ndarrayimage.shape # (2270, 3600) ì´ë¯¸ì§€ ë°ì´í„°ëŠ” ë³¸ë˜ ê°œë³„ ì›ì†Œë¡œ ì´ë£¨ì–´ì§„ í–‰ë ¬ì˜ ì§‘í•©ì´ë‹¤. ì—¬ê¸°ì„œ ê°œë³„ ì›ì†ŒëŠ” í”½ì…€(pixel)ì´ë¼ê³  í•  ìˆ˜ ìˆìœ¼ë©° ê°œë³„ ì›ì†Œì˜ ê°’ì€ í”½ì…€ì˜ ê°•ë„ë¼ê³  í•  ìˆ˜ ìˆë‹¤. ê·¸ë¦¬ê³  í”½ì…€ì˜ ê°•ë„ëŠ” 0(ê²€ì •)ë¶€í„° 255(í°ìƒ‰) ì‚¬ì´ì˜ ë²”ìœ„ë¥¼ ê°€ì§€ê³  ìˆë‹¤. ì´ë¯¸ì§€ë¥¼ í–‰ë ¬ ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ê²Œ ë˜ë©´ ì•„ë˜ì™€ ê°™ì´ í‘œí˜„ëœë‹¤.1image 1234567array([[140, 136, 146, ..., 132, 139, 134], [144, 136, 149, ..., 142, 124, 126], [152, 139, 144, ..., 121, 127, 134], ..., [156, 146, 144, ..., 157, 154, 151], [146, 150, 147, ..., 156, 158, 157], [143, 138, 147, ..., 156, 157, 157]], dtype=uint8) ì»¬ëŸ¬ë¥¼ ì´ë¯¸ì§€ë¥¼ ì½ê¸° ìœ„í•´ì„œëŠ” imread() ë©”ì†Œë“œì— cv2.IMREAD_COLOR ë§¤ê°œë³€ìˆ˜ë¥¼ ë„£ì–´ì£¼ë©´ ëœë‹¤. ê·¸ëŸ°ë° ì£¼ì˜í• ì ì€ OpenCVëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ì´ë¯¸ì§€ë¥¼ BGRíƒ€ì…ìœ¼ë¡œ ì½ëŠ”ë‹¤ëŠ” ê²ƒì´ë‹¤. í•˜ì§€ë§Œ Matplotlibë“± ëŒ€ë¶€ë¶„ì˜ ì´ë¯¸ì§€ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” RGBíƒ€ì…ì„ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— BGR RGBíƒ€ì…ìœ¼ë¡œ ë³€ê²½í•´ì£¼ëŠ” ê²ƒì´ ì¢‹ë‹¤. 12345678# ì»¬ëŸ¬ ì´ë¯¸ì§€ ë¡œë“œimage_bgr = cv2.imread('images/plane.jpg', cv2.IMREAD_COLOR)# RGBíƒ€ì…ìœ¼ë¡œ ë³€í™˜image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)# plotplt.imshow(); plt.show() 3. ì´ë¯¸ì§€ ì €ì¥OpenCVì˜ imwrite() ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ ì €ì¥í•  ìˆ˜ ìˆë‹¤. 12# ì´ë¯¸ì§€ ë¡œë“œ image = cv2.imread('images/plane.jpg', cv.IMREAD_GRAYSCALE) 12# ì´ë¯¸ì§€ ì €ì¥ cv2.imwrite('images/new_plane.jpg', image) 4. ì´ë¯¸ì§€ í¬ê¸° ë³€ê²½OpenCVì˜ resize() ë©”ì†Œë“œë¥¼ ì´ìš©í•˜ì—¬ ì´ë¯¸ì§€ í¬ê¸° ë³€ê²½ì´ ê°€ëŠ¥í•˜ë‹¤.256x256 í¬ê¸°ì˜ ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•œ í›„ ì´ë¥¼ 50x50 í¬ê¸°ì˜ ì´ë¯¸ì§€ë¡œ ë³€ê²½í•œ í›„ ì¶œë ¥í•´ë³¸ë‹¤.1image = cv2.imread('images/plane_256x256.jpg', cv2.IMREAD_GRAYSCALE) 123456789# ì´ë¯¸ì§€ í¬ê¸°ë¥¼ 50x50ìœ¼ë¡œ ë³€ê²½image_50x50 = cv2.resize(image, (50, 50))# ì¶œë ¥ fig, ax = plt.subplots(1,2, figsize=(10,5))ax[0].imshow(image, cmap='gray')ax[0].set_title('Original Image')ax[1].imshow(image_50x50, cmap='gray')ax[1].set_title('Resized Image') 5. ì´ë¯¸ì§€ ìë¥´ê¸°(crop)ì´ë¯¸ì§€ë¥¼ ìë¥´ê³  ì‹¶ì„ ê²½ìš° ë°°ì—´ ìŠ¬ë¼ì´ì‹±ì„ ì´ìš©í•˜ì—¬ ì›í•˜ëŠ” ë¶€ë¶„ë§Œ cropí•  ìˆ˜ ìˆë‹¤.1234567image = cv2.imread('images/plane_256x256.jpg', cv2.IMREAD_GRAYSCALE)# ì´ë¯¸ì§€ì˜ ëª¨ë“  í–‰ê³¼ ì—´ì˜ ì ˆë°˜ë§Œ ì„ íƒimage_cropped = image[:,:128]plt.imshow(image_cropped, cmap='gray')plt.show() 6. ì´ë¯¸ì§€ blur ì²˜ë¦¬ì´ë¯¸ì§€ë¥¼ íë¦¬ê²Œ í•˜ê¸° ìœ„í•´ì„œëŠ” ê° í”½ì…€ì„ ì£¼ë³€ í”½ì…€ì˜ í‰ê· ê°’ìœ¼ë¡œ ë³€í™˜í•˜ë©´ ë˜ë©°, ì´ë ‡ê²Œ ì£¼ë³€ í”½ì…€ì— ìˆ˜í–‰ë˜ëŠ” ì—°ì‚°ì„ ì»¤ë„(kernel)ì´ë¼ê³  í•œë‹¤. ì»¤ë„ì´ í´ìˆ˜ë¡ ì´ë¯¸ì§€ê°€ ë” ë¶€ë“œëŸ¬ì›Œì§€ê²Œ ëœë‹¤. 12# ì´ë¯¸ì§€ ë¡œë“œ image = cv2.imread('images/plane_256x256.jpg', cv2.IMREAD_GRAYSCALE) 12345# blur() : ê° í”½ì…€ì— ì»¤ë„ ê°œìˆ˜ì˜ ì—­ìˆ˜ë¥¼ ê³±í•˜ì—¬ ëª¨ë‘ ë”í•¨image_blurry = cv2.blur(image, (5,5)) # 5 x 5 ì»¤ë„ í‰ê· ê°’ìœ¼ë¡œ ì´ë¯¸ì§€ë¥¼ íë¦¬ê²Œ í•¨ plt.imshow(image_blurry, cmap='gray')plt.show() 100x100 ì»¤ë„ê³¼ ê°™ì´ í° ì»¤ë„ì„ ì ìš©í•  ê²½ìš° ì´ë¯¸ì§€ê°€ í›¨ì”¬ ë­‰ê°œì§€ê²Œ ëœë‹¤.1234image_very_blurry = cv2.blur(image, (100,100))plt.imshow(image_very_blurry, cmap='gray')plt.show() ì•„ë˜ì™€ ê°™ì´ ì»¤ë„ì„ ì§ì ‘ ì •ì˜í•œ í›„ filter2D() ë©”ì†Œë“œë¥¼ í†µí•´ ì´ë¯¸ì§€ì— ì ìš©í•˜ëŠ” ê²ƒë„ ê°€ëŠ¥í•˜ë‹¤.ìƒì„±ëœ ì»¤ë„ì„ ì´ë¯¸ì§€ì— ì ìš© ì‹œ ì¤‘ì•™ ì›ì†Œê°€ ë³€í™˜ë˜ëŠ” í”½ì…€ì´ë©°, ë‚˜ë¨¸ì§€ëŠ” ê·¸ í”½ì…€ì˜ ì´ì›ƒì´ ëœë‹¤.123# ì»¤ë„ ìƒì„± kernel = np.ones((10,10)) / 25.0 # ëª¨ë‘ ë”í•˜ë©´ 1ì´ ë˜ë„ë¡ ì •ê·œí™”kernel 12345678910array([[0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04], [0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04], [0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04], [0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04], [0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04], [0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04], [0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04], [0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04], [0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04], [0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04]]) 12345# filter2Dí•¨ìˆ˜ë¡œ ì»¤ë„ì„ ì´ë¯¸ì§€ì— ì§ì ‘ ì ìš© image_kernel = cv2.filter2D(image, -1, kernel)plt.imshow(image_kernel, cmap='gray')plt.show() ìì£¼ ì‚¬ìš©ë˜ëŠ” ë¸”ëŸ¬ í•¨ìˆ˜ë¡œ ê°€ìš°ì‹œì•ˆ ë¶„í¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬(GaussianBlur)ê°€ ìˆë‹¤. GaussianBlur() í•¨ìˆ˜ì˜ ì„¸ ë²ˆì§¸ ë§¤ê°œë³€ìˆ˜ëŠ” Xì¶•(ë„ˆë¹„) ë°©í–¥ì˜ í‘œì¤€í¸ì°¨ì´ë©°, 0ìœ¼ë¡œ ì§€ì •í•˜ë©´ ((ë„ˆë¹„-1)0.5-1)0.3+0.8ê³¼ ê°™ì´ ê³„ì‚°ëœë‹¤. 1234image_very_blurry = cv2.GaussianBlur(image, (5,5), 0) plt.imshow(image_very_blurry, cmap='gray')plt.show() 7. ì´ë¯¸ì§€ ì„ ëª…í•˜ê²Œ í‘œí˜„ëŒ€ìƒ í”½ì…€ì„ ê°•ì¡°í•˜ëŠ” ì»¤ë„ì„ ì •ì˜í•œ í›„ filter2D() ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ì— ì ìš©í•œë‹¤.123456789101112131415image = cv2.imread('images/plane_256x256.jpg', cv2.IMREAD_GRAYSCALE)# ì»¤ë„ ìƒì„±(ëŒ€ìƒì´ ìˆëŠ” í”½ì…€ì„ ê°•ì¡°)kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])# ì»¤ë„ ì ìš© image_sharp = cv2.filter2D(image, -1, kernel)fig, ax = plt.subplots(1,2, figsize=(10,5))ax[0].imshow(image, cmap='gray')ax[0].set_title('Original Image')ax[1].imshow(image_sharp, cmap='gray')ax[1].set_title('Sharp Image') 8. ì´ë¯¸ì§€ ëŒ€ë¹„ ë†’ì´ê¸°íˆìŠ¤í† ê·¸ë¨ í‰í™œí™”(Histogram Equalization)ì€ ê°ì²´ì˜ í˜•íƒœê°€ ë‘ë“œëŸ¬ì§€ë„ë¡ ë§Œë“¤ì–´ì£¼ëŠ” ì´ë¯¸ì§€ ì²˜ë¦¬ ë„êµ¬ì´ë©°, OpenCVì—ì„œëŠ” equalizeHist() ë©”ì†Œë“œë¥¼ í†µí•´ ì ìš©í•  ìˆ˜ ìˆë‹¤.1234567891011image = cv2.imread('images/plane_256x256.jpg', cv2.IMREAD_GRAYSCALE)# ì´ë¯¸ì§€ ëŒ€ë¹„ë¥¼ í–¥ìƒimage_enhanced = cv2.equalizeHist(image)# plotfig, ax = plt.subplots(1,2, figsize=(10, 5))ax[0].imshow(image, cmap='gray')ax[0].set_title('Original Image')ax[1].imshow(image_enhanced, cmap='gray')ax[1].set_title('Enhanced Image') ì»¬ëŸ¬ ì´ë¯¸ì§€ì˜ ê²½ìš° ë¨¼ì € YUV ì»¬ëŸ¬ í¬ë§·ìœ¼ë¡œ ë³€í™˜í•´ì•¼ í•œë‹¤. YëŠ” ë£¨ë§ˆ ë˜ëŠ” ë°ê¸°ì´ê³  Uì™€ VëŠ” ì»¬ëŸ¬ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. ë³€í™˜í•œ ë’¤ì— ìœ„ì™€ ë™ì¼í•˜ê²Œ equlizeHist() ë©”ì†Œë“œë¥¼ ì ìš©í•˜ê³  ë‹¤ì‹œ RGB í¬ë§·ìœ¼ë¡œ ë³€í™˜ í›„ ì¶œë ¥í•œë‹¤.123456789101112131415image_bgr = cv2.imread('images/plane.jpg')# YUV ì»¬ë¡œ í¬ë§·ìœ¼ë¡œ ë³€í™˜image_yuv = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2YUV)# íˆìŠ¤í† ê·¸ë¨ í‰í™œí™” ì ìš©image_yuv[:, :, 0] = cv2.equalizeHist(image_yuv[:, :, 0])# #RGBë¡œ ë³€í™˜image_rgb = cv2.cvtColor(image_yuv, cv2.COLOR_YUV2RGB)# plotfig, ax = plt.subplots(1,2, figsize=(12, 8))ax[0].imshow(image_bgr, cmap='gray')ax[0].set_title('Original Color Image')ax[1].imshow(image_rgb, cmap='gray')ax[1].set_title('Enhanced Color Image') 9. ì´ë¯¸ì§€ ì´ì§„í™”ì´ë¯¸ì§€ ì´ì§„í™”(ì„ê³„ì²˜ë¦¬)ëŠ” ì–´ë–¤ ê°’ë³´ë‹¤ í° ê°’ì„ ê°€ì§„ í”½ì…€ì„ í°ìƒ‰ìœ¼ë¡œ ë§Œë“¤ê³  ì‘ì€ ê°’ì„ ê°€ì§„ í”½ì…€ì€ ê²€ì€ìƒ‰ìœ¼ë¡œ ë§Œë“œëŠ” ê³¼ì •ì´ë‹¤. ë” ê³ ê¸‰ ê¸°ìˆ ì€ ì ì‘ì  ì´ì§„í™”(Adaptive Thresholding)ë¡œ, í”½ì…€ì˜ ì„ê³—ê°’ì´ ì£¼ë³€ í”½ì…€ì˜ ê°•ë„ì— ì˜í•´ ê²°ì •ëœë‹¤. ì´ëŠ” ì´ë¯¸ì§€ ì•ˆì˜ ì˜ì—­ë§ˆë‹¤ ë¹› ì¡°ê±´ì´ ë‹¬ë¼ì§ˆ ë•Œ ë„ì›€ì´ ëœë‹¤.1234567891011121314# ì´ë¯¸ì§€ ë¡œë“œ image_grey = cv2.imread('images/plane_256x256.jpg', cv2.IMREAD_GRAYSCALE)# Adaptive Thresholding ì ìš© max_output_value = 255 # ì¶œë ¥ í”½ì…€ ê°•ë„ì˜ ìµœëŒ€ê°’neighborhood_size = 99subtract_from_mean = 10image_binarized = cv2.adaptiveThreshold(image_grey, max_output_value, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, neighborhood_size, subtract_from_mean) adaptiveThreshold() í•¨ìˆ˜ì—ëŠ” ë„¤ ê°œì˜ ì¤‘ìš”í•œ ë§¤ê°œë³€ìˆ˜ê°€ ìˆë‹¤. max_output_value : ì¶œë ¥ í”½ì…€ ê°•ë„ì˜ ìµœëŒ“ê°’ ì €ì¥ cv2.ADAPTIVE_THRESH_GAUSSIAN_C : í”½ì…€ì˜ ì„ê³—ê°’ì„ ì£¼ë³€ í”½ì…€ ê°•ë„ì˜ ê°€ì¤‘ì¹˜ í•©ìœ¼ë¡œ ì„¤ì •. ê°€ì¤‘ì¹˜ëŠ” ê°€ìš°ì‹œì•ˆ ìœˆë„ìš°ì— ì˜í•´ ê²°ì • cv2.ADAPTIVE_THRESH_MEAN_C : ì£¼ë³€ í”½ì…€ì˜ í‰ê· ì„ ì„ê³—ê°’ìœ¼ë¡œ ì„¤ì • 123# plotplt.imshow(image_binarized, cmap='gray')plt.show() 10. ë°°ê²½ ì œê±°ë°°ê²½ì„ ì œê±°í•˜ê³ ì í•˜ëŠ” ì „ê²½ ì£¼ìœ„ì— ì‚¬ê°í˜• ë°•ìŠ¤ë¥¼ ê·¸ë¦¬ê³  ê·¸ë©ì»·(grabCut) ì•Œê³ ë¦¬ì¦˜ì„ ì ìš©í•˜ì—¬ ë°°ê²½ì„ ì œê±°í•œë‹¤.grabCutì˜ ê²½ìš° ì˜ ì‘ë™í•˜ë”ë¼ë„ ì—¬ì „íˆ ì´ë¯¸ì§€ì— ì œê±°í•˜ì§€ ëª»í•œ ë°°ê²½ì´ ë°œìƒí•  ìˆ˜ ìˆë‹¤. ì´ë ‡ê²Œ ì œê±° ë˜ì§€ ëª»í•œ ë¶€ë¶„ì€ ë‹¤ì‹œ ì ìš©í•˜ì—¬ ì œê±°í•  ìˆ˜ ìˆì§€ë§Œ ì‹¤ì „ì—ì„œ ìˆ˜ ì²œì¥ì˜ ì´ë¯¸ì§€ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ê³ ì¹˜ëŠ” ê²ƒì€ ë¶ˆê°€ëŠ¥í•œ ì¼ì´ë¯€ë¡œ ë¨¸ì‹ ëŸ¬ë‹ì„ ì ìš©í•œë‹¤ê±°ë‚˜ í•  ë•Œë„ ì¼ë¶€ëŸ¬ noiseë¥¼ ì ìš©í•˜ëŠ” ê²ƒì²˜ëŸ¼ ì¼ë¶€ ë°°ê²½ì´ ë‚¨ì•„ìˆëŠ” ê²ƒì„ ìˆ˜ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤.123# ì´ë¯¸ì§€ ë¡œë“œ í›„ RGBë¡œ ë³€í™˜image_bgr = cv2.imread('images/plane_256x256.jpg')image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB) 123456789101112131415161718# ì‚¬ê°í˜• ì¢Œí‘œ: ì‹œì‘ì ì˜ x,y ,ë„¢ì´, ë„ˆë¹„rectangle = (0, 56, 256, 150)# ì´ˆê¸° ë§ˆìŠ¤í¬ ìƒì„±mask = np.zeros(image_rgb.shape[:2], np.uint8)# grabCutì— ì‚¬ìš©í•  ì„ì‹œ ë°°ì—´ ìƒì„±bgdModel = np.zeros((1, 65), np.float64)fgdModel = np.zeros((1, 65), np.float64)# grabCut ì‹¤í–‰cv2.grabCut(image_rgb, # ì›ë³¸ ì´ë¯¸ì§€ mask, # ë§ˆìŠ¤í¬ rectangle, # ì‚¬ê°í˜• bgdModel, # ë°°ê²½ì„ ìœ„í•œ ì„ì‹œ ë°°ì—´ fgdModel, # ì „ê²½ì„ ìœ„í•œ ì„ì‹œ ë°°ì—´ 5, # ë°˜ë³µ íšŸìˆ˜ cv2.GC_INIT_WITH_RECT) # ì‚¬ê°í˜•ì„ ìœ„í•œ ì´ˆê¸°í™” 123456789# ë°°ê²½ì¸ ê³³ì€ 0, ê·¸ ì™¸ì—ëŠ” 1ë¡œ ì„¤ì •í•œ ë§ˆìŠ¤í¬ ìƒì„±mask_2 = np.where((mask==2) | (mask==0), 0, 1).astype('uint8')# ì´ë¯¸ì§€ì— ìƒˆë¡œìš´ ë§ˆìŠ¤í¬ë¥¼ ê³±í–‰ ë°°ê²½ì„ ì œì™¸image_rgb_nobg = image_rgb * mask_2[:, :, np.newaxis]# plotplt.imshow(image_rgb_nobg)plt.show() ìœ„ì—ì„œ ë¨¼ì € ì „ê²½ì´ ë“¤ì–´ìˆëŠ” ì˜ì—­ ì£¼ìœ„ë¥¼ ì‚¬ê°í˜•ìœ¼ë¡œ í‘œì‹œí•˜ì˜€ëŠ”ë°, grabCutì€ ì´ ì‚¬ê°í˜• ë°–ì— ìˆëŠ” ëª¨ë“  ê²ƒì´ ë°°ê²½ì´ë¼ê³  ê°€ì •í•˜ê³  ì´ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ê°í˜• ì•ˆì— ìˆëŠ” ë°°ê²½ì„ ì°¾ëŠ”ë‹¤.ì™¼ìª½ ê·¸ë¦¼ì˜ ê²€ì€ ì˜ì—­ì€ ë°°ê²½ì´ë¼ê³  í™•ì‹¤í•˜ê²Œ ê°€ì •í•œ ì‚¬ê°í˜•ì˜ ë°”ê¹¥ìª½ ì˜ì—­ì´ë©°, íšŒìƒ‰ ì˜ì—­ì€ ê·¸ë©ì»·ì´ ë°°ê²½ì´ë¼ê³  ìƒê°í•˜ëŠ” ì˜ì—­, ê·¸ë¦¬ê³  í°ìƒ‰ ì˜ì—­ì€ ì „ê²½ì´ë‹¤. ì˜¤ë¥¸ìª½ ê·¸ë¦¼ì€ ë‘ ë²ˆì§¸ ë§ˆìŠ¤í¬ë¥¼ ì´ë¯¸ì§€ì— ì ìš©í•˜ì—¬ ì „ê²½ë§Œ ë‚¨ê¸´ ì´ë¯¸ì§€ì´ë‹¤. 11. ê²½ê³„ì„  ê°ì§€Canny()ë©”ì†Œë“œë¥¼ í™œìš©í•˜ì—¬ ê²½ê³„ì„ ì„ ê°ì§€ í•  ìˆ˜ ìˆë‹¤. Canny()ë©”ì†Œë“œëŠ” ê·¸ë˜ë””ì–¸íŠ¸ ì„ê³—ê°’ ì‚¬ì´ì˜ ì €ì ê³¼ ê³ ì ì„ ë‚˜íƒ€ë‚´ëŠ” ë‘ ë§¤ê°œë³€ìˆ˜ë¥¼ í•„ìš”ë¡œ í•˜ë©°, ë‚®ì€ ì„ê³—ê°’ê³¼ ë†’ì€ ì„ê³—ê°’ ì‚¬ì´ì˜ ê°€ëŠ¥ì„± ìˆëŠ” ê²½ê³„ì„  í”½ì…€ì€ ì•½í•œ ê²½ê³„ì„  í”½ì…€ë¡œ ê°„ì£¼í•˜ê³ , ë†’ì€ ì„ê³—ê°’ë³´ë‹¤ í° í”½ì…€ì€ ê°•í•œ ê²½ê³„ì„  í”½ì…€ë¡œ ê°„ì£¼í•œë‹¤.123456789# ì´ë¯¸ì§€ ë¡œë“œimage_gray = cv2.imread('images/plane_256x256.jpg', cv2.IMREAD_GRAYSCALE)# í”½ì…€ ê°•ë„ì˜ ì¤‘ê°„ê°’ì„ ê³„ì‚°median_intensity = np.median(image_gray)# ì¤‘ê°„ í”½ì…€ ê°•ë„ì—ì„œ ìœ„ì•„ë˜ 1 í‘œì¤€í¸ì°¨ ë–¨ì–´ì§„ ê°’ì„ ì„ê³—ê°’ìœ¼ë¡œ ì§€ì •lower_threshold = int(max(0, (1.0 - 0.33) * median_intensity))upper_threshold = int(min(255, (1.0 + 0.33) * median_intensity)) 12345# Canny edge detection ì ìš©image_canny = cv2.Canny(image_gray, lower_threshold, upper_threshold)plt.imshow(image_canny, cmap='gray')plt.show()","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://jaehyeongan.github.io/tags/python/"},{"name":"opencv","slug":"opencv","permalink":"https://jaehyeongan.github.io/tags/opencv/"},{"name":"image","slug":"image","permalink":"https://jaehyeongan.github.io/tags/image/"}]},{"title":"AWS EC2ì— í”Œë¼ìŠ¤í¬(Flask) í´ë¼ìš°ë“œ ì›¹ ì„œë²„ êµ¬ì¶•í•˜ê¸°","slug":"aws-flask","date":"2020-01-13T12:26:58.000Z","updated":"2020-12-10T14:50:03.000Z","comments":true,"path":"2020/01/13/aws-flask/","link":"","permalink":"https://jaehyeongan.github.io/2020/01/13/aws-flask/","excerpt":"","text":"Intro ì§€ë‚œ ë²ˆ ê¸€ì—ì„œ Flask ì›¹ í”„ë ˆì„ì›Œí¬ë¥¼ í†µí•´ ê°„ë‹¨í•œ ë”¥ëŸ¬ë‹ ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ê°œë°œí•´ë³´ì•˜ë‹¤. í•˜ì§€ë§Œ ë¡œì»¬(local) í™˜ê²½ì—ì„œ ê°œë°œí•˜ì˜€ê¸° ë•Œë¬¸ì— ê°œë°œ ì„œë²„ë¥¼ ì¢…ì¼ ì¼œë†“ê±°ë‚˜ ê³ ì • ë„ë©”ì¸ì„ ë”°ë¡œ ë°›ì§€ ì•Šì€ ì´ìƒ ì™¸ë¶€ IPë¡œ ì ‘ê·¼ì€ ë¶ˆê°€ëŠ¥í•˜ë‹¤.ê·¸ë ‡ê¸° ë•Œë¬¸ì— ë‚˜ì²˜ëŸ¼ ë¬¼ë¦¬ì ì¸ ì„œë²„ë¥¼ êµ¬ì¶• ë° ìš´ì˜í•  í™˜ê²½ì´ ë˜ì§€ ì•Šì„ ê²½ìš°ëŠ” í´ë¼ìš°ë“œ(Cloud) ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í•˜ê²Œ ë˜ëŠ”ë°, ì´ë²ˆ ê¸€ì—ì„œëŠ” AWS(Amazon Web Services)ë¼ê³  í•˜ëŠ” í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤ë¥¼ í™œìš©í•˜ì—¬ ì›¹ ì„œë²„ë¥¼ êµ¬ì¶• í›„ Flaskë¥¼ ë°°í¬í•˜ëŠ” ê³¼ì •ì„ ì„¤ëª…í•˜ë ¤ê³  í•œë‹¤. 1. AWS EC2 ê°€ì… ë° ì¸ìŠ¤í„´ìŠ¤ ìƒì„±ìš°ì„  AWS Management Consolë¡œ ì´ë™ í›„ ê°€ì…ì´ ë˜ì–´ìˆì§€ ì•Šë‹¤ë©´ ê°€ì… í›„ ë¡œê·¸ì¸ì„ í•œë‹¤. (ê°€ì… ì‹œ regionì„ Seoulë¡œ ì„¤ì •í•  ê²ƒ)ì„œë¹„ìŠ¤ ê²€ìƒ‰ì„ í†µí•´ EC2ë¥¼ ì„ íƒí•œë‹¤. EC2 ëŒ€ì‹œë³´ë“œì—ì„œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì•„ë˜ì˜ ì¸ìŠ¤í„´ìŠ¤ ì‹œì‘ ë²„íŠ¼ì„ í´ë¦­ AMIë¡œëŠ” ê¸°ì—…ìš©ì´ ì•„ë‹ˆë‹ˆ ê°œì¸ ê°œë°œìš©ìœ¼ë¡œ í¸í•œ Ubuntu Linux 18.04 ë²„ì „ì„ ì‚¬ìš©í•˜ë©°,ë¬´ë£Œ ì„œë²„ ì´ìš©ì´ ê°€ëŠ¥í•œ í”„ë¦¬ í‹°ì–´(Free Tier)ë¡œ ì„œë²„ë¥¼ ìƒì„±í•œë‹¤. ìœ„ ì´ë¯¸ì§€ì—ì„œ ì‹œì‘ ë²„íŠ¼ì„ ëˆ„ë¥¼ ê²½ìš° í‚¤ í˜ì–´ë¥¼ ì„¤ì •í•˜ëŠ” ë©”ì‹œì§€ê°€ ë‚˜íƒ€ë‚˜ëŠ”ë° ì´ í‚¤ í˜ì–´ëŠ” ë§ ê·¸ëŒ€ë¡œ ìƒì„±í•œ ì›¹ ì„œë²„ì— ì¶”í›„ ì ‘ì†í•  ë•Œ ê¼­ í•„ìš”í•œ í‚¤ ì—­í• ì„ í•œë‹¤. â€˜ìƒˆ í‚¤ í˜ì–´ ìƒì„±â€™ì„ ì„ íƒí•˜ê³  â€˜í‚¤ í˜ì–´ ì´ë¦„â€™ì„ ë³¸ì¸ ì·¨í–¥ì— ë§ê²Œ ì„¤ì • í›„ â€˜í‚¤ í˜ì–´ ë‹¤ìš´ë¡œë“œâ€™ë¥¼ ì„ íƒí•œë‹¤. (ì´ í‚¤ í˜ì–´ëŠ” ì¶”í›„ ì„œë²„ ì ‘ì† ì‹œ ê¼­ í•„ìš”í•˜ë¯€ë¡œ ë³¸ì¸ ê°œë°œ í´ë”ì— ì˜ ë³´ê´€í•´ë‘”ë‹¤.)í‚¤ í˜ì–´ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ì¸ìŠ¤í„´ìŠ¤ ì‹œì‘ ë²„íŠ¼ì´ í™œì„±í™”ë˜ë©´ ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ì§„í–‰í•œë‹¤. ì¸ìŠ¤í„´ìŠ¤ ë³´ê¸°ë¥¼ ì„ íƒ ì—¬ê¸°ê¹Œì§€ê°€ ì§„í–‰í•˜ê²Œ ë˜ë©´ ì¸ìŠ¤í„´ìŠ¤ê°€ ì•„ë˜ì™€ ê°™ì´ ìƒì„±ëœë‹¤. 2. Key Pair ê¶Œí•œ ì„¤ì • ë³€ê²½ì „ ê³¼ì •ì—ì„œ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•˜ë©´ì„œ Key Pairë¥¼ ê°™ì´ ë‹¤ìš´ë¡œë“œ í•˜ì˜€ì„ ê²ƒì´ë‹¤. í•˜ì§€ë§Œ ì ‘ì†í•˜ê¸° ìœ„í•´ì„œëŠ” ì´ ê¶Œí•œ ì„¤ì •ì„ ë³€ê²½í•´ì¤˜ì•¼ë§Œ ì ‘ì†ì´ ê°€ëŠ¥í•˜ë‹¤. ìš°ì„  ë‹¤ìš´ë°›ì€ í‚¤í˜ì–´ë¥¼ ìš°í´ë¦­í•˜ì—¬ [ì†ì„±]-[ë³´ì•ˆ] íƒ­ìœ¼ë¡œ ì´ë™ í›„ [ê³ ê¸‰]ì„ í´ë¦­í•œë‹¤. ì•„ë˜ì™€ ê°™ì€ í™”ë©´ì—ì„œ,[ìƒì† ì‚¬ìš© ì•ˆ í•¨]ì„ í´ë¦­ í›„, íŒì—… ë©”ì‹œì§€ì—ì„œ â€˜ìƒì†ëœ ì‚¬ìš© ê¶Œí•œì„ ì´ ê°œì²´ì— ëŒ€í•œ ëª…ì‹œì  ì‚¬ìš© ê¶Œí•œìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤â€™ë¥¼ ì„ íƒ ì´í›„ ì•„ë˜ì™€ ê°™ì´ Administratorsë¥¼ ì œì™¸í•œ ëª¨ë“  ì‚¬ìš© ê¶Œí•œ í•­ëª©ì„ ì œê±°í•œë‹¤ 3. ë³´ì•ˆ ê·¸ë£¹ ì„¤ì •ì¸ìŠ¤í„´ìŠ¤ í™”ë©´ìœ¼ë¡œ ëŒì•„ì™€ Flask ì›¹ ì„œë²„ í¬íŠ¸ ë²ˆí˜¸ì¸ 5000ë²ˆ í¬íŠ¸ë¥¼ ì—´ì–´ ì£¼ê¸° ìœ„í•´ ë³´ì•ˆ ê·¸ë£¹ì„ ì„¤ì •í•œë‹¤. [ì¸ë°”ìš´ë“œ] íƒ­ì—ì„œ [í¸ì§‘]ì„ í´ë¦­ í›„ [ê·œì¹™ ì¶”ê°€]ë¥¼ í•˜ì—¬ ì•„ë˜ì™€ ê°™ì´ 5000ë²ˆ í¬íŠ¸ë¥¼ ì„¤ì •í•œë‹¤. 4. ì¸ìŠ¤í„´ìŠ¤ ì ‘ì†í•˜ê¸°ë‹¤ì‹œ ì¸ìŠ¤í„´ìŠ¤ í™”ë©´ìœ¼ë¡œ ëŒì•„ì™€ ì•„ë˜ í™”ë©´ì—ì„œ ìƒì„±í•œ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì„ íƒ í›„ ì—°ê²° ë²„íŠ¼ì„ í´ë¦­í•œë‹¤.ì—°ê²° ë°©ë²•ìœ¼ë¡œëŠ” â€˜ë…ë¦½ ì‹¤í–‰í˜• SSH í´ë¼ì´ì–¸íŠ¸â€™ë¡œ ì„ íƒí•˜ê³ , ì•„ë˜ ssh ëª…ë ¹ì–´ë¥¼ ë³µì‚¬í•œë‹¤. ëª…ë ¹í”„ë¡¬í”„íŠ¸(CMD)ë¥¼ ê´€ë¦¬ì ê¶Œí•œìœ¼ë¡œ ì‹¤í–‰ í›„ ë‹¤ìš´ ë°›ì€ Key Pairê°€ ìˆëŠ” ìœ„ì¹˜ë¡œ ì´ë™í•œë‹¤. ê·¸ í›„ ìœ„ì—ì„œ ë³µì‚¬í•œ SSH ëª…ë ¹ì–´ë¥¼ ë³µì‚¬í•˜ì—¬ ìš°ë¶„íˆ¬ ë¦¬ëˆ…ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ì— ì ‘ì†í•œë‹¤. ìš°ì„  íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë„êµ¬ì¸ pip ë° java jdk ë“±ì„ ì„¤ì¹˜í•´ì£¼ê³ , ë³¸ì¸ì˜ íŒŒì´ì¬ ì½”ë“œê°€ ìˆ˜í–‰ë˜ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•´ì¤€ë‹¤.12345678910111213$ sudo apt update# java ì„¤ì¹˜ $ sudo apt install openjdk-8-jre$ sudo apt install openjdk-8-jdk# pip ì„¤ì¹˜ ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜$ sudo apt install python3-pip$ sudo apt install tensorflow$ sudo apt install keras$ sudo apt install opencv-python$ sudo apt install scipy : ì´í›„ ê°œë°œí•œ flaskë¥¼ ì›¹ ì„œë²„ë¡œ cloneí•˜ì—¬ í•´ë‹¹ ê²½ë¡œë¡œ ì´ë™ í›„ ì›¹ ì„œë²„ë¥¼ ì‹¤í–‰í•´ì¤€ë‹¤. ì´ì œ ì›¹ ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì´ë‹ˆ í¼ë¸”ë¦­ IPë¡œ ì ‘ì†ì´ ê°€ëŠ¥í•˜ë‹¤. ì•„ë˜ ì¸ìŠ¤í„´ìŠ¤ í™”ë©´ì—ì„œ â€˜IPv4 í¼ë¸”ë¦­ IPâ€™ ì£¼ì†Œë¥¼ ë³µì‚¬ í›„ 5000ë²ˆ í¬íŠ¸ë²ˆí˜¸( http://54.180.150.154:5000/ )ë¡œ ì ‘ì†í•œë‹¤. ê³ ì • IPì—ì„œ ì„œë²„ê°€ ì˜ ì‹¤í–‰ë˜ê³  ìˆë‹¤. 5. íŒŒì´ì¬ ì„œë²„ ê³„ì† ì‹¤í–‰ ì‹œí‚¤ê¸°ìœ„ì™€ ê°™ì´ ì •ìƒì ìœ¼ë¡œ ê³ ì • IPë¥¼ í†µí•´ ì ‘ì†ì´ ê°€ëŠ¥í•¨ì„ í™•ì¸í•˜ì˜€ë‹¤. í•˜ì§€ë§Œ SSH í”„ë¡¬í”„íŠ¸ë¥¼ ì¢…ë£Œí•˜ê²Œ ë˜ë©´ íŒŒì´ì¬ ì„œë²„ë„ í•¨ê»˜ ì¢…ë£Œë˜ê²Œ ëœë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ íŒŒì´ì¬ ì„œë²„ê°€ í•­ìƒ ì‹¤í–‰ë  ìˆ˜ ìˆë„ë¡ ì„¤ì •í•œë‹¤. Ctrl+Z ë¥¼ í†µí•´ íŒŒì´ì¬ í”„ë¡œì„¸ìŠ¤ ì¤‘ì§€ $ bg : ë°±ê·¸ë¼ìš´ë“œì—ì„œ í”„ë¡œì„¸ìŠ¤ ì¬ êµ¬ë™ $ disown -h : ì†Œìœ ê¶Œ í¬ê¸° References https://ndb796.tistory.com/244","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://jaehyeongan.github.io/tags/python/"},{"name":"flask","slug":"flask","permalink":"https://jaehyeongan.github.io/tags/flask/"},{"name":"web","slug":"web","permalink":"https://jaehyeongan.github.io/tags/web/"},{"name":"amazon","slug":"amazon","permalink":"https://jaehyeongan.github.io/tags/amazon/"},{"name":"aws","slug":"aws","permalink":"https://jaehyeongan.github.io/tags/aws/"},{"name":"webservice","slug":"webservice","permalink":"https://jaehyeongan.github.io/tags/webservice/"},{"name":"cloud","slug":"cloud","permalink":"https://jaehyeongan.github.io/tags/cloud/"},{"name":"webframework","slug":"webframework","permalink":"https://jaehyeongan.github.io/tags/webframework/"}]},{"title":"íŒŒì´ì¬ ì›¹ í”„ë ˆì„ì›Œí¬ Flaskë¥¼ í™œìš©í•œ ë”¥ëŸ¬ë‹ ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œ","slug":"flask-deeplearning-webapplication","date":"2019-12-27T14:20:58.000Z","updated":"2020-12-10T15:19:35.000Z","comments":true,"path":"2019/12/27/flask-deeplearning-webapplication/","link":"","permalink":"https://jaehyeongan.github.io/2019/12/27/flask-deeplearning-webapplication/","excerpt":"","text":"IntroJavaì˜ Springì²˜ëŸ¼ Pythonì—ì„œë„ ì›¹ í”„ë ˆì„ì›Œí¬ë¥¼ ì œê³µí•œë‹¤. ê·¸ ì¤‘ ê°€ì¥ ì¸ê¸° ìˆëŠ” ê²ƒì´ Djangoì™€ Flaskì¸ë°, Djangoì˜ ê²½ìš° Instagram, LinkedIn ì‚¬ì´íŠ¸ë¡œ ì‚¬ìš©ë  ì •ë„ë¡œ ì¸ê¸° ìˆê³  ì•ˆì •ì ì¸ ì›¹ í”„ë ˆì„ì›Œí¬ë¼ê³  í•  ìˆ˜ ìˆë‹¤. ê·¸ ë§Œí¼ ì²´ê³„ì ì´ê³  ì •êµí•œ êµ¬ì¡°ë¥¼ ê°€ì§€ê³  ìˆë‹¤ê³  í•  ìˆ˜ ìˆëŠ”ë° ê·¸ì™€ ë°˜ëŒ€ë¡œ FlaskëŠ” ì¢€ ë” ê°„í¸í•˜ê³  ê²½ëŸ‰í™” ëœ ì›¹ í”„ë ˆì„ì›Œí¬ë¼ê³  ìƒê°í•˜ë©´ ë  ê²ƒ ê°™ë‹¤. ê·¸ë˜ì„œ ì‹¤ì œ ì„œë¹„ìŠ¤ í•˜ê¸° ë³´ë‹¤ëŠ” ê°„ë‹¨í•œ í”„ë¡œí† íƒ€ì… ê°œë°œ ìš©ë„ë¡œ ë§ì´ ì‚¬ìš©ë˜ëŠ” ê²ƒ ê°™ë‹¤. ì´ë²ˆì—ëŠ” Flask ì›¹ í”„ë ˆì„ì›Œí¬ì— ëŒ€í•´ ì•Œì•„ë³´ê³  ë”¥ëŸ¬ë‹ ëª¨ë¸ ì¤‘ Neural Style Transferë¥¼ Flaskì—ì„œ ì‹¤í–‰í•˜ì—¬ ê²°ê³¼ë¥¼ ì›¹ìœ¼ë¡œ í‘œì¶œí•´ë³´ë„ë¡ í•  ê²ƒì´ë‹¤. â€» í•´ë‹¹ ì „ì²´ ì½”ë“œëŠ” githubì—ì„œ í™•ì¸ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 1. Flask ì„¤ì¹˜1) flask í”„ë¡œì íŠ¸ í´ë” ìƒì„±ìš°ì„  flask í”„ë¡œì íŠ¸ë¥¼ ìˆ˜í–‰í•  í´ë”ë¥¼ ë³¸ì¸ì˜ ì„ì˜ ê²½ë¡œì— ì„¤ì¹˜í•´ì¤€ë‹¤. ë‚˜ëŠ” ì•„ë˜ì™€ ê°™ì€ ê²½ë¡œì— í´ë”ë¥¼ ë§Œë“¤ì—ˆë‹¤. 12cd&nbsp;workspaceworkspace&nbsp;&gt;&nbsp;mkdir&nbsp;pyflaskcs 2) ê°€ìƒí™˜ê²½flask í™˜ê²½ì„ ìœ„í•œ virtualenv ê°€ìƒí™˜ê²½ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•œë‹¤. 1pip&nbsp;install&nbsp;virtualenvcs ì„¤ì¹˜ ì™„ë£Œ í›„ ë³¸ì¸ì˜ flask ê²½ë¡œ ë‚´ì—ì„œ ê°€ìƒí™˜ê²½ì„ ìƒì„±í•´ì¤€ë‹¤. 1workspace\\pyflask&nbsp;&gt;&nbsp;virtualenv&nbsp;venvcs ì •ìƒì ìœ¼ë¡œ ì‹¤í–‰ ì‹œ flask í´ë” ë‚´ì— venv í´ë”ê°€ ìƒì„±ë˜ëŠ”ë° venv/Scripts í´ë”ë¡œ ì´ë™í•˜ì—¬ ê°€ìƒí™˜ê²½ì„ í™œì„±í™”(active) í•œë‹¤. 12workspace\\pyflask&nbsp;&gt;&nbsp;cd&nbsp;venv/Scriptsworkspace\\pyflask\\venv\\Scripts&nbsp;&gt;&nbsp;activatecs 3) Flask ì„¤ì¹˜ìœ„ì—ì„œ activeí•œ ê°€ìƒí™˜ê²½ ë‚´ì—ì„œ flaskë¥¼ ì„¤ì¹˜í•´ì¤€ë‹¤. 1(venv)&nbsp;pip&nbsp;install&nbsp;flaskcs ì„¤ì¹˜ì™„ë£Œ ëœ flaskì˜ ë²„ì „ì€ ì•„ë˜ì²˜ëŸ¼ í™•ì¸í•  ìˆ˜ ìˆë‹¤. 1flask&nbsp;--versioncs 2. ì›¹ êµ¬ì„±ì´ì œ Flaskë¥¼ ê°œë°œí•  í™˜ê²½ì€ êµ¬ì¶•í•˜ì˜€ìœ¼ë‹ˆ, ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ êµ¬ì¡°ë¥¼ ì„¤ê³„í•´ ë³¼ ê²ƒì´ë‹¤.ì—¬ê¸°ì„œ í”„ë¡œí† íƒ€ì…ìœ¼ë¡œ êµ¬ì„±í•´ ë³¼ ì›¬ ì• í”Œë¦¬ì¼€ì´ì…˜ì€ ê°„ë‹¨í•˜ê²Œ ë©”ì¸ í˜ì´ì§€ë¡œ êµ¬ì„±ë˜ê³ , ê° ê¸°ëŠ¥ì„ ìˆ˜í–‰í•˜ëŠ” ì„œë¸Œ í˜ì´ì§€ë¡œ ì´ë™í•˜ê²Œ ëœë‹¤. ì´í›„ ì‚¬ìš©ìë¡œ ë¶€í„° ì…ë ¥ê°’ì„ ë°›ì•„ ë”¥ëŸ¬ë‹ ê¸°ëŠ¥ ìˆ˜í–‰ í›„ ê·¸ ê²°ê³¼ê°’ì„ ë‹¤ì‹œ ì›¹ìœ¼ë¡œ ì¶œë ¥í•´ì£¼ëŠ” êµ¬ì¡°ë¥¼ ê°€ì§„ë‹¤. 123index&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(ë©”ì¸&nbsp;í˜ì´ì§€)â”œâ”€â”€&nbsp;nst_get&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(user&nbsp;input&nbsp;ë°›ëŠ”&nbsp;í˜ì´ì§€)â””â”€â”€&nbsp;nst_post&nbsp;&nbsp;&nbsp;&nbsp;(ê²°ê³¼&nbsp;ì¶œë ¥&nbsp;í˜ì´ì§€)cs 1) í´ë” êµ¬ì„±ìš°ì„  íš¨ìœ¨ì ì¸ ì›¹ ê°œë°œì„ ìœ„í•˜ì—¬ flask í´ë” ë‚´ì— ëª‡ ê°œì˜ í´ë”ë¥¼ ì„¤ì¹˜í•˜ì—¬ ê¸°ëŠ¥ë³„ë¡œ ê´€ë¦¬í•œë‹¤. 123456pyflask/â”œâ”€â”€&nbsp;static/&nbsp;&nbsp;&nbsp;&nbsp;â””â”€â”€&nbsp;images/â”œâ”€â”€&nbsp;templates/â”œâ”€â”€&nbsp;venv/â””â”€â”€&nbsp;neural_style_transfer.pycs static/images : ì‚¬ìš©ìë¡œë¶€í„° ë°›ì„ ì´ë¯¸ì§€ë¥¼ ì €ì¥í•  ê²½ë¡œ templates : html íŒŒì¼ neural_style_transfer.py : neural style transferë¥¼ ìˆ˜í–‰í•  ë”¥ëŸ¬ë‹ ì½”ë“œ 2) HTML í…œí”Œë¦¿ìš°ì„  í™”ë©´ êµ¬ì„±ì„ ìœ„í•˜ì—¬ HTML í…œí”Œë¦¿ì„ ì•„ë˜ì™€ ê°™ì´ ìµœëŒ€í•œ ê°„ë‹¨í•˜ê²Œ ì‘ì„±í•˜ì˜€ë‹¤.(ì§€ë©´ìƒ CSSì™€ JSì½”ë“œëŠ” ì œê±°í•˜ì˜€ëŠ”ë° ì „ì²´ ì½”ë“œëŠ” githubì—ì„œ í™•ì¸í•  ìˆ˜ ìˆë‹¤.) index.html (ë©”ì¸ í˜ì´ì§€) 1234567891011121314151617181920&lt;!DOCTYPE&nbsp;html&gt;&lt;html&nbsp;lang=\"ko\"&gt;&lt;head&gt;&nbsp;&nbsp;&nbsp;&nbsp;&lt;meta&nbsp;charset=\"UTF-8\"&gt;&nbsp;&nbsp;&nbsp;&nbsp;&lt;meta&nbsp;name=\"viewport\"&nbsp;content=\"width=device-width,&nbsp;initial-scale=1.0\"&gt;&nbsp;&nbsp;&nbsp;&nbsp;&lt;meta&nbsp;http-equiv=\"X-UA-Compatible\"&nbsp;content=\"ie=edge\"&gt;&nbsp;&nbsp;&nbsp;&nbsp;&lt;title&gt;Flask&nbsp;Index&lt;/title&gt;&lt;/head&gt;&nbsp;&lt;body&gt;&nbsp;&nbsp;&nbsp;&nbsp;&lt;br&gt;&nbsp;&nbsp;&nbsp;&nbsp;&lt;h1&nbsp;align=\"center\"&gt;Flask&nbsp;for&nbsp;Deep&nbsp;ConvNet&lt;/h1&gt;&nbsp;&nbsp;&nbsp;&nbsp;&lt;br&gt;&nbsp;&nbsp;&nbsp;&nbsp;&lt;ul&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;li&gt;&lt;h2&gt;&lt;a&nbsp;href=\"/nst_get\"&gt;Neural&nbsp;Style&nbsp;Transfer&lt;/a&gt;&lt;/h2&gt;&lt;/li&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;li&gt;&lt;h2&gt;&lt;a&nbsp;href=\"#\"&gt;Obejct&nbsp;Detection&lt;/a&gt;&lt;/h2&gt;&lt;/li&gt;&nbsp;&nbsp;&nbsp;&nbsp;&lt;/ul&gt;&lt;/body&gt;&lt;footer&nbsp;align='center'&gt;Powerd&nbsp;by&nbsp;&lt;strong&gt;Â©&nbsp;2019&nbsp;JaeHyeong&lt;/strong&gt;&lt;/footer&gt;&lt;/html&gt;Colored by Color Scriptercs â€” aíƒœê·¸ ë§í¬ì— /nst_getì„ ëª…ì‹œí•˜ì—¬ í´ë¦­ ì‹œ nst_get.htmlë¡œ ì´ë™ nst_get.html (user input ë°›ëŠ” í˜ì´ì§€) 1234567891011121314151617181920212223242526272829303132333435363738394041&lt;!DOCTYPE&nbsp;html&gt;&lt;html&nbsp;lang=\"ko\"&gt;&lt;head&gt;&nbsp;&nbsp;&nbsp;&nbsp;&lt;meta&nbsp;charset=\"UTF-8\"&gt;&nbsp;&nbsp;&nbsp;&nbsp;&lt;meta&nbsp;name=\"viewport\"&nbsp;content=\"width=device-width,&nbsp;initial-scale=1.0\"&gt;&nbsp;&nbsp;&nbsp;&nbsp;&lt;meta&nbsp;http-equiv=\"X-UA-Compatible\"&nbsp;content=\"ie=edge\"&gt;&nbsp;&nbsp;&nbsp;&nbsp;&lt;title&gt;Flask&nbsp;image&nbsp;get&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;br&gt;&nbsp;&nbsp;&nbsp;&nbsp;&lt;h1&nbsp;align=\"center\"&gt;Neural&nbsp;Sytle&nbsp;Transfer&lt;/h1&gt;&nbsp;&nbsp;&nbsp;&nbsp;&lt;br&gt;&nbsp;&nbsp;&nbsp;&nbsp;&lt;form&nbsp;align=\"center\"&nbsp;action=\"/nst_post\"&nbsp;method=\"POST\"&nbsp;enctype=\"multipart/form-data\"&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;h2&nbsp;align=\"center\"&gt;Reference&nbsp;Images&lt;/h2&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;table&nbsp;align=\"center\"&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;tr&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;td&gt;&lt;img&nbsp;class=\"refer_img\"&nbsp;id=\"refer_img1\"&nbsp;src=\"./static/images/rain_princess.jpg\"&gt;&lt;/td&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;td&gt;&lt;img&nbsp;class=\"refer_img\"&nbsp;id=\"refer_img2\"&nbsp;src=\"./static/images/the_stary_night.JPG\"&gt;&lt;/td&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;td&gt;&lt;img&nbsp;class=\"refer_img\"&nbsp;id=\"refer_img3\"&nbsp;src=\"./static/images/scream.jpg\"&gt;&lt;/td&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;td&gt;&lt;img&nbsp;class=\"refer_img\"&nbsp;id=\"refer_img3\"&nbsp;src=\"./static/images/zentangle_art.jpg\"&gt;&lt;/td&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;/tr&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;tr&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;td&gt;&lt;input&nbsp;type=\"radio\"&nbsp;name=\"refer_img\"&nbsp;value=\"rain_princess.jpg\"&gt;&lt;/td&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;td&gt;&lt;input&nbsp;type=\"radio\"&nbsp;name=\"refer_img\"&nbsp;value=\"the_stary_night.JPG\"&gt;&lt;/td&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;td&gt;&lt;input&nbsp;type=\"radio\"&nbsp;name=\"refer_img\"&nbsp;value=\"scream.jpg\"&gt;&lt;/td&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;td&gt;&lt;input&nbsp;type=\"radio\"&nbsp;name=\"refer_img\"&nbsp;value=\"zentangle_art.jpg\"&gt;&lt;/td&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;/tr&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;/table&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;br&gt;&lt;br&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;h2&nbsp;align=\"center\"&gt;Target&nbsp;Image&lt;/h2&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;div&nbsp;align=\"center\"&nbsp;id='view_area'&gt;&lt;/div&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;br&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;input&nbsp;type=\"file\"&nbsp;name=\"user_img\"&nbsp;id=\"user_img\"&nbsp;value=\"userIMgage\"&nbsp;onchange=\"previewImage(this,'view_area')\"/&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;input&nbsp;type=\"submit\"&nbsp;value=\"í™•ì¸\"/&gt;&nbsp;&nbsp;&nbsp;&nbsp;&lt;/form&gt;&nbsp;&nbsp;&nbsp;&nbsp;&lt;br&gt;&lt;br&gt;&lt;br&gt;&nbsp;&lt;/body&gt;&lt;/html&gt;&nbsp;&nbsp;Colored by Color Scriptercs â€” ì´ë¯¸ì§€ë¥¼ Flaskë¡œ ë„˜ê²¨ì£¼ëŠ” í˜ì´ì§€ì´ë¯€ë¡œ form íƒœê·¸ì˜ POST ë°©ì‹ì„ ìˆ˜í–‰â€” neural style transfer í•™ìŠµì„ ìœ„í•œ reference ì´ë¯¸ì§€ ê²½ë¡œë¥¼ ì§€ì •í•˜ì—¬ í‘œì‹œâ€” ì‚¬ìš©ìë¡œë¶€í„° ì´ë¯¸ì§€ íŒŒì¼ì„ ì…ë ¥ ë°›ìŒâ€” í™•ì¸ ë²„íŠ¼ì„ í†µí•´ ì„ íƒí•œ reference ì´ë¯¸ì§€ì™€ ì‚¬ìš©ì ì´ë¯¸ì§€ë¥¼ ì „ì†¡ nst_post.html (ê²°ê³¼ ì¶œë ¥ í˜ì´ì§€) 1234567891011121314151617181920212223242526272829303132333435363738394041&lt;!DOCTYPE&nbsp;html&gt;&lt;html&nbsp;lang=\"ko\"&gt;&lt;head&gt;&nbsp;&nbsp;&nbsp;&nbsp;&lt;meta&nbsp;charset=\"UTF-8\"&gt;&nbsp;&nbsp;&nbsp;&nbsp;&lt;meta&nbsp;name=\"viewport\"&nbsp;content=\"width=device-width,&nbsp;initial-scale=1.0\"&gt;&nbsp;&nbsp;&nbsp;&nbsp;&lt;meta&nbsp;http-equiv=\"X-UA-Compatible\"&nbsp;content=\"ie=edge\"&gt;&nbsp;&nbsp;&nbsp;&nbsp;&lt;title&gt;Flask&nbsp;image&nbsp;get&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;br&gt;&nbsp;&nbsp;&nbsp;&nbsp;&lt;h1&nbsp;align=\"center\"&gt;Neural&nbsp;Sytle&nbsp;Transfer&lt;/h1&gt;&nbsp;&nbsp;&nbsp;&nbsp;&lt;br&gt;&nbsp;&nbsp;&nbsp;&nbsp;&lt;form&nbsp;align=\"center\"&nbsp;action=\"/nst_post\"&nbsp;method=\"POST\"&nbsp;enctype=\"multipart/form-data\"&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;h2&nbsp;align=\"center\"&gt;Reference&nbsp;Images&lt;/h2&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;table&nbsp;align=\"center\"&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;tr&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;td&gt;&lt;img&nbsp;class=\"refer_img\"&nbsp;id=\"refer_img1\"&nbsp;src=\"./static/images/rain_princess.jpg\"&gt;&lt;/td&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;td&gt;&lt;img&nbsp;class=\"refer_img\"&nbsp;id=\"refer_img2\"&nbsp;src=\"./static/images/the_stary_night.JPG\"&gt;&lt;/td&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;td&gt;&lt;img&nbsp;class=\"refer_img\"&nbsp;id=\"refer_img3\"&nbsp;src=\"./static/images/scream.jpg\"&gt;&lt;/td&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;td&gt;&lt;img&nbsp;class=\"refer_img\"&nbsp;id=\"refer_img3\"&nbsp;src=\"./static/images/zentangle_art.jpg\"&gt;&lt;/td&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;/tr&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;tr&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;td&gt;&lt;input&nbsp;type=\"radio\"&nbsp;name=\"refer_img\"&nbsp;value=\"rain_princess.jpg\"&gt;&lt;/td&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;td&gt;&lt;input&nbsp;type=\"radio\"&nbsp;name=\"refer_img\"&nbsp;value=\"the_stary_night.JPG\"&gt;&lt;/td&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;td&gt;&lt;input&nbsp;type=\"radio\"&nbsp;name=\"refer_img\"&nbsp;value=\"scream.jpg\"&gt;&lt;/td&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;td&gt;&lt;input&nbsp;type=\"radio\"&nbsp;name=\"refer_img\"&nbsp;value=\"zentangle_art.jpg\"&gt;&lt;/td&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;/tr&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;/table&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;br&gt;&lt;br&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;h2&nbsp;align=\"center\"&gt;Target&nbsp;Image&lt;/h2&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;div&nbsp;align=\"center\"&nbsp;id='view_area'&gt;&lt;/div&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;br&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;input&nbsp;type=\"file\"&nbsp;name=\"user_img\"&nbsp;id=\"user_img\"&nbsp;value=\"userIMgage\"&nbsp;onchange=\"previewImage(this,'view_area')\"/&gt;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;input&nbsp;type=\"submit\"&nbsp;value=\"í™•ì¸\"/&gt;&nbsp;&nbsp;&nbsp;&nbsp;&lt;/form&gt;&nbsp;&nbsp;&nbsp;&nbsp;&lt;br&gt;&lt;br&gt;&lt;br&gt;&nbsp;&lt;/body&gt;&lt;/html&gt;&nbsp;&nbsp;Colored by Color Scriptercs â€” Flaskë¡œ ë¶€í„° ë„˜ê²¨ ë°›ì€ ê²°ê³¼ ì´ë¯¸ì§€ë¥¼ ë°›ì•„ì„œ ì¶œë ¥ 3) Neural Style Transfer ìˆ˜í–‰ ì½”ë“œ ì‘ì„±ì „ì²´ ì½”ë“œëŠ” github ì°¸ì¡° 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162def&nbsp;preprocess_image(image_path):&nbsp;&nbsp;&nbsp;&nbsp;img&nbsp;=&nbsp;load_img(image_path,&nbsp;target_size=(img_height,&nbsp;img_width))&nbsp;#&nbsp;(400,&nbsp;381)&nbsp;&nbsp;&nbsp;&nbsp;img&nbsp;=&nbsp;img_to_array(img)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;(400,&nbsp;381,&nbsp;3)&nbsp;&nbsp;&nbsp;&nbsp;img&nbsp;=&nbsp;np.expand_dims(img,&nbsp;axis=0)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;(1,&nbsp;400,&nbsp;381,&nbsp;3)&nbsp;&nbsp;&nbsp;&nbsp;img&nbsp;=&nbsp;vgg19.preprocess_input(img)&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;img&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:&nbsp;(&nbsp;ì¤‘ëµ&nbsp;)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:&nbsp;def&nbsp;main(refer_img_path,&nbsp;target_img_path):&nbsp;&nbsp;&nbsp;&nbsp;style_reference_image_path&nbsp;=&nbsp;'flask_deep/static/'+&nbsp;refer_img_path&nbsp;&nbsp;&nbsp;&nbsp;target_image_path&nbsp;=&nbsp;'flask_deep/static/'+&nbsp;target_img_path&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;ëª¨ë“ &nbsp;ì´ë¯¸ì§€ë¥¼&nbsp;fixed-size(400pixel)ë¡œ&nbsp;ë³€ê²½&nbsp;&nbsp;&nbsp;&nbsp;width,&nbsp;height&nbsp;=&nbsp;load_img(target_image_path).size&nbsp;&nbsp;&nbsp;&nbsp;global&nbsp;img_height;&nbsp;global&nbsp;img_width;&nbsp;&nbsp;&nbsp;&nbsp;img_height&nbsp;=&nbsp;400&nbsp;&nbsp;&nbsp;&nbsp;img_width&nbsp;=&nbsp;int(width&nbsp;*&nbsp;img_height&nbsp;/&nbsp;height)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;target_image&nbsp;=&nbsp;K.constant(preprocess_image(target_image_path))&nbsp;#&nbsp;creates&nbsp;img&nbsp;to&nbsp;a&nbsp;constant&nbsp;tensor&nbsp;&nbsp;&nbsp;&nbsp;style_reference_image&nbsp;=&nbsp;K.constant(preprocess_image(style_reference_image_path))&nbsp;&nbsp;&nbsp;&nbsp;combination_image&nbsp;=&nbsp;K.placeholder((1,&nbsp;img_height,&nbsp;img_width,&nbsp;3))&nbsp;#&nbsp;ìƒì„±ëœ&nbsp;ì´ë¯¸ì§€ë¥¼&nbsp;ë‹´ì„&nbsp;placeholder&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;3ê°œì˜&nbsp;ì´ë¯¸ì§€ë¥¼&nbsp;í•˜ë‚˜ì˜&nbsp;ë°°ì¹˜ë¡œ&nbsp;í•©ì¹¨&nbsp;&nbsp;&nbsp;&nbsp;input_tensor&nbsp;=&nbsp;K.concatenate([target_image,&nbsp;style_reference_image,&nbsp;combination_image],&nbsp;axis=0)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;3ê°œ&nbsp;ì´ë¯¸ì§€ì˜&nbsp;ë°°ì¹˜ë¥¼&nbsp;ì…ë ¥ìœ¼ë¡œ&nbsp;ë°›ëŠ”&nbsp;VGGNet&nbsp;ìƒì„±&nbsp;&nbsp;&nbsp;&nbsp;model&nbsp;=&nbsp;vgg19.VGG19(input_tensor=input_tensor,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;weights='imagenet',&nbsp;#&nbsp;pre-trained&nbsp;ImageNet&nbsp;ê°€ì¤‘ì¹˜&nbsp;ë¡œë“œ&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;include_top=False)&nbsp;#&nbsp;FC&nbsp;layer&nbsp;ì œì™¸&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:&nbsp;(&nbsp;ì¤‘ëµ&nbsp;)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;evaluator&nbsp;=&nbsp;Evaluator()&nbsp;&nbsp;&nbsp;&nbsp;refer_img_name&nbsp;=&nbsp;refer_img_path.split('.')[0].split('/')[-1]&nbsp;&nbsp;&nbsp;&nbsp;result_prefix&nbsp;=&nbsp;'flask_deep/static/images/nst_result_'+refer_img_name&nbsp;&nbsp;&nbsp;&nbsp;iterations&nbsp;=&nbsp;30&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;ë‰´ëŸ´&nbsp;ìŠ¤íƒ€ì¼&nbsp;íŠ¸ëœìŠ¤í¼ì˜&nbsp;ì†ì‹¤ì„&nbsp;ìµœì†Œí™”í•˜ê¸°&nbsp;ìœ„í•´&nbsp;ìƒì„±ëœ&nbsp;ì´ë¯¸ì§€ì—&nbsp;ëŒ€í•´&nbsp;L-BFGS&nbsp;ìµœì í™”ë¥¼&nbsp;ìˆ˜í–‰&nbsp;&nbsp;&nbsp;&nbsp;x&nbsp;=&nbsp;preprocess_image(target_image_path)&nbsp;&nbsp;&nbsp;&nbsp;x&nbsp;=&nbsp;x.flatten()&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;i&nbsp;in&nbsp;range(iterations):&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;start_time&nbsp;=&nbsp;time.time()&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;x,&nbsp;min_val,&nbsp;info&nbsp;=&nbsp;fmin_l_bfgs_b(evaluator.loss,&nbsp;x,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;fprime=evaluator.grads,&nbsp;maxfun=20)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;ìƒì„±ëœ&nbsp;í˜„ì¬&nbsp;ì´ë¯¸ì§€ë¥¼&nbsp;ì €ì¥&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;img&nbsp;=&nbsp;x.copy().reshape((img_height,&nbsp;img_width,&nbsp;3))&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;img&nbsp;=&nbsp;deprocess_image(img)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;fname&nbsp;=&nbsp;result_prefix&nbsp;+&nbsp;'.png'&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;end_time&nbsp;=&nbsp;time.time()&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;save_img(fname,&nbsp;img)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;fname&nbsp;if&nbsp;__name__&nbsp;==&nbsp;\"__main__\":&nbsp;&nbsp;&nbsp;&nbsp;main()Colored by Color Scriptercs 4) Flask app íŒŒì¼ ìƒì„±Flask ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰ì„ ìœ„í•œ _init_.py íŒŒì¼ì„ ìƒì„±í•œë‹¤. ì—¬ê¸°ì„œ Flask íŒŒë¼ë¯¸í„°ë¡œ ì „ë‹¬ë˜ëŠ” __name\\ íŒŒë¼ë¯¸í„°ëŠ” Flask ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ë¶„í•˜ê¸° ìœ„í•œ êµ¬ë¶„ìë¡œ ì‚¬ìš©ëœë‹¤.app.debug ë¥¼ Trueë¡œ ì§€ì •í•  ê²½ìš° ì½”ë“œ ìˆ˜ì • ì‹œ ë°”ë¡œë°”ë¡œ ë””ë²„ê¹…ì´ ê°€ëŠ¥í•˜ê²Œ í•´ì¤€ë‹¤. @app.routeëŠ” í˜ì´ì§€ URLê³¼ í•¨ìˆ˜ë¥¼ ì—°ê²°í•´ì£¼ëŠ” ì—­í• ì„ í•˜ë©° ì•„ë˜ì™€ ê°™ì´ @app.route ë°ì½”ë ˆì´í„° ì§€ì • í›„ render_template(â€˜URLâ€™)ì„ í†µí•´ ì—°ê²°í•  í˜ì´ì§€ ê²½ë¡œë¥¼ ì…ë ¥í•˜ë©´ í•´ë‹¹ ê²½ë¡œë¥¼ ì›¹ ë¸Œë¼ìš°ì €ë¡œ ì „ë‹¬í•´ì£¼ê²Œ ëœë‹¤. _init_.py 123456789101112131415161718192021222324252627282930313233343536import&nbsp;os,&nbsp;sysfrom&nbsp;flask&nbsp;import&nbsp;Flask,&nbsp;escape,&nbsp;request,&nbsp;&nbsp;Response,&nbsp;g,&nbsp;make_responsefrom&nbsp;flask.templating&nbsp;import&nbsp;render_templatefrom&nbsp;werkzeug&nbsp;import&nbsp;secure_filenamefrom&nbsp;.&nbsp;import&nbsp;neural_style_transfer&nbsp;app&nbsp;=&nbsp;Flask(__name__)app.debug&nbsp;=&nbsp;True&nbsp;#&nbsp;Main&nbsp;page@app.route('/')def&nbsp;index():&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;render_template('index.html')&nbsp;@app.route('/nst_get')def&nbsp;nst_get():&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;render_template('nst_get.html')&nbsp;@app.route('/nst_post',&nbsp;methods=['GET','POST'])def&nbsp;nst_post():&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;request.method&nbsp;==&nbsp;'POST':&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Reference&nbsp;Image&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;refer_img&nbsp;=&nbsp;request.form['refer_img']&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;refer_img_path&nbsp;=&nbsp;'static/images/'+str(refer_img)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;User&nbsp;Image&nbsp;(target&nbsp;image)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;user_img&nbsp;=&nbsp;request.files['user_img']&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;user_img.save('./flask_deep/static/images/'+str(user_img.filename))&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;user_img_path&nbsp;=&nbsp;'./static/images/'+str(user_img.filename)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Neural&nbsp;Style&nbsp;Transfer&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;transfer_img&nbsp;=&nbsp;neural_style_transfer.main(refer_img_path,&nbsp;user_img_path)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;transfer_img_path&nbsp;=&nbsp;'./static/images/'+str(transfer_img.split('/')[-1])&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;render_template('nst_post.html',&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;refer_img=refer_img_path,&nbsp;user_img=user_img_path,&nbsp;transfer_img=transfer_img_path)cs â€” indexì—ì„œ nst_get ë§í¬ í´ë¦­ ì‹œ nst_get.htmlë¡œ ê²½ë¡œ ì´ë™â€” nst_get.htmlì—ì„œ POSTë°©ì‹ì„ í†µí•´ ì „ë‹¬ë°›ì€ ì´ë¯¸ì§€ë¥¼ nst_post í•¨ìˆ˜ì—ì„œ request ë©”ì†Œë“œë¥¼ í†µí•´ ë„˜ê²¨ë°›ìŒâ€” reference ì´ë¯¸ì§€ì™€ user ì´ë¯¸ì§€ ê²½ë¡œë¥¼ neural style transferë¥¼ ìˆ˜í–‰í•˜ëŠ” ë”¥ëŸ¬ë‹ ë©”ì†Œë“œë¡œ ì „ë‹¬â€” neural style transfer ë”¥ëŸ¬ë‹ ì½”ë“œì—ì„œ ëª¨ë¸ ìˆ˜í–‰ í›„ ë°›ì€ ê²°ê³¼ê°’ì„ nst_post.htmlë¡œ ì „ë‹¬ 3. ì›¹ ì‹¤í–‰1) Flask ì„œë²„ ì‹¤í–‰Flask ì„œë²„ ì‹¤í–‰ì„ ìœ„í•´ í”„ë¡œì íŠ¸ í´ë” ìƒìœ„ì— ì•„ë˜ì™€ ê°™ì€ íŒŒì´ì¬ ì½”ë“œë¥¼ ìƒì„±í•˜ì˜€ë‹¤. 123from&nbsp;pyflask&nbsp;import&nbsp;app&nbsp;app.run(host='127.0.0.1')cs ìœ„ ì½”ë“œëŠ” _init_.pyì—ì„œ app ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì‹¤í–‰í•˜ê²Œ í•´ì£¼ë©°, ìœ„ íŒŒì´ì¬ íŒŒì¼ ì‹¤í–‰ ì‹œ ì•„ë˜ì™€ ê°™ì´ flask ì„œë²„ê°€ ì‹¤í–‰ë˜ê²Œ ëœë‹¤. ì´í›„ ë¸Œë¼ìš°ì €ì—ì„œ http://127.0.0.1:5000 ì…ë ¥ ì‹œ ìœ„ì—ì„œ ë§Œë“  í™”ë©´ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. 2) ì›¹ í™”ë©´ ë©”ì¸ í˜ì´ì§€ ì‚¬ìš©ì ì…ë ¥ í˜ì´ì§€ ê²°ê³¼ ì¶œë ¥ í˜ì´ì§€ OutroFlaskëŠ” ì´ë²ˆì— ê°„ë‹¨í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì ìš©í•´ë³´ê¸° ìœ„í•´ì„œ ì²˜ìŒ ì‚¬ìš©í•´ë³´ì•˜ë‹¤. ì´ì „ì— Spring í”„ë ˆì„ì›Œí¬ë¥¼ í†µí•´ í”„ë¡œì íŠ¸ë¥¼ í•´ë³¸ ì ì€ ìˆëŠ”ë° ì‚¬ìš©ì•ˆí•˜ì§€ ë„ˆë¬´ ì˜¤ë˜ë˜ë‹¤ë³´ë‹ˆ ê¹Œë¨¹ê¸°ë„ í–ˆê³  í™˜ê²½ êµ¬ì„±í•˜ëŠ” ê²ƒë„ ì¼ì´ì–´ì„œ ì¢€ ê°€ë²¼ìš´ Flaskë¥¼ ì‚¬ìš©í•´ë³´ê²Œ ë˜ì—ˆë‹¤. ì¼ë‹¨ ê¸°ë³¸ì ìœ¼ë¡œ html/css ê·¸ë¦¬ê³  pythonë§Œ ê¸°ì´ˆì ìœ¼ë¡œ ì•Œì•„ë„ ëˆ„êµ¬ë‚˜ ì‰½ê³  ê°„ë‹¨í•˜ê²Œ ì›¹ í™”ë©´ì„ êµ¬ì„±í•´ë³¼ ìˆ˜ ìˆë‹¤ëŠ” ì¥ì ì´ ìˆëŠ” ê²ƒ ê°™ë‹¤. ê·¼ë° í™•ì‹¤íˆ í° í”„ë¡œì íŠ¸ ì„±ìœ¼ë¡œ ì—¬ëŸ¬ì‚¬ëŒì´ ë³µì¡í•œ í™”ë©´ì„ êµ¬ì„±í•  ë•ŒëŠ” ì‘ì—… ê´€ë¦¬ê°€ ì‰½ì§€ ì•Šì„ ê²ƒ ê°™ë‹¤ëŠ” ìƒê°ì´ ë“ ë‹¤. ì‹œê°„ë‚˜ë©´ Djangoë„ ê³µë¶€í•´ë´ì•¼í•  ê²ƒ ê°™ë‹¤.","categories":[],"tags":[{"name":"deeplearning","slug":"deeplearning","permalink":"https://jaehyeongan.github.io/tags/deeplearning/"},{"name":"python","slug":"python","permalink":"https://jaehyeongan.github.io/tags/python/"},{"name":"flask","slug":"flask","permalink":"https://jaehyeongan.github.io/tags/flask/"},{"name":"webframework","slug":"webframework","permalink":"https://jaehyeongan.github.io/tags/webframework/"},{"name":"webdevelopment","slug":"webdevelopment","permalink":"https://jaehyeongan.github.io/tags/webdevelopment/"},{"name":"django","slug":"django","permalink":"https://jaehyeongan.github.io/tags/django/"},{"name":"neuralstyletransfer","slug":"neuralstyletransfer","permalink":"https://jaehyeongan.github.io/tags/neuralstyletransfer/"}]},{"title":"R-CNN(Regions with CNN features) ë…¼ë¬¸ ë¦¬ë·°","slug":"R-CNN","date":"2019-10-10T14:56:28.000Z","updated":"2020-12-10T14:54:43.000Z","comments":true,"path":"2019/10/10/R-CNN/","link":"","permalink":"https://jaehyeongan.github.io/2019/10/10/R-CNN/","excerpt":"","text":"Intro ì˜¤ëŠ˜ì€ ì´ˆê¸° Object Detection ë°œì „ì— ê°€ì¥ ë§ì€ ì˜í–¥ì„ ë¯¸ì¹œ ë…¼ë¬¸ì¸ Ross Girshickì˜ Rich feature hierarchies for accurate object detection and semantic segmentation ì¦‰, R-CNNì— ëŒ€í•œ ë…¼ë¬¸ ë¦¬ë·°ë¥¼ ê°„ë‹¨íˆ í•˜ê³ ì í•œë‹¤. ìš°ì„  Obejct Detectionì´ë€ ì´ë¯¸ì§€ê°€ ë¬´ì—‡ì¸ì§€ íŒë‹¨í•˜ëŠ” Classificationê³¼ ì´ë¯¸ì§€ ë‚´ì˜ ë¬¼ì²´ì˜ ìœ„ì¹˜ ì •ë³´ë¥¼ ì°¾ëŠ” Localizationì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒì„ ë§í•œë‹¤. ì´ë¥¼ í†µí•´ ì˜ìƒ ë‚´ì˜ ê°ì²´ê°€ ì‚¬ëŒì¸ì§€ ë™ë¬¼ì¸ì§€ ë¬¼ê±´ì¸ì§€ ë“±ì„ êµ¬ë³„í•˜ì—¬ ê° ê°ì²´ê°€ ì–´ë””ì— ìœ„ì¹˜í•˜ëŠ”ì§€ í‘œì‹œí•˜ëŠ” ê²ƒì´ ê°€ëŠ¥í•˜ë‹¤. Abstractì§€ë‚œ ëª‡ ë…„ ë™ì•ˆ PASCAL VOC ë°ì´í„°ì…‹ì—ì„œ Object Detectionì˜ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ëŠ” ê²ƒì€ high-level contextì˜ ë³µì¡í•œ ì•™ìƒë¸” ëª¨ë¸ì´ì—ˆë‹¤. í•˜ì§€ë§Œ ì´ ë…¼ë¬¸ì—ì„œëŠ” VOC 2012 ë°ì´í„°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì´ì „ ëª¨ë¸ì— ë¹„í•´ mAP(mean average precision)ê°€ 30%ì´ìƒ í–¥ìƒëœ ë” ê°„ë‹¨í•˜ê³  í™•ì¥ ê°€ëŠ¥í•œ detection ì•Œê³ ë¦¬ì¦˜ì„ ì†Œê°œí•˜ì˜€ë‹¤.ì´ ì•Œê³ ë¦¬ì¦˜ì€ í¬ê²Œ ë‘ ê°€ì§€ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ë¥¼ ê°€ì§€ê³  ìˆëŠ”ë° ë‹¤ìŒê³¼ ê°™ë‹¤. ê°ì²´ë¥¼ localize ë° segmentí•˜ê¸° ìœ„í•´ bottom-upë°©ì‹ì˜ region proposal(ì§€ì—­ ì œì•ˆ)ì— Convolutional Neural Networkë¥¼ ì ìš© domain-specific fine-tuningì„ í†µí•œ supervised pre-trainingì„ ì ìš© ì €ìëŠ” í•´ë‹¹ ëª¨ë¸ì„ R-CNN(Regions with CNN features)ì´ë¼ê³  ëª…ì‹œí•˜ì˜€ìœ¼ë©°, ê·¸ ì´ìœ ëŠ” CNNê³¼ Region proposalì´ ê²°í•©ë˜ì—ˆê¸° ë•Œë¬¸ì´ë¼ê³  í•œë‹¤. 1. Introductionì§€ë‚œ 10ë…„ê°„ ë‹¤ì–‘í•œ visual recognition ì‘ì—…ì—ì„œëŠ” ì£¼ë¡œ SIFTì™€ HOG(gradient ê¸°ë°˜ì˜ íŠ¹ì§•ì  ì¶”ì¶œ ì•Œê³ ë¦¬ì¦˜)ê°€ ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ì—ˆëŠ”ë°, ì´ëŠ” 2010 ~ 2012ë…„ì˜ PASCAL VOC obeject detectionì—ì„œ ì¼ë°˜ì ìœ¼ë¡œ ì¸ì •ë˜ëŠ” ë°©ë²•ì´ì—ˆë‹¤. í•˜ì§€ë§Œ ì´í›„ back-propagationì´ ê°€ëŠ¥í•œ SGD(Stochastic Gradient Descent)ê¸°ë°˜ì˜ CNN(Convolutional Neural Networks)ì´ ë“±ì¥í•˜ê¸° ì‹œì‘í•˜ì˜€ê³  SIFTì™€ HOGì™€ ê°™ì€ ì•Œê³ ë¦¬ì¦˜ê³¼ ë¹„êµí•˜ì—¬ PASCAL VOC object detectionì—ì„œ êµ‰ì¥í•œ ì„±ëŠ¥ì„ ë³´ì´ê²Œ ë˜ì—ˆë‹¤.Image Classificationê³¼ ë‹¤ë¥´ê²Œ detectionì€ ì´ë¯¸ì§€ë‚´ì—ì„œ ê°ì²´ë¥¼ localizingí•˜ëŠ” ê²ƒì´ ìš”êµ¬ë˜ëŠ”ë° ì´ë¥¼ ìœ„í•´, ë…¼ë¬¸ì˜ ëª¨ë¸ì€ sliding-window ë°©ì‹ì„ ì ìš©í•˜ì˜€ê³ , ë†’ì€ ê³µê°„ í•´ìƒë„(high spartial resolution)ì„ ìœ ì§€í•˜ê¸° ìœ„í•´ 5ê°œì˜ Convolutional ë ˆì´ì–´ë¥¼ ì ìš©í•˜ì˜€ë‹¤.ìš°ì„  ê°„ë‹¨í•˜ê²Œ R-CNNì€ ì•„ë˜ì™€ ê°™ì€ í”„ë¡œì„¸ìŠ¤ë¡œ ì‘ë™í•œë‹¤. R-CNN í”„ë¡œì„¸ìŠ¤ Input ì´ë¯¸ì§€ë¡œë¶€í„° 2,000ê°œì˜ ë…ë¦½ì ì¸ region proposalì„ ìƒì„± CNNì„ í†µí•´ ê° proposal ë§ˆë‹¤ ê³ ì •ëœ ê¸¸ì´ì˜ feature vectorë¥¼ ì¶”ì¶œ(CNN ì ìš© ì‹œ ì„œë¡œ ë‹¤ë¥¸ region shapeì— ì˜í–¥ì„ ë°›ì§€ ì•Šê¸° ìœ„í•´ fixed-sizeë¡œ ì´ë¯¸ì§€ë¥¼ ë³€ê²½) ì´í›„, ê° region ë§ˆë‹¤ category-specific linear SVMì„ ì ìš©í•˜ì—¬ classificationì„ ìˆ˜í–‰ 2. Object detection with R-CNNì´ ë…¼ë¬¸ì˜ object detectionì€ í¬ê²Œ 3ê°€ì§€ ëª¨ë“ˆë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤. 1. category-independentí•œ region proposalsë¥¼ ìƒì„±2. ê° regionìœ¼ë¡œë¶€í„° feature vectorë¥¼ ì¶”ì¶œí•˜ê¸° ìœ„í•œ large CNN3. classificationì„ ìœ„í•œ linear SVMsì´ì œ ì•„ë˜ì—ì„œ ë³¸ê²©ì ìœ¼ë¡œ ê° ëª¨ë“ˆì— ëŒ€í•´ ì„¤ëª…í•˜ê³  PASCAL VOC2010-12ì— ëŒ€í•œ ê²°ê³¼ë¥¼ ì†Œê°œí•œë‹¤. Region proposalsì¹´í…Œê³ ë¦¬ ë…ë¦½ì ì¸ region proposalì„ ìƒì„±í•˜ê¸° ìœ„í•œ ë°©ë²•ì€ ì—¬ëŸ¬ê°€ì§€ê°€ ìˆëŠ”ë° í•´ë‹¹ ë…¼ë¬¸ì—ì„œëŠ” ì´ì „ detection ì‘ì—…ë“¤ê³¼ ë¹„êµí•˜ê¸° ìœ„í•˜ì—¬ Selective Searchë¼ëŠ” ìµœì ì˜ region proposalë¥¼ ì œì•ˆí•˜ëŠ” ê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬ ë…ë¦½ì ì¸ region proposalì„ ì¶”ì¶œí•˜ì˜€ë‹¤. selective searchëŠ” ì•„ë˜ì™€ ê°™ì€ í”„ë¡œì„¸ìŠ¤ë¡œ ì´ë£¨ì–´ì§„ë‹¤. Selective Search ì´ë¯¸ì§€ì˜ ì´ˆê¸° ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ì •í•˜ì—¬, ìˆ˜ë§ì€ region ì˜ì—­ì„ ìƒì„± greedy ì•Œê³ ë¦¬ì¦˜ì„ ì´ìš©í•˜ì—¬ ê° regionì„ ê¸°ì¤€ìœ¼ë¡œ ì£¼ë³€ì˜ ìœ ì‚¬í•œ ì˜ì—­ì„ ê²°í•© ê²°í•©ë˜ì–´ ì»¤ì§„ regionì„ ìµœì¢… region proposalë¡œ ì œì•ˆ Feature extractionìš°ì„  ìœ„ì—ì„œ ì–¸ê¸‰í•œ Selective Searchë¥¼ í†µí•´ ë„ì¶œ ëœ ê° region proposalë¡œë¶€í„° CNNì„ ì‚¬ìš©í•˜ì—¬ 4096ì°¨ì›ì˜ feature vectorë¥¼ ì¶”ì¶œí•œë‹¤. ì´í›„, featureë“¤ì€ 5ê°œì˜ convolutional layerì™€ 2ê°œì˜ fully connected layerë¡œ ì „íŒŒë˜ëŠ”ë°, ì´ë•Œ CNNì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©ë˜ê¸° ìœ„í•´ ê° regionì€ 227x227 RGBì˜ ê³ ì •ëœ ì‚¬ì´ì¦ˆë¡œ ë³€í™˜ë˜ê²Œ ëœë‹¤. Trainingí•™ìŠµì— ì‚¬ìš©ë˜ëŠ” CNN ëª¨ë¸ì˜ ê²½ìš° ILSVRC 2012 ë°ì´í„° ì…‹ìœ¼ë¡œ ë¯¸ë¦¬ í•™ìŠµëœ pre-trained CNN(AlexNet)ëª¨ë¸ì„ ì‚¬ìš©í•œë‹¤. Domain-specific fine-tuningClassificationì— ìµœì í™”ëœ CNN ëª¨ë¸ì„ ìƒˆë¡œìš´ Detection ì‘ì—… ê·¸ë¦¬ê³  VOC ë°ì´í„°ì…‹ì— ì ìš©í•˜ê¸° ìœ„í•´ ì˜¤ì§ VOCì˜ region proposalsë¥¼ í†µí•´ SGD(stochastic gradient descent)ë°©ì‹ìœ¼ë¡œ CNN íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸ í•œë‹¤. ì´í›„ CNNì„ í†µí•´ ë‚˜ì˜¨ feature mapì€ SVMì„ í†µí•´ classification ë° bounding regreesionì´ ì§„í–‰ë˜ê²Œ ë˜ëŠ”ë°, ì—¬ê¸°ì„œ SVM í•™ìŠµì„ ìœ„í•´ NMS(non-maximum suppresion)ê³¼ IoU(inter-section-over-union)ì´ë¼ëŠ” ê°œë…ì´ í™œìš©ëœë‹¤. IoUëŠ” Area of Overlap(êµì§‘í•©) / Area of Union(í•©ì§‘í•©)ìœ¼ë¡œ ê³„ì‚°ë˜ë©°, ê°„ë‹¨íˆ ë§í•´ ì „ì²´ bounding box ì˜ì—­ ì¤‘ ê²¹ì¹˜ëŠ” ë¶€ë¶„ì˜ ë¹„ìœ¨ì„ ë‚˜íƒ€ë‚´ëŠ”ë° NMS ì•Œê³ ë¦¬ì¦˜ì´ ì´ IoU ì ìˆ˜ë¥¼ í™œìš©í•˜ì—¬ ê²¹ì¹˜ëŠ” ë°•ìŠ¤ë¥¼ ëª¨ë‘ ì œê±°í•˜ê³  ê°€ì¥ ì í•©í•œ ë°•ìŠ¤ë§Œ ë‚¨ê¸°ê²Œ ëœë‹¤. NMSì˜ ê³¼ì •ì„ ê°„ë‹¨íˆ ì‚´í´ë³´ë©´ ì•„ë˜ì™€ ê°™ì€ í”„ë¡œì„¸ë¡œ ì§„í–‰ëœë‹¤. NMS(Non-maximum suppresion) ì˜ˆì¸¡í•œ bounding boxë“¤ì˜ ì˜ˆì¸¡ ì ìˆ˜ë¥¼ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬ ë†’ì€ ì ìˆ˜ì˜ ë°•ìŠ¤ë¶€í„° ì‹œì‘í•˜ì—¬ ë‚˜ë¨¸ì§€ ë°•ìŠ¤ë“¤ ê°„ì˜ IoUë¥¼ ê³„ì‚° IoUê°’ì´ ì§€ì •í•œ threholdë³´ë‹¤ ë†’ì€ ë°•ìŠ¤ë¥¼ ì œê±° ìµœì ì˜ ë°•ìŠ¤ë§Œ ë‚¨ì„ ë–„ê¹Œì§€ ìœ„ ê³¼ì •ì„ ë°˜ë³µ í•´ë‹¹ ë…¼ë¬¸ì—ì„œëŠ” SVM í•™ìŠµì„ ìœ„í•œ ë¼ë²¨ë¡œì„œ IoUë¥¼ í™œìš©í•˜ì˜€ê³  IoU ê°€ 0.5ì´ìƒì¸ ê²ƒë“¤ì„ positive ê°ì²´ë¡œ ë³´ê³  ë‚˜ë¨¸ì§€ëŠ” negativeë¡œ ë¶„ë¥˜í•˜ì—¬ í•™ìŠµí•˜ê²Œ ëœë‹¤. ê° SGD iterationë§ˆë‹¤ 32ê°œì˜ positive windowì™€ 96ê°œì˜ backgroud window ì´ 128ê°œì˜ ë°°ì¹˜ë¡œ í•™ìŠµì´ ì§„í–‰ëœë‹¤. 3. Results on PASCAL VOC 2010-12 ìœ„ í…Œì´ë¸”ì€ VOC 2010 í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ê° ëª¨ë¸ë³„ ê²°ê³¼ì´ë‹¤. ë§¨ ì˜¤ë¥¸ìª½ì—ì„œ mAPë¥¼ í™•ì¸í•  ìˆ˜ ìˆëŠ”ë°, ë…¼ë¬¸ì—ì„œëŠ” ê²°ê³¼ë¥¼ ë¹„êµí•˜ëŠ”ë° ê°™ì€ region proposal ì•Œê³ ë¦¬ì¦˜ì„ ì ìš©í•œ UVAëª¨ë¸ê³¼ mAPë¥¼ ë¹„êµí•œë‹¤.ìœ„ í‘œë¥¼ ë³´ë©´ UVA ëª¨ë¸ì˜ mAPëŠ” 35.1%ì´ê³ , R-CNNì˜ mAPëŠ” 53.7%ì¸ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìœ¼ë©° ì´ê²ƒì€ ë†’ì€ ì¦ê°€ìœ¨ì´ë¼ê³  ì €ìëŠ” ë§í•œë‹¤. ë˜í•œ VOC 2011/12 ë°ì´í„° ì…‹ ë˜í•œ 53.3% mAP ë†’ì€ ì„±ëŠ¥ì„ ë‚˜íƒ€ëƒˆë‹¤. 4. ProblemsR-CNNì˜ ê°€ì¥ í° ë¬¸ì œëŠ” ë³µì¡í•œ í”„ë¡œì„¸ìŠ¤ë¡œ ì¸í•œ ê³¼ë„í•œ ì—°ìƒëŸ‰ì— ìˆë‹¤. ìµœê·¼ì—ëŠ” ê³ ì„±ëŠ¥ GPUê°€ ë§ì´ ë³´ê¸‰ ë˜ì—ˆê¸° ë•Œë¬¸ì— deepí•œ neural netì´ë¼ë„ GPUì—°ì‚°ì„ í†µí•´ ë¹ ë¥¸ ì²˜ë¦¬ê°€ ê°€ëŠ¥í•˜ë‹¤. í•˜ì§€ë§Œ R-CNNì€ selective search ì•Œê³ ë¦¬ì¦˜ë¥¼ í†µí•œ region proposal ì‘ì—… ê·¸ë¦¬ê³  NMS ì•Œê³ ë¦¬ì¦˜ ì‘ì—… ë“±ì€ CPU ì—°ì‚°ì— ì˜í•´ ì´ë£¨ì–´ ì§€ê¸° ë•Œë¬¸ì— êµ‰ì¥íˆ ë§ì€ ì—°ì‚°ëŸ‰ ë° ì‹œê°„ì´ ì†Œëª¨ëœë‹¤.ë˜í•œ SVM ì˜ˆì¸¡ ì‹œ regionì— ëŒ€í•œ classification ë° bounding boxì— ëŒ€í•œ regression ì‘ì—…ì´ í•¨ê»˜ ì‘ë™í•˜ë‹¤ ë³´ë‹ˆ ëª¨ë¸ ì˜ˆì¸¡ ë¶€ë¶„ì—ì„œë„ ì—°ì‚° ë° ì‹œê°„ì´ ë§ì´ ì†Œëª¨ë˜ì–´ real-time ë¶„ì„ì´ ì–´ë µë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤. R-CNNì˜ ì´ëŸ¬í•œ í•œê³„ë“¤ë¡œ ì¸í•´, ì¶”í›„ í”„ë¡œì„¸ìŠ¤ ë° ì—°ì‚° ì¸¡ë©´ì—ì„œ ë³´ì™„ëœ ëª¨ë¸ì´ ë‚˜ì˜¤ê²Œ ë˜ëŠ”ë° ê·¸ê²ƒì´ ë°”ë¡œ Fast R-CNNê³¼ Faster R-CNNì´ë‹¤. ìœ„ ê·¸ë¦¼ì€ R-CNN, SPP-Net, Fast R-CNN, Faster R-CNNì˜ ì‹¤í–‰ ì†ë„ ì°¨ì´ë¥¼ ë‚˜íƒ€ë‚´ëŠ”ë° Faster R-CNNì´ ì´ì „ ëª¨ë¸ë³´ë‹¤ ë¹„êµê°€ ì•ˆë  ì •ë„ë¡œ í› ì–¼ì”¬ ë¹ ë¥´ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. (ì„±ëŠ¥ë„ ë” ì¢‹ì•„ì¡Œë‹¤.)ì•„ë˜ì—ì„œ Fast R-CNNê³¼ Faster R-CNNì— ëŒ€í•´ ê°„ë‹¨í•˜ê²Œ ì§‘ê³  ë„˜ì–´ê°€ ë³´ë„ë¡ í•œë‹¤. 5. Fast R-CNN &amp; Faster R-CNNFast R-CNNFast R-CNNì˜ R-CNNì˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë‚˜ì˜¨ ëª¨ë¸ì´ë‹¤.ë™ì‘ ë°©ì‹ì€ R-CNNê³¼ ìœ ì‚¬í•˜ê²Œ region proposalì´ ì‘ë™í•˜ì§€ë§Œ, RCNNê³¼ ë‹¤ë¥´ê²Œ Fast R-CNNì€ ë¨¼ì € ì „ì²´ ì´ë¯¸ì§€ê°€ ConvNetì˜ inputìœ¼ë¡œ ì…ë ¥ì´ ëœë‹¤. ì´ë¯¸ì§€ëŠ” ConvNetì„ í†µê³¼í•˜ë©° feature mapì„ ì¶”ì¶œí•˜ê²Œ ë˜ê³ , ì´ feature mapì€ selectice search ê¸°ë°˜ì˜ region proposalì„ í†µí•´ RoI(Regions of Interest)ë¥¼ ë½‘ì•„ë‚¸ë‹¤. ì´í›„ ì„ íƒ ëœ Regionë“¤ì€ RoI Pooling layerë¥¼ ê±°ì¹˜ê²Œ ë˜ëŠ”ë°, ì´ ê³¼ì •ì€ ì¶”í›„ ì˜ˆì¸¡ì„ ìœ„í•´ regionë“¤ì„ ë‹¤ìš´ ì‚¬ì´ì¦ˆí•˜ì—¬ ëª¨ë‘ ê°™ì€ ê³ ì •ëœ í¬ê¸°ë¡œ ë³€í™˜í•´ì£¼ëŠ” ì—­í• ì„ í•œë‹¤. ë§ˆì§€ë§‰ ê³¼ì •ìœ¼ë¡œ fully connected layerë¥¼ ê±°ì¹˜ë©° Softmax Classificationê³¼ Bounding Box Regressionì´ ìˆ˜í–‰ëœë‹¤. ìœ„ì˜ ê³¼ì •ì€ í•˜ë‚˜ì˜ ConvNetëª¨ë¸ì— ì˜í•´ ë™ì‹œì— ìˆ˜í–‰ì´ ë˜ê¸° ë•Œë¬¸ì— RCNNì— ë¹„í•˜ì—¬ í›¨ì”¬ ë¹ ë¥´ê²Œ ì‘ë™í•˜ëŠ” ì¥ì ì´ ìˆë‹¤. í•˜ì§€ë§Œ ê²°êµ­ Fast RCNN ë˜í•œ ë§ì€ ì—°ì‚°ì„ í•„ìš”ë¡œ í•˜ëŠ” Selective Search ê¸°ë²•ì´ ì‘ë™ì„ í•˜ë¯€ë¡œ í° ë°ì´í„° ì…‹ì— ì ìš©í•˜ëŠ”ë°ëŠ” í•œê³„ê°€ ìˆë‹¤. Faster R-CNNFaster R-CNNì€ R-CNNê³¼ Fast R-CNNì´ region proposalë¡œ ì¸í•œ ê³¼ë„í•œ ì—°ì‚° ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë‚˜ì˜¨ ëª¨ë¸ì´ë‹¤. ê¸°ì¡´ region proposalì— ì‚¬ìš©ë˜ì—ˆë˜ selective searchëŠ” ì—°ì‚°ëŸ‰ì„ ëŠ˜ë¦¬ê³  ì‹œê°„ì„ ë§ì´ ì†Œëª¨í•˜ëŠ” ì£¼ìš” ì›ì¸ì´ì—ˆë‹¤. ê·¸ë˜ì„œ Faster R-CNNì—ì„œëŠ” selective search ì•Œê³ ë¦¬ì¦˜ì„ ì—†ì• ê³  Region Proposal Networks(RPN)ë¼ëŠ” ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬ë¥¼ ì¶”ê°€í•˜ì—¬ region proposalì„ ì˜ˆì¸¡í•˜ë„ë¡ í–ˆë‹¤. ê·¸ í›„, ì˜ˆì¸¡ëœ region proposalì€ Fast R-CNNê³¼ ìœ ì‚¬í•˜ê²Œ RoI Pooling layerë¥¼ ê±°ì¹˜ë©° ëª¨ë“  regionì„ ê°™ì€ í¬ê¸°ë¡œ ê³ ì • í›„, Classification ë° Bounding Box Regreesionì´ ìˆ˜í–‰ëœë‹¤. Referencespaper R-Rich feature hierarchies for accurate object detection and semantic segmentation(https://arxiv.org/abs/1311.2524) Fast R-CNN(https://arxiv.org/abs/1504.08083) Faster R-CNN(https://arxiv.org/abs/1506.01497) blog https://reniew.github.io/10/ https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365ehttps://blog.lunit.io/2017/06/01/r-cnns-tutorial/","categories":[],"tags":[{"name":"selectivesearch","slug":"selectivesearch","permalink":"https://jaehyeongan.github.io/tags/selectivesearch/"},{"name":"rcnn","slug":"rcnn","permalink":"https://jaehyeongan.github.io/tags/rcnn/"},{"name":"fastrcnn","slug":"fastrcnn","permalink":"https://jaehyeongan.github.io/tags/fastrcnn/"},{"name":"fasterrcnn","slug":"fasterrcnn","permalink":"https://jaehyeongan.github.io/tags/fasterrcnn/"},{"name":"nms","slug":"nms","permalink":"https://jaehyeongan.github.io/tags/nms/"},{"name":"regionproposals","slug":"regionproposals","permalink":"https://jaehyeongan.github.io/tags/regionproposals/"},{"name":"iou","slug":"iou","permalink":"https://jaehyeongan.github.io/tags/iou/"},{"name":"voc","slug":"voc","permalink":"https://jaehyeongan.github.io/tags/voc/"}]},{"title":"í•©ì„±ê³± ì‹ ê²½ë§(ConvNet, Convolutional Neural Network)","slug":"basic-convnet","date":"2019-09-23T14:18:01.000Z","updated":"2020-12-10T14:50:21.000Z","comments":true,"path":"2019/09/23/basic-convnet/","link":"","permalink":"https://jaehyeongan.github.io/2019/09/23/basic-convnet/","excerpt":"","text":"Intro í˜„ì¬ ConvNet ê¸°ë°˜ì˜ ëª¨ë¸ì€ ë‹¨ìˆœ ì´ë¯¸ì§€ ì¸ì‹ì„ ë„˜ì–´ Object Detection, Semantic Segmentation ê¹Œì§€ ë”¥ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ ì¤‘ ê°€ì¥ í™œë°œíˆ ì—°êµ¬ë˜ê³  ì„±ê³¼ë¥¼ ë‚´ê³  ìˆëŠ” ë¶„ì•¼ì´ë‹¤. ìš°ì„  ê° ë¶„ì•¼ë³„ ì ìš©ë˜ê³  ìˆëŠ” ì£¼ìš” ëª¨ë¸ì„ ê°„ë‹¨íˆ ì‚´í´ë³´ë©´ ì•„ë˜ì™€ ê°™ë‹¤. Classification Image Detection Semantic Segentation VGG Net RCNN FCN GoogLeNet Fast RCNN DeepLab ResNet Faster RCNN U-Net MobileNet YOLO ReSeg ShuffleNet SDD ì´ ê¸€ì—ì„œëŠ” ìœ„ì˜ í›Œë¥­í•œ ëª¨ë¸ë“¤ì´ ê°€ì¥ ê¸°ë³¸ìœ¼ë¡œ í•˜ëŠ” ConvNetì˜ êµ¬ì¡° ë° í•™ìŠµ ë°©ë²•ì— ëŒ€í•´ ê°„ë‹¨íˆ ì•Œë ¤ë³´ë ¤ê³  í•œë‹¤. í•©ì„±ê³± ì‹ ê²½ë§(Convolutional Neural Network, ConvNet)í•©ì„±ê³± ì‹ ê²½ë§ì€ í•©ì„±ê³± ì—°ì‚°ì„ ì‚¬ìš©í•˜ëŠ” ì‹ ê²½ë§ ì¤‘ í•˜ë‚˜ë¡œì„œ, ì£¼ë¡œ ìŒì„± ì¸ì‹ì´ë‚˜ ì‹œê°ì  ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ëŠ”ë° ì‚¬ìš©ëœë‹¤. í•©ì„±ê³± ì‹ ê²½ë§ì´ ì¼ë°˜ ì‹ ê²½ë§ê³¼ ë‹¤ë¥¸ ì ì€ ì¼ë°˜ì ì¸ ì‹ ê²½ë§ë“¤ì´ ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ì›ë³¸ ê·¸ëŒ€ë¡œ 1ì°¨ì› ì‹ ê²½ë§ì¸ Fully-Connected layer(FC layer í˜¹ì€ Dense layer)ì— ì…ë ¥ë˜ì–´ ì „ì²´ íŠ¹ì„±ì„ í•™ìŠµí•˜ê³  ì²˜ë¦¬í•˜ëŠ” ë° ë°˜í•´, í•©ì„±ê³± ì‹ ê²½ë§ì€ FC layerë¥¼ ê±°ì¹˜ê¸° ì „ì— Convolution ë° Poollingê³¼ ê°™ì€ ë°ì´í„°ì˜ ì£¼ìš” íŠ¹ì§• ë²¡í„°ë¥¼ ì¶”ì¶œí•˜ëŠ” ê³¼ì •ì„ ê±°ì¹œ í›„ FC layerë¡œ ì…ë ¥ë˜ê²Œ ëœë‹¤.ê·¸ë ‡ê¸° ë•Œë¬¸ì— ëŒ€ë¶€ë¶„ì˜ ì´ë¯¸ì§€ ì¸ì‹ ë¶„ì•¼ëŠ” ë”¥ëŸ¬ë‹ ê¸°ë°˜ì˜ í•©ì„±ê³± ì‹ ê²½ë§ì´ ì£¼ë¥¼ ì´ë£¨ê³  ìˆë‹¤. ConvNetì˜ êµ¬ì¡° í•©ì„±ê³± ì‹ ê²½ë§ì€ ê¸°ë³¸ì ìœ¼ë¡œ ìœ„ì™€ ê°™ì€ êµ¬ì¡°ë¡œ ì´ë£¨ì–´ì ¸ ìˆë‹¤. ìœ„ ConvNetì„ í¬ê²Œ 3ë©ì–´ë¦¬ë¡œ ì§¤ë¼ì„œ ë³´ë©´ ì•„ë˜ì™€ ê°™ë‹¤. 1. Input layer2. Convolutional layer ~ Max pooling layer3. Fully-connected layer ~ Output layersë¡œ ìœ„ êµ¬ì¡°ë¥¼ ê°„ë‹¨íˆ ì„¤ëª…í•˜ë©´ ìœ„ì—ì„œë„ ë§í–ˆë“¯ì´ ConvNetì€ ë‹¨ìˆœ FC Layerë¡œë§Œ êµ¬ì„±ë˜ì–´ ìˆì§€ ì•Šë‹¤. Convolutional Layerì™€ Pooling Layerë¼ëŠ” ê³¼ì •ì„ ê±°ì¹˜ê²Œ ë˜ëŠ”ë° ì´ëŠ” Input Imageì˜ ì£¼ìš” íŠ¹ì§• ë²¡í„°ë¥¼ ì¶”ì¶œí•˜ëŠ” ê³¼ì •ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤. ê·¸ í›„ ì´ë ‡ê²Œ ì¶”ì¶œëœ ì£¼ìš” íŠ¹ì§• ë²¡í„°ë“¤ì€ ê·¸ì œì•¼ FC Layerë¥¼ ê±°ì¹˜ë©° 1ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜ë˜ê³  ë§ˆì§€ë§‰ Output layerì—ì„œ í™œì„±í™” í•¨ìˆ˜ì¸ Softmaxí•¨ìˆ˜ë¥¼ í†µí•´ ê° í•´ë‹¹ í´ë˜ìŠ¤ì˜ í™•ë¥ ë¡œ ì¶œë ¥ë˜ê²Œ ëœë‹¤. ì•„ë˜ì—ì„œ ê° ê³¼ì •ì„ ì¢€ ë” ìƒì„¸íˆ ì‚´í´ë³´ê² ë‹¤. 1. Input LayerInput LayerëŠ” ì…ë ¥ëœ ì´ë¯¸ì§€ ë°ì´í„°ê°€ ìµœì´ˆë¡œ ê±°ì¹˜ê²Œ ë˜ëŠ” Layerì´ë‹¤. ëª¨ë‘ê°€ ì•Œê³  ìˆë“¯ì´ ì´ë¯¸ì§€ëŠ” ë‹¨ìˆœ 1ì°¨ì›ì˜ ë°ì´í„°ê°€ ì•„ë‹ˆë‹¤. ì´ë¯¸ì§€ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ (ë†’ì´, ë„“ì´, ì±„ë„)ì˜ í¬ê¸°ë¥¼ ê°–ëŠ” 3ì°¨ì›ì˜ í¬ê¸°ë¥¼ ê°€ì§€ë©°, ì—¬ê¸°ì„œ ì±„ë„(channels)ì˜ ê²½ìš° Gray Scale(1)ì´ëƒ RGB(3)ì´ëƒ ì— ë”°ë¼ í¬ê¸°ê°€ ë‹¬ë¼ì§€ê²Œ ëœë‹¤. (ì±„ë„ì˜ ì»¬ëŸ¬ ê³µê°„ì€ Gray, RGB, HSV, CMYK ë“± ë‹¤ì–‘í•˜ë‹¤) ìœ„ì™€ ê°™ì€ í˜•íƒœëŠ” ë†’ì´ 4, ë„“ì´ 4, ì±„ë„ RGBë¥¼ ê°–ê³  ìˆìœ¼ë¯€ë¡œ ìœ„ ì´ë¯¸ì§€ì˜ shapeì€ (4, 4, 3)ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆìœ¼ë©°, ì´ë¯¸ì§€ ì¸ì‹ì˜ êµê³¼ì„œë¼ í•  ìˆ˜ ìˆëŠ” ìœ„ MNIST ì†ê¸€ì”¨ ë°ì´í„° ì…‹ì˜ ê²½ìš° ë†’ì´ 28, ë„“ì´ 28, ì±„ë„ Grayë¥¼ ê°€ì§€ê³  ìˆìœ¼ë¯€ë¡œ (28, 28, 1)ì˜ shapeì„ ê°€ì¡Œë‹¤ê³  ë§í•  ìˆ˜ ìˆë‹¤. ë˜í•œ ë‹¤ë¥¸ ë§ë¡œ íŠ¹ì„± ë§µ(Feature Map)ì´ë¼ê³ ë„ í•œë‹¤. 2. Convolutional LayerConvolutional Layerì™€ FC Layerì˜ ê²½ìš° ê·¼ë³¸ì ì€ ì°¨ì´ê°€ ì¡´ì¬í•˜ëŠ”ë°, Dense ì¸µì˜ ê²½ìš° íŠ¹ì„± ê³µê°„ì— ìˆëŠ” ì „ì—­ íŒ¨í„´(ì…ë ¥ëœ ì´ë¯¸ì§€ì˜ ëª¨ë“  í”½ì…€ì— ê±¸ì¹œ íŒ¨í„´)ì„ í•™ìŠµí•˜ëŠ” ë°˜ë©´ í•©ì„±ê³± ì¸µì˜ ê²½ìš° ì§€ì—­ íŒ¨í„´ì„ í•™ìŠµí•˜ê²Œ ëœë‹¤. 2.1 kernelConvolutional Layerì—ì„œëŠ” Input Imageì˜ í¬ê¸°ì¸ íŠ¹ì„± ë§µ(Feature Map)ì„ ì…ë ¥ìœ¼ë¡œ ë°›ê²Œ ë˜ëŠ”ë° ì§€ì—­ íŒ¨í„´ í•™ìŠµì„ ìœ„í•˜ì—¬ ì´ëŸ¬í•œ íŠ¹ì„± ë§µì— ì»¤ë„(kernel) í˜¹ì€ í•„í„°(Filter)ë¼ ë¶ˆë¦¬ëŠ” ì •ì‚¬ê° í–‰ë ¬ì„ ì ìš©í•˜ë©° í•©ì„±ê³± ì—°ì‚°ì„ ìˆ˜í–‰í•˜ê²Œ ëœë‹¤.ì»¤ë„ì˜ ê²½ìš° 3 x 3, 5 x 5í¬ê¸°ë¡œ ì ìš©ë˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì´ë©° ìŠ¤íŠ¸ë¼ì´ë“œ(Stride)ë¼ê³  ë¶ˆë¦¬ëŠ” ì§€ì •ëœ ê°„ê²©ì— ë”°ë¼ ìˆœì°¨ì ìœ¼ë¡œ ì´ë™í•˜ê²Œ ëœë‹¤. ìœ„ ê·¸ë¦¼ì˜ ê²½ìš° Imageì˜ í¬ê¸°ëŠ” (5, 5, 1)ì˜ í¬ê¸°ë¥¼ ê°€ì§€ê³  ìˆìœ¼ë©°, í˜„ì¬ 3 x 3í¬ê¸°ì˜ kernelì´ 1 Strideì˜ ê°„ê²©ìœ¼ë¡œ ì´ë™í•˜ë©° í•©ì„±ê³± ì—°ì‚°ì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒì„ ë³´ì—¬ì¤€ë‹¤. ë§Œì•½ ì»¤ë„ì´ 2ê°œì˜ í¬ê¸°ë§Œí¼ ì´ë™í•˜ê³  ìˆë‹¤ë©´ 2 Stride ê°„ê²©ìœ¼ë¡œ ì´ë™í•œë‹¤ê³  í•  ìˆ˜ ìˆë‹¤. ì´ë ‡ê²Œ ì»¤ë„ì€ ìŠ¤íŠ¸ë¼ì´ë“œ ê°„ê²©ë§Œí¼ ìˆœíšŒí•˜ë©° ëª¨ë“  ì±„ë„ì˜ í•©ì„±ê³±ì˜ í•©ì„ ìƒˆë¡œìš´ íŠ¹ì„± ë§µìœ¼ë¡œ ë§Œë“¤ê²Œ ë˜ë©°, ê²°êµ­ ìœ„ ê·¸ë¦¼ì˜ ê²½ìš° ì»¤ë„ê³¼ ìŠ¤íŠ¸ë¼ì´ë“œì˜ ìƒí˜¸ì‘ìš©ìœ¼ë¡œ ì¸í•´ ì›ë³¸ (5, 5, 1) í¬ê¸°ì˜ Feature Mapì•„ (3, 3, 1)í¬ê¸°ì˜ Feature Mapì˜ í¬ê¸°ë¡œ ì¤„ì–´ë“¤ê²Œ ë˜ì—ˆë‹¤. ì»¤ë„ê³¼ ìŠ¤íŠ¸ë¼ì´ë“œì˜ ê²½ìš° í¬ê¸°ê°€ í´ ìˆ˜ ë¡ ì¢€ë” ë¹¨ë¦¬ ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆì§€ë§Œ, ë„“ì€ íŠ¹ì„±ì„ í° ë³´í­ìœ¼ë¡œ ì´ë™í•˜ëŠ” ë§Œí¼ ì£¼ìš” íŠ¹ì„±ì„ ë†“ì¹  ìˆ˜ ìˆë‹¤ëŠ” ë‹¨ì ì´ ì¡´ì¬í•œë‹¤. 2.2 Paddingí•©ì„±ê³± ì—°ì‚°ì„ ìˆ˜í–‰í•  ê²½ìš° ë‹¨ì ì´ ì¡´ì¬í•˜ëŠ”ë°, ë°”ë¡œ ìœ„ì—ì„œ ì‚´í´ë³´ì•˜ë“¯ì´ kernelê³¼ strideì˜ ì‘ìš©ìœ¼ë¡œ ì¸í•´ ì›ë³¸ í¬ê¸°ê°€ ì¤„ì–´ë“ ë‹¤ëŠ” ê²ƒì´ë‹¤. ë”°ë¼ì„œ ì´ë ‡ê²Œ Feature Mapì˜ í¬ê¸°ê°€ ì‘ì•„ì§€ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ì„œ Paddingì´ë€ ê¸°ë²•ì„ ì´ìš©í•˜ê²Œ ë˜ëŠ”ë°, ì‰½ê²Œ ë§í•´ ë‹¨ìˆœíˆ ì›ë³¸ ì´ë¯¸ì§€ì— 0ì´ë¼ëŠ” paddingê°’ì„ ì±„ì›Œ ë„£ì–´ ì´ë¯¸ì§€ë¥¼ í™•ì¥í•œ í›„ í•©ì„±ê³± ì—°ì‚°ì„ ì ìš©í•˜ëŠ” ê²ƒì„ ë§í•œë‹¤. ìœ„ ê·¸ë¦¼ì„ ë³´ë©´ ìœ„ì—ì„œ ì‚´í´ë³¸ ë°”ì™€ ê°™ì´ ë˜‘ê°™ì€ (5, 5, 1)í¬ê¸°ì˜ ì´ë¯¸ì§€ ë°ì´í„°ê°€ ë†“ì—¬ìˆë‹¤. ë‹¤ë¥¸ ì ì€ ì‚¬ë°©ìœ¼ë¡œ ë¹ˆ ê³µê°„(0)ì´ 1ì¹¸ì”© ë” ì±„ì›Œì ¸ ìˆë‹¤ëŠ” ê²ƒì¸ë° ì´ê²ƒì´ ë°”ë¡œ paddingì´ë‹¤. ì´í›„ ìœ„ì™€ ë˜‘ê°™ì€ 3 x 3 í¬ê¸°ì˜ ì»¤ë„ì„ ì ìš©í•˜ê²Œ ë˜ëŠ”ë° ì¶œë ¥ë˜ëŠ” feature mapì˜ í¬ê¸°ëŠ” (3, 3, 1)ì´ ì•„ë‹Œ ì›ë³¸ ì´ë¯¸ì§€ì™€ ë˜‘ê°™ì€ (5, 5, 1)í¬ê¸°ì˜ feature mapì´ë‹¤. ì´ë ‡ë“¯, ì›ë³¸ ì´ë¯¸ì§€ì˜ í¬ê¸°ë¥¼ ì¤„ì´ì§€ ì•Šìœ¼ë©´ì„œ í•©ì„±ê³± ì—°ì‚°ì„ ìˆ˜í–‰ê°€ëŠ¥í•˜ê²Œ í•´ì£¼ëŠ” ê²ƒì´ ë°”ë¡œ paddingì˜ ì—­í• ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤. 2.3 ReLU Activation Functioní•©ì„±ê³± ì—°ì‚°ì„ ê±°ì¹œ Feature Mapì€ í™œì„±í™” í•¨ìˆ˜ë¥¼ ê±°ì¹˜ê²Œ ë˜ëŠ”ë°, ë§ì€ í™œì„±í™” í•¨ìˆ˜ ì¤‘ ê°€ì¥ ë„ë¦¬ ì‚¬ìš©ë˜ê³  ìˆëŠ” ê²ƒì€ ReLU í•¨ìˆ˜ ì´ë‹¤. ì¼ë°˜ì ìœ¼ë¡œ Sigmoid í•¨ìˆ˜ì˜ ê²½ìš° ê°’ì„ 0 ~ 1ì‚¬ì´ë¡œ ì •ê·œí™”ì‹œí‚¤ëŠ”ë° ë ˆì´ì–´ê°€ ê¹Šì–´ì§ˆ ìˆ˜ ë¡ 0.xxxì˜ ê°’ì´ ê³„ì† ë¯¸ë¶„ë˜ê²Œ ë˜ë©´ ê°’ì´ ì ì°¨ 0ìœ¼ë¡œ ìˆ˜ë ´í•˜ê²Œ ë˜ì–´ ê²°êµ­ weightê°’ì´ í¬ë¯¸í•´ì§€ëŠ” gradient vanishingë¬¸ì œê°€ ë°œìƒí•˜ê²Œ ëœë‹¤.í•˜ì§€ë§Œ ReLUì˜ ê²½ìš° 0ë¯¸ë§Œì˜ ê°’ì€ 0ìœ¼ë¡œ ì¶œë ¥í•˜ê³  0ì´ìƒì˜ ê°’ì€ ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ê¸° ë•Œë¬¸ì— ì´ëŸ¬í•œ ë¬¸ì œì— ëœ ë¯¼ê°í•˜ê³  ê·¸ë ‡ê¸° ë•Œë¬¸ì— ê¹Šì€ ë ˆì´ì–´ì—ì„œë„ íš¨ìœ¨ì ì¸ ì—­ì „íŒŒ(Back Propagation)ê°€ ê°€ëŠ¥í•˜ë‹¤. ì´ëŸ¬í•œ ì´ìœ ë¡œ í•©ì„±ê³± ì—°ì‚°ì„ í†µí•´ ì¶œë ¥ëœ feature mapì˜ ê²½ìš° ì¼ë°˜ì ìœ¼ë¡œ ReLU í™œì„±í™” í•¨ìˆ˜ë¥¼ ê±°ì¹˜ê²Œ ë˜ë©°, ReLU í•¨ìˆ˜ê°€ ì–‘ì˜ ê°’ë§Œì„ í™œì„±í™”í•˜ë©° íŠ¹ì§•ì„ ì¢€ ë” ë‘ë“œëŸ¬ì§€ê²Œ í‘œí˜„í•´ì£¼ê²Œ ëœë‹¤. 3. Pooling LayerConvolutional Layerì™€ ìœ ì‚¬í•˜ê²Œ feature mapì˜ ì°¨ì›ì„ ë‹¤ìš´ ìƒ˜í”Œë§í•˜ì—¬ ì—°ì‚°ëŸ‰ì„ ê°ì†Œì‹œí‚¤ê³  ì£¼ìš”í•œ íŠ¹ì§• ë²¡í„°ë¥¼ ì¶”ì¶œí•˜ì—¬ í•™ìŠµì„ íš¨ê³¼ì ìœ¼ë¡œ í•˜ëŠ” ê²ƒì´ pooling layerì˜ ì—­í• ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤. í’€ë§ ì—°ì‚°ì—ëŠ” ëŒ€í‘œì ìœ¼ë¡œ ë‘ ê°€ì§€ê°€ ì‚¬ìš©ëœë‹¤. Max Pooling : ê° ì»¤ë„ì—ì„œ ë‹¤ë£¨ëŠ” ì´ë¯¸ì§€ íŒ¨ì¹˜ì—ì„œ ìµœëŒ€ê°’ì„ ì¶”ì¶œ Average Pooling: ê° ì»¤ë„ì—ì„œ ë‹¤ë£¨ëŠ” ì´ë¯¸ì§€ íŒ¨ì¹˜ì—ì„œ ëª¨ë“  ê°’ì˜ í‰ê· ì„ ë°˜í™˜ í•˜ì§€ë§Œ ëŒ€ë¶€ë¶„ì˜ ConvNetì—ì„œëŠ” Avg Poolingì´ ì•„ë‹Œ Max Poolingì´ ì‚¬ìš©ëœë‹¤. Avg Poolingì˜ ê²½ìš° ê° ì»¤ë„ì˜ ê°’ì„ í‰ê· í™”ì‹œí‚¤ê¸° ë•Œë¬¸ì— ì£¼ìš”í•œ ê°€ì¤‘ì¹˜ë¥¼ ê°–ëŠ” valueì˜ íŠ¹ì„±ì´ í¬ë¯¸í•´ì§ˆ ìˆ˜ ìˆëŠ” ë¬¸ì œê°€ ìˆê¸° ë•Œë¬¸ì´ë‹¤. ë˜í•œ Pooling ì‚¬ì´ì¦ˆì˜ ê²½ìš° Strideì™€ ê°™ì€ í¬ê¸°ë¡œ ì„¤ì •í•˜ì—¬ ëª¨ë“  ì›ì†Œê°€ í•œë²ˆì”© ì²˜ë¦¬ë˜ë„ë¡ í•˜ëŠ”ê²ƒì´ ì¼ë°˜ì ì´ë©°, ë³´í†µ Max Poolingì˜ ê²½ìš° 2 x 2ì»¤ë„ê³¼ 2 strideë¥¼ ì‚¬ìš©í•˜ì—¬ feature mapì„ ì ˆë°˜ í¬ê¸°ë¡œ ë‹¤ìš´ìƒ˜í”Œë§í•˜ê²Œ ëœë‹¤. 4. Fully Connected Layerìœ„ì—ì„œ ì„¤ëª…í•œ Convolutional Layer - ReLU Activation Function - Pooling Layerì˜ ê³¼ì •ì„ ê±°ì¹˜ë©° ì°¨ì›ì´ ì¶•ì†Œ ëœ feature mapì€ ìµœì¢…ì ìœ¼ë¡œ Fully Connected Layerë¼ëŠ” ì™„ì „ ì—°ê²° ì¸µìœ¼ë¡œ ì „ë‹¬ë˜ê²Œ ëœë‹¤. ì´ ë¶€ë¶„ì—ì„œëŠ” ì´ë¯¸ì§€ì˜ 3ì°¨ì› ë²¡í„°ëŠ” 1ì°¨ì›ìœ¼ë¡œ Flattenë˜ê²Œ ë˜ê³  ì‹ ê²½ë§ì—ì„œ í”íˆ ì‚¬ìš©ë˜ëŠ” í™œì„±í™” í•¨ìˆ˜(relu)ì™€ í•¨ê»˜ Output Layerë¡œ í•™ìŠµì´ ì§„í–‰ëœë‹¤.Output LayerëŠ” Softmax í™œì„±í™” í•¨ìˆ˜ê°€ ì‚¬ìš©ë˜ëŠ”ë°, Softmax í•¨ìˆ˜ëŠ” ì…ë ¥ë°›ì€ ê°’ì„ ëª¨ë‘ 0 ~ 1ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ì •ê·œí™”í•˜í•˜ê³  ì´ë ‡ê²Œ ì •ê·œí™”ëœ ê°’ë“¤ì˜ ì´í•©ì€ í•­ìƒ 1ì´ë˜ëŠ” íŠ¹ì„±ì„ ê°€ì§€ëŠ” í•¨ìˆ˜ì´ë‹¤. ë”°ë¼ì„œ ë§ˆì§€ë§‰ Output layerì˜ softmaxí•¨ìˆ˜ë¥¼ í†µí•´ ì´ë¯¸ì§€ê°€ ê° ë ˆì´ë¸”ì— ì†í•  í™•ë¥ ê°’ì´ ë ˆì´ë¸”ë§ˆë‹¤ ê°ê° ì¶œë ¥ë˜ê²Œ ë˜ê³  ì´ì¤‘ ê°€ì¥ ë†’ì€ í™•ë¥ ê°’ì„ ê°€ì§€ëŠ” ë ˆì´ë¸”ì´ ìµœì¢… ì˜ˆì¸¡ì¹˜ë¡œ ì„ ì •ë˜ê²Œ ëœë‹¤. References https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53 https://sonofgodcom.wordpress.com/2018/12/31/cnn%EC%9D%84-%EC%9D%B4%ED%95%B4%ED%95%B4%EB%B3%B4%EC%9E%90-fully-connected-layer%EB%8A%94-%EB%AD%94%EA%B0%80/ https://medium.com/dataseries/basic-overview-of-convolutional-neural-network-cnn-4fcc7dbb4f17 https://de-novo.org/2018/05/27/convnet-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0/ https://reniew.github.io/10/","categories":[],"tags":[{"name":"cnn","slug":"cnn","permalink":"https://jaehyeongan.github.io/tags/cnn/"},{"name":"image","slug":"image","permalink":"https://jaehyeongan.github.io/tags/image/"},{"name":"deeplearing","slug":"deeplearing","permalink":"https://jaehyeongan.github.io/tags/deeplearing/"},{"name":"convnet","slug":"convnet","permalink":"https://jaehyeongan.github.io/tags/convnet/"},{"name":"convolutional","slug":"convolutional","permalink":"https://jaehyeongan.github.io/tags/convolutional/"},{"name":"kernel","slug":"kernel","permalink":"https://jaehyeongan.github.io/tags/kernel/"},{"name":"padding","slug":"padding","permalink":"https://jaehyeongan.github.io/tags/padding/"},{"name":"maxpooling","slug":"maxpooling","permalink":"https://jaehyeongan.github.io/tags/maxpooling/"},{"name":"relu","slug":"relu","permalink":"https://jaehyeongan.github.io/tags/relu/"}]},{"title":"[Kaggle] ë¶„ì íŠ¹ì„± ì˜ˆì¸¡(Predicting Molecular Properties)","slug":"molecular-prediction","date":"2019-09-06T14:20:52.000Z","updated":"2021-02-07T14:57:14.051Z","comments":true,"path":"2019/09/06/molecular-prediction/","link":"","permalink":"https://jaehyeongan.github.io/2019/09/06/molecular-prediction/","excerpt":"","text":"Introìµœê·¼ kaggleì—ì„œ êµ‰ì¥íˆ ëˆˆì— ë„ëŠ” competitionì´ ìˆì—ˆìœ¼ë‹ˆ ë°”ë¡œ, Predicting Molecular Propertiesë¼ëŠ” ì´ë¦„ì˜ ëŒ€íšŒì˜€ë‹¤. í•´ë‹¹ competitionì€ ë¸Œë¦¬ìŠ¤í†¨ ëŒ€í•™êµ, ì¹´ë””í”„ ëŒ€í•™êµ, ì„í˜ë¦¬ì–¼ ì¹¼ë¦¬ì§€ ë° ë¦¬ì¦ˆ ëŒ€í•™êµë¡œ ì´ë£¨ì–´ì§„ CHAMPS(CHemistry And Mathematics in Phase Space) ì— ì˜í•´ ì£¼ìµœë˜ì—ˆìœ¼ë©°, ìˆ˜ìƒí•˜ëŠ” íŒ€ì—ê²ŒëŠ” ëŒ€í•™ ì—°êµ¬ í”„ë¡œê·¸ë¨ê³¼ í˜‘ë ¥í•  ìˆ˜ ìˆëŠ” ê¸°íšŒê°€ ì£¼ì–´ì§„ë‹¤ê³  í•œë‹¤. ì˜ˆì¸¡ ëŒ€ìƒìš°ì„  í•´ë‹¹ ëŒ€íšŒì˜ ë„ì „ê³¼ì œëŠ” ì†Œì œëª© ë° Descriptionì„ í†µí•´ íŒŒì•…í•  ìˆ˜ ìˆë‹¤. Can you measure the magnetic interactions between a pair of atoms?In this competition, you will develop an algorithm that can predict the magnetic interaction between two atoms in a molecule ì´ë²ˆ ëŒ€íšŒë¥¼ í†µí•´ ìš°ë¦¬ê°€ ì˜ˆì¸¡ í•´ì•¼ í•˜ëŠ” ê²ƒì€ ë°”ë¡œ ë¶„ì ì„¤ê³„ ì‹œ í•œ ìŒì˜ ì›ì ê°„ ê²°í•©ìœ¼ë¡œ ì¸í•´ ë°œìƒí•˜ëŠ” ê²°í•©ìƒìˆ˜(Coupling Constant)ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ë‹¤.ê²°í•© ìƒìˆ˜ë¼ëŠ” ê²ƒì€ ë¬¼ë¦¬ì  ìƒí˜¸ì‘ìš©(ì—¬ê¸°ì„œëŠ” ì›ì ê°„)ì˜ ì„¸ê¸°ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ìƒìˆ˜ë¡œì„œ, ê²°í•©ìƒìˆ˜ê°€ 1ì¼ ë•Œ ì™„ì „ê²°í•©ì´ë¼ê³  í•œë‹¤. ì•„ë˜ì—ì„œ ì¢€ ë” ìì„¸íˆ ì‚´í´ë³´ê² ì§€ë§Œ, ì œê³µë˜ëŠ” ë°ì´í„°ì—ëŠ” ë¶„ì ë° ì›ìì— ëŒ€í•œ ì •ë³´ê°€ ìˆìœ¼ë©° ë‘ ì›ì ê°„ì˜ ê²°í•©ìƒìˆ˜ê°€ target valueë¡œ ì¡´ì¬í•˜ê³  ìˆë‹¤. í•™ìŠµ ì „ëµì²˜ìŒ ì œê³µëœ ë°ì´í„°ë¥¼ ë³´ì•˜ì„ ë•Œ train, test ì™¸ì— ì¶”ê°€ë¡œ ì œê³µë˜ëŠ” ë°ì´í„°ë¥¼ ì–´ë–»ê²Œ í™œìš©í•´ì•¼ í• ì§€ ë‚œê°í–ˆë‹¤. ê·¸ ì´ìœ ëŠ” structures ë°ì´í„°ë¥¼ ì œì™¸í•˜ê³ ëŠ” ëª¨ë‘ train ë°ì´í„°ì— ëŒ€í•œ ì •ë³´ ë°–ì— ì—†ì—ˆê¸° ë–„ë¬¸ì´ë‹¤. ëª¨ë¸ì„ í•™ìŠµí•˜ê³  ì˜ˆì¸¡í•  ë•Œ ë‹¹ì—°íˆ train setê³¼ test setì˜ ì°¨ì›ì˜ í¬ê¸°ê°€ ê°™ì•„ì•¼ í–ˆê¸° ë•Œë¬¸ì— trainì— ëŒ€í•œ ì •ë³´ë§Œ ìˆëŠ” ë°ì´í„°ë¥¼ í™œìš©í•˜ëŠ” ê²ƒì´ ì˜ë¯¸ê°€ ì—†ë‹¤ê³  íŒë‹¨ë˜ì—ˆë‹¤. ê·¸ë˜ì„œ ìµœëŒ€í•œ í™œìš©í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ë§Œ ì‚¬ìš©í•˜ì˜€ìœ¼ë©° ëª‡ ê°€ì§€ íŒŒìƒë³€ìˆ˜ë¥¼ ë§Œë“¤ì–´ ë¶€ì¡±í•œ ì°¨ì›ì„ ì±„ì›Œì£¼ì—ˆë‹¤. ëª¨ë¸ í•™ìŠµì„ ìœ„í•´ì„œëŠ” LightGBMì´ë¼ëŠ” ìµœê·¼ ìºê¸€ì—ì„œ ê°€ì¥ ì¸ê¸° ìˆëŠ” Gradient Boosting ê¸°ë°˜ì˜ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì˜€ë‹¤. í•´ë‹¹ ë°ì´í„°ì—ëŠ” type ì´ë¼ëŠ” ë¶„ìì˜ íƒ€ì…ì„ êµ¬ë¶„í•˜ëŠ” ì¹¼ëŸ¼ì´ ì¡´ì¬í•˜ëŠ”ë°, ì²˜ìŒ ëª¨ë¸ì„ ë§Œë“¤ ë•ŒëŠ” type êµ¬ë¶„ ì—†ì´ ì „ì²´ë¥¼ í•™ìŠµì‹œì¼°ìœ¼ë‚˜ ì„±ëŠ¥ì´ ê¸°ëŒ€ë§Œí¼ ì˜ ë‚˜ì˜¤ì§€ ì•Šì•˜ë‹¤. ê·¸ë˜ì„œ featureë¥¼ ëŠ˜ë ¤ì•¼ í•˜ë‚˜ ê³ ë¯¼í•˜ë˜ ì¤‘ ìš°ì—°íˆ Nanashië¼ëŠ” ì‚¬ëŒì˜ kernelì—ì„œ ì „ì²´ ë¶„ìë¥¼ í•™ìŠµì‹œí‚¤ì§€ ì•Šê³  ë¶„ìì˜ typeë³„ë¡œ ë”°ë¡œ í•™ìŠµ ë° ì˜ˆì¸¡ì„ ì§„í–‰í•˜ëŠ” ê²ƒì„ ë³´ê²Œ ë˜ì—ˆë‹¤. scoreë¥¼ ë³´ë‹ˆ ìƒë‹¹íˆ ë†’ì€ scoreë¥¼ ê°€ì§€ê³  ìˆì—ˆê³  ì‹œë„í•´ë³¼ ë§Œ í•œ ê°€ì¹˜ê°€ ìˆë‹¤ê³  íŒë‹¨ë˜ì–´ ì´ë²ˆ ëª¨ë¸ì— ë²¤ì¹˜ë§ˆí‚¹í•˜ì—¬ ì ìš©í•˜ì˜€ë‹¤. í‰ê°€ ë°©ë²•ì´ë²ˆ ëŒ€íšŒì—ì„œëŠ” Evaluationì„ ìœ„í•´ í‰ê· ì ˆëŒ€ì˜¤ì°¨(MAE, Mean Absolute Error)ì— logê°’ì„ ì”Œìš´ ì ìˆ˜ë¡œ í‰ê°€ë¥¼ ì§„í–‰í•˜ê²Œ ëœë‹¤. ê³µì‹ metricì€ ì•„ë˜ì™€ ê°™ìœ¼ë©°, ì™„ë²½í•˜ê²Œ ì˜ˆì¸¡í–ˆì„ ë•Œ ìµœì¢… ì ìˆ˜ëŠ” -20.7232ì´ë‹¤. !ì½”ë“œ ì‘ì„±ì€ Jupyter labì„ ì´ìš©í•˜ì˜€ìœ¼ë©°, ì•„ë˜ ì‘ì„±ëœ ì½”ë“œëŠ” ipynbíŒŒì¼ì„ markdownìœ¼ë¡œ ë³€í™˜ í›„ ì—…ë¡œë“œí•œ ê²ƒì´ë‹¤. 1234567891011121314import pandas as pdimport numpy as npimport osimport seaborn as snsimport matplotlib.pyplot as pltfrom mpl_toolkits.mplot3d import Axes3Dfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import LabelEncoderimport lightgbm as lgb# pathpath_dir = 'C:/Users/USER/.kaggle/competitions/champs-scalar-coupling/'file_list = os.listdir(path_dir)file_list [â€˜dipole_moments.csvâ€™, â€˜magnetic_shielding_tensors.csvâ€™, â€˜mulliken_charges.csvâ€™, â€˜potential_energy.csvâ€™, â€˜sample_submission.csvâ€™, â€˜scalar_coupling_contributions.csvâ€™, â€˜structures.csvâ€™, â€˜structures.zipâ€™, â€˜test.csvâ€™, â€˜train.csvâ€™] 1. Load Train/Test DataColumns molecule_name : ë¶„ì ì´ë¦„ atom_index_0 / atom_index_1 : ì›ì ì¸ë±ìŠ¤ type Coupling Constant(ê²°í•©ìƒìˆ˜) : ë¬¼ë¦¬ì  ìƒí˜¸ì‘ìš©(ì—¬ê¸°ì„œëŠ” ì›ì ê°„)ì˜ ì„¸ê¸°ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ìƒìˆ˜, ê²°í•©ìƒìˆ˜ê°€ 1ì¼ë•Œ ì™„ì „ê²°í•©ì´ë¼ê³  í•¨ 12345train_df = pd.read_csv(path_dir+'train.csv')test_df = pd.read_csv(path_dir+'test.csv') # target = 'scalar_coupling_constant'print('Length of train set: &#123;&#125;'.format(len(train_df)))print('Length of test set: &#123;&#125;'.format(len(test_df))) Length of train set: 4658147Length of test set: 2505542 12print('Unique molecule of train set: &#123;&#125;'.format(len(train_df['molecule_name'].unique())))train_df.head() Unique molecule of train set: 85003 12print('Unique molecule of test set: &#123;&#125;'.format(len(test_df['molecule_name'].unique())))test_df.head() Unique molecule of test set: 45772 2. EDA2.1 Distribution of Target (â€˜scalar_coupling_constantâ€™) Min Value : -36.2186 Max Value : 204.88 ëŒ€ë¶€ë¶„ì´ -20 ~ +20 ì‚¬ì´ì— ì¡´ì¬ ì‘ì€ ë¶„í¬ë¡œ 80 ~ 100 ì‚¬ì´ì— ì¡´ì¬ 12345678# Distribution of targetprint('Min Value of Target : &#123;&#125;'.format(train_df['scalar_coupling_constant'].min()))print('Max Value of Target : &#123;&#125;'.format(train_df['scalar_coupling_constant'].max()))plt.figure(figsize=(11, 5))sns.distplot(train_df['scalar_coupling_constant'])plt.title('Distribution of scalar_coupling_constant')plt.show() Min Value of Target : -36.2186Max Value of Target : 204.88 2.2 Distribution of â€˜scalar_coupling_constantâ€™ by type â€˜1JHCâ€™ typeì´ ìƒëŒ€ì ìœ¼ë¡œ ë†’ì€ scalar coupling ë²”ìœ„ì— ë¶„í¬(+66.6 ~ +204.8) â€˜2JHHâ€™ typeì´ ìƒëŒ€ì ìœ¼ë¡œ ë‚®ì€ scalar coupling ë²”ìœ„ì— ë¶„í¬(-35.1 ~ +11.8 1234567# Distribution of 'scalar_coupling_constant' by typeplt.figure(figsize=(14, 13))for i, t in enumerate(train_df['type'].unique()): plt.subplot(4,2, i+1) sns.distplot(train_df[train_df['type'] == t]['scalar_coupling_constant']) plt.title('Distribution of coupling constant by type '+ t) plt.tight_layout() 2.3 Count by â€˜typeâ€™ 3JHC, 2JHC, 1JHC, 3JHH, 2JHH, 3JHN, 2JHN, 1JHN ìˆœì„œë¡œ ë†’ìŒ 123456789# Count by 'type'type_index = train_df['type'].value_counts().indextype_cnt = train_df['type'].value_counts()plt.figure(figsize=(11, 4))sns.barplot(x=type_index, y=type_cnt)plt.xlabel('type'); plt.ylabel('Count')plt.title('Count by type')plt.tight_layout() 2.4 Count by atom index 0, 1 atom index 0ì˜ ê²½ìš° 9 ~ 18ë²ˆì´ ê°€ì¥ ë§ì´ ë¶„í¬ atom index 1ì˜ ê²½ìš° 1 ~ 8ë²ˆì´ ê°€ì¥ ë§ì´ ë¶„í¬ 12345678910# Count by atom index 0, 1for i in [0, 1]: atom_index = train_df['atom_index_'+str(i)].value_counts().index atom_cnt = train_df['atom_index_'+str(i)].value_counts() plt.figure(figsize=(11, 4)) sns.barplot(x=atom_index, y=atom_cnt) plt.xlabel('atom index '+str(i)); plt.ylabel('Count') plt.title('Count by atom index '+str(i)) plt.tight_layout() 3. Load Structures DataColumns molecule_name atom_index atom x, y, z axis of atom 1234structures_df = pd.read_csv(path_dir+'structures.csv')print('Length of test set: &#123;&#125;'.format(len(structures_df)))structures_df.head() Length of test set: 2358657 3.1. 3Dimension plot by Molecule1234567891011for name in structures_df['molecule_name'].unique()[:4]: structures_molecule =structures_df[structures_df['molecule_name'] == name] fig = plt.figure(figsize=(8, 5)) ax = fig.add_subplot(111, projection='3d') ax.scatter(structures_molecule['x'], structures_molecule['y'], structures_molecule['z'], s=200, edgecolors='white') ax.set_title(str(name)+ ' 3D plot') ax.set_xlabel('x') ax.set_ylabel('y') ax.set_zlabel('z') plt.show() 4. Preprocessing4.1. Merge Train&amp;Test - Structures Data12345678910111213def mapping_atom_index(df, atom_idx): atom_idx = str(atom_idx) df = pd.merge(df, structures_df, left_on = ['molecule_name', 'atom_index_'+atom_idx], right_on = ['molecule_name', 'atom_index'], how = 'left') df = df.drop('atom_index', axis=1) df = df.rename(columns=&#123;'atom': 'atom_'+atom_idx, 'x': 'x_'+atom_idx, 'y': 'y_'+atom_idx, 'z': 'z_'+atom_idx&#125;) return df 12345train_merge = mapping_atom_index(train_df, 0)train_merge = mapping_atom_index(train_merge, 1)test_merge = mapping_atom_index(test_df, 0)test_merge = mapping_atom_index(test_merge, 1) 1234train_tmp = train_merge[['id','molecule_name','type']]test_tmp = test_merge[['id','molecule_name','type']]train_merge.head() 4.2. Derived variables - â€˜Distanceâ€™ distance between x axis of atom index distance between y axis of atom index distance between z axis of atom index distance between atom 12345678910111213def dist_between_atom(df): # distance between axis of atom df['x_dist'] = (df['x_0'] - df['x_1'])**2 df['y_dist'] = (df['y_0'] - df['y_1'])**2 df['z_dist'] = (df['z_0'] - df['z_1'])**2 # distance between atom df['atom_dist'] = (df['x_dist']+df['y_dist']+df['z_dist'])**0.5 return df train_dist = dist_between_atom(train_merge)test_dist = dist_between_atom(test_merge) 1train_dist.head() 4.3. Label encoding type, atom_0, atom_1 12345678910# Label encodingcategorical_features = ['type', 'atom_0', 'atom_1']for col in categorical_features: le = LabelEncoder() le.fit(list(train_dist[col].values) + list(test_dist[col].values)) train_dist[col] = le.transform(list(train_dist[col].values)) test_dist[col] = le.transform(list(test_dist[col].values))train_le = train_dist.copy()test_le = test_dist.copy() 1train_le.head() 4.4. Standardization z = (x - u) / s 12345# traintrain_data = train_le.drop(['id','molecule_name','scalar_coupling_constant'], axis=1)train_target = train_le['scalar_coupling_constant']# testtest_data = test_le.drop(['id','molecule_name',], axis=1) 1234# z-score standardizationtrain_scale = (train_data - train_data.mean()) / train_data.std()train_scale = train_scale.fillna(0)test_scale = (test_data - train_data.mean()) / train_data.std() 4.5. Variable Correlations123456789train_corr = train_scale.copy()train_corr['scalar_coupling_constant'] = train_targetcorrmat = train_corr.corr()top_corr_features = corrmat.index[abs(corrmat['scalar_coupling_constant']) &gt;= 0.1]plt.figure(figsize=(10,7))sns.heatmap(train_corr[top_corr_features].corr(), annot=True, cmap=\"RdYlGn\")plt.title('Variable Correlations')plt.show() 5. Training Model5.1. Training by â€˜typeâ€™ through LightGBM123456train_scale = train_scale.drop('type', axis=1)train_scale['type'] = train_tmp['type']train_scale['scalar_coupling_constant'] = train_targettest_scale = test_scale.drop('type', axis=1)test_scale[['id', 'type']] = test_tmp[['id', 'type']] 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859score_by_type = [] # List of Validation score by type feature_importance_df = []test_pred_df = pd.DataFrame(columns=['id', 'scalar_coupling_constant']) # Dataframe for submission# Extract data by typetypes = train_tmp['type'].unique()for typ in types: print('---Type of '+str(typ)+'---') train = train_scale[train_scale['type'] == typ] target = train['scalar_coupling_constant'] train = train.drop(['type','scalar_coupling_constant'], axis=1) # Split train set / valid set x_train, x_val, y_train, y_val = train_test_split(train, target, random_state=42) # LightGBM categorical_features = ['atom_0','atom_1'] lgb_train = lgb.Dataset(x_train, y_train, categorical_feature=categorical_features) lgb_val = lgb.Dataset(x_val, y_val, categorical_feature=categorical_features) # Parameters of LightGBM params = &#123;'num_leaves': 128, 'min_child_samples': 79, 'objective': 'regression', 'max_depth': 9, 'learning_rate': 0.1, \"boosting_type\": \"gbdt\", \"subsample_freq\": 1, \"subsample\": 0.9, \"bagging_seed\": 11, \"metric\": 'mae', \"verbosity\": -1, 'reg_alpha': 0.13, 'reg_lambda': 0.36, 'colsample_bytree': 1.0 &#125; # Training lgb_model = lgb.train(params, lgb_train, valid_sets=[lgb_train, lgb_val], num_boost_round=20000, # Number of boosting iterations. early_stopping_rounds=500, # early stopping for valid set verbose_eval=2500) # eval metric on the valid set is printed at 1000 each boosting # Feature Importances feature_importance = lgb_model.feature_importance() df_fi = pd.DataFrame(&#123;'columns':x_train.columns, 'importances':feature_importance&#125;) df_fi = df_fi[df_fi['importances'] &gt; 0].sort_values(by=['importances'], ascending=False) feature_importance_df.append(df_fi) # Predict Validation set score_by_type.append(list(lgb_model.best_score['valid_1'].values())) # Predict Test set test = test_scale[test_scale['type'] == typ] test_id = test['id'] test = test.drop(['id','type'], axis=1) test_preds = lgb_model.predict(test) test_pred_df = pd.concat([test_pred_df, pd.DataFrame(&#123;'id':test_id, 'scalar_coupling_constant':test_preds&#125;)], axis=0) â€”-Type of 1JHCâ€”-Training until validation scores donâ€™t improve for 500 rounds.[2500] trainingâ€™s l1: 2.77031 valid_1â€™s l1: 3.67976[5000] trainingâ€™s l1: 2.19648 valid_1â€™s l1: 3.59536[7500] trainingâ€™s l1: 1.81083 valid_1â€™s l1: 3.56509[10000] trainingâ€™s l1: 1.52513 valid_1â€™s l1: 3.55207[12500] trainingâ€™s l1: 1.30189 valid_1â€™s l1: 3.54733Early stopping, best iteration is:[12398] trainingâ€™s l1: 1.31009 valid_1â€™s l1: 3.54716â€”-Type of 2JHHâ€”-Training until validation scores donâ€™t improve for 500 rounds.[2500] trainingâ€™s l1: 0.599486 valid_1â€™s l1: 0.930904[5000] trainingâ€™s l1: 0.429952 valid_1â€™s l1: 0.920848Early stopping, best iteration is:[6444] trainingâ€™s l1: 0.363731 valid_1â€™s l1: 0.919744â€”-Type of 1JHNâ€”-Training until validation scores donâ€™t improve for 500 rounds.[2500] trainingâ€™s l1: 0.574269 valid_1â€™s l1: 1.869Early stopping, best iteration is:[2790] trainingâ€™s l1: 0.51238 valid_1â€™s l1: 1.86748â€”-Type of 2JHNâ€”-Training until validation scores donâ€™t improve for 500 rounds.[2500] trainingâ€™s l1: 0.493351 valid_1â€™s l1: 1.29156[5000] trainingâ€™s l1: 0.245368 valid_1â€™s l1: 1.26562[7500] trainingâ€™s l1: 0.136667 valid_1â€™s l1: 1.25931[10000] trainingâ€™s l1: 0.0806357 valid_1â€™s l1: 1.25729[12500] trainingâ€™s l1: 0.0501038 valid_1â€™s l1: 1.25649[15000] trainingâ€™s l1: 0.0325061 valid_1â€™s l1: 1.25602Early stopping, best iteration is:[16603] trainingâ€™s l1: 0.025108 valid_1â€™s l1: 1.25588â€”-Type of 2JHCâ€”-Training until validation scores donâ€™t improve for 500 rounds.[2500] trainingâ€™s l1: 1.45707 valid_1â€™s l1: 1.78894[5000] trainingâ€™s l1: 1.22333 valid_1â€™s l1: 1.74951[7500] trainingâ€™s l1: 1.0595 valid_1â€™s l1: 1.734[10000] trainingâ€™s l1: 0.931867 valid_1â€™s l1: 1.72621[12500] trainingâ€™s l1: 0.827693 valid_1â€™s l1: 1.7222[15000] trainingâ€™s l1: 0.740337 valid_1â€™s l1: 1.72091[17500] trainingâ€™s l1: 0.665914 valid_1â€™s l1: 1.7203Early stopping, best iteration is:[17624] trainingâ€™s l1: 0.662523 valid_1â€™s l1: 1.72024â€”-Type of 3JHHâ€”-Training until validation scores donâ€™t improve for 500 rounds.[2500] trainingâ€™s l1: 0.750457 valid_1â€™s l1: 1.084[5000] trainingâ€™s l1: 0.549578 valid_1â€™s l1: 1.04283[7500] trainingâ€™s l1: 0.427516 valid_1â€™s l1: 1.02508[10000] trainingâ€™s l1: 0.343351 valid_1â€™s l1: 1.01595[12500] trainingâ€™s l1: 0.281535 valid_1â€™s l1: 1.01117[15000] trainingâ€™s l1: 0.234395 valid_1â€™s l1: 1.00805[17500] trainingâ€™s l1: 0.197292 valid_1â€™s l1: 1.00612[20000] trainingâ€™s l1: 0.167506 valid_1â€™s l1: 1.00516Did not meet early stopping. Best iteration is:[20000] trainingâ€™s l1: 0.167506 valid_1â€™s l1: 1.00516â€”-Type of 3JHCâ€”-Training until validation scores donâ€™t improve for 500 rounds.[2500] trainingâ€™s l1: 1.14564 valid_1â€™s l1: 1.35361[5000] trainingâ€™s l1: 0.957949 valid_1â€™s l1: 1.28948[7500] trainingâ€™s l1: 0.830959 valid_1â€™s l1: 1.2569[10000] trainingâ€™s l1: 0.735763 valid_1â€™s l1: 1.23707[12500] trainingâ€™s l1: 0.65866 valid_1â€™s l1: 1.22372[15000] trainingâ€™s l1: 0.594712 valid_1â€™s l1: 1.2142[17500] trainingâ€™s l1: 0.540621 valid_1â€™s l1: 1.2074[20000] trainingâ€™s l1: 0.493743 valid_1â€™s l1: 1.20232Did not meet early stopping. Best iteration is:[20000] trainingâ€™s l1: 0.493743 valid_1â€™s l1: 1.20232â€”-Type of 3JHNâ€”-Training until validation scores donâ€™t improve for 500 rounds.[2500] trainingâ€™s l1: 0.231975 valid_1â€™s l1: 0.513043[5000] trainingâ€™s l1: 0.127354 valid_1â€™s l1: 0.502675[7500] trainingâ€™s l1: 0.0764305 valid_1â€™s l1: 0.49964[10000] trainingâ€™s l1: 0.0486108 valid_1â€™s l1: 0.498466[12500] trainingâ€™s l1: 0.0325766 valid_1â€™s l1: 0.498049[15000] trainingâ€™s l1: 0.0228697 valid_1â€™s l1: 0.497765[17500] trainingâ€™s l1: 0.0167369 valid_1â€™s l1: 0.497581[20000] trainingâ€™s l1: 0.0128106 valid_1â€™s l1: 0.497526Did not meet early stopping. Best iteration is:[20000] trainingâ€™s l1: 0.0128106 valid_1â€™s l1: 0.497526 5.2. Validation MAE by type1234for typ, score in zip(types, score_by_type): print('Type &#123;&#125; valid MAE : &#123;&#125;'.format(str(typ), score))print('\\nAverage of valid MAE : &#123;&#125;'.format(np.mean(score_by_type))) Type 1JHC valid MAE : [3.5471584407190475]Type 2JHH valid MAE : [0.9197439377103146]Type 1JHN valid MAE : [1.8674786631630775]Type 2JHN valid MAE : [1.255876548899015]Type 2JHC valid MAE : [1.7202390170123096]Type 3JHH valid MAE : [1.0051635344922942]Type 3JHC valid MAE : [1.2023186835296467]Type 3JHN valid MAE : [0.4975260038664571] Average of valid MAE : 1.5019381036740203 5.3. Feature Importances Plot by Type1234567for typ, df_fi in zip(types, feature_importance_df): fig = plt.figure(figsize=(12, 6)) ax = sns.barplot(df_fi['columns'], df_fi['importances']) ax.set_xticklabels(df_fi['columns'], rotation=80, fontsize=13) plt.title('Type '+str(typ)+' feature importance') plt.tight_layout() plt.show() 5.4. Save prediction of test set to *.csv1test_pred_df.head(10) 1test_pred_df.to_csv('lgb_submission.csv', index=False) Referenceskaggle kernels https://www.kaggle.com/jesucristo/single-lgbm-2-242-top54 https://www.kaggle.com/super13579/simple-eda-and-lightgbm https://www.kaggle.com/artgor/molecular-properties-eda-and-models blog/docs https://gorakgarak.tistory.com/1285 https://towardsdatascience.com/understanding-gradient-boosting-machines-using-xgboost-and-lightgbm-parameters-3af1f9db9700 https://lightgbm.readthedocs.io/en/latest/ Outroì²˜ìŒ ì˜ˆì¸¡ ëª¨ë¸ì„ ìœ„í•´ì„œ Neural Netì„ ì´ìš©í•˜ì˜€ì—ˆëŠ”ë° default parameterì˜ Random Forest ì•Œê³ ë¦¬ì¦˜ë³´ë‹¤ë„ í›¨ì”¬ ë‚®ì€ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤. ì–´ëŠ ê¸€ì—ì„œ ë§í•˜ê¸¸, ë”¥ëŸ¬ë‹ì´ í•­ìƒ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ì§€ ì•ŠëŠ”ë‹¤ê³  í•œë‹¤.ì´ëŸ° Structured tabular í˜•íƒœì˜ ë°ì´í„°ì— neural netì€ over-fitting í•˜ëŠ” ê²½ìš°ê°€ ë§ìœ¼ë©°, ê±°ì˜ ëŒ€ë¶€ë¶„ì˜ ê²½ìš° xgboostë‚˜ lightgbmê³¼ ê°™ì€ gradient boosting ê³„ì—´ì˜ ì•Œê³ ë¦¬ì¦˜ì´ ì˜ ì‘ë™í•œë‹¤ê³  í•œë‹¤.(íŒŒë¼ë¯¸í„°ê°’ì„ ì˜ optimize í–ˆì„ ë•Œ..) ì•ìœ¼ë¡œ gbm ê³„ì—´ì˜ ì•Œê³ ë¦¬ì¦˜ì— ëŒ€í•´ ì¢€ë” ê³µë¶€í•´ë´ì•¼í•  ê²ƒ ê°™ë‹¤.","categories":[],"tags":[{"name":"kaggle","slug":"kaggle","permalink":"https://jaehyeongan.github.io/tags/kaggle/"},{"name":"molecular","slug":"molecular","permalink":"https://jaehyeongan.github.io/tags/molecular/"},{"name":"atom","slug":"atom","permalink":"https://jaehyeongan.github.io/tags/atom/"},{"name":"couplingconstant","slug":"couplingconstant","permalink":"https://jaehyeongan.github.io/tags/couplingconstant/"},{"name":"competitions","slug":"competitions","permalink":"https://jaehyeongan.github.io/tags/competitions/"},{"name":"lightgbm","slug":"lightgbm","permalink":"https://jaehyeongan.github.io/tags/lightgbm/"},{"name":"eda","slug":"eda","permalink":"https://jaehyeongan.github.io/tags/eda/"}]},{"title":"ë°ì´í„° ë¶„ì„ì„ ìœ„í•œ ê¸°ì´ˆ ì‹œê°í™” with Python","slug":"Basic-Visualization-Analytics","date":"2019-08-13T14:00:16.000Z","updated":"2020-12-10T14:50:45.000Z","comments":true,"path":"2019/08/13/Basic-Visualization-Analytics/","link":"","permalink":"https://jaehyeongan.github.io/2019/08/13/Basic-Visualization-Analytics/","excerpt":"","text":"Introë°ì´í„°ë¥¼ ë¶„ì„í•˜ë ¤ëŠ”ë° ë°ì´í„°ì˜ rowì™€ columns ìˆ˜ê°€ ë§ì€ ìˆ˜ë°± ì°¨ì› ë°ì´í„°ì˜ ê²½ìš° ë°ì´í„°ë¥¼ íŒŒì•…í•˜ê¸°ê°€ ì‰½ì§€ ì•Šë‹¤. ê·¸ë ‡ê¸°ì— ì¸ê°„ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ì •ë„ì˜ ì°¨ì›ìœ¼ë¡œ ì¤„ì—¬ ë°ì´í„°ë¥¼ ê°œëµì ìœ¼ë¡œ íŒŒì•…í•˜ëŠ” ê²ƒì´ í•„ìš”í•˜ê³ , ì—­ì‹œ ì¸ê°„ì€ ì½ê³ , ë“£ëŠ” ê²ƒ ë³´ë‹¤ëŠ” ëˆˆìœ¼ë¡œ ë³´ëŠ”ê²Œ í™•ì‹¤íˆ ê¸°ì–µì— ì˜¤ë˜ë‚¨ê³  ì´í•´í•˜ê¸° ì‰½ê¸° ë•Œë¬¸ì— ë°ì´í„°ë¥¼ ì‹œê°í™”í•˜ì—¬ ë¶„ì„í•˜ëŠ” ê²ƒì´ í•„ìš”í•˜ë‹¤. ì´ë²ˆì—ëŠ” ë°ì´í„° ë¶„ì„ì— ì•ì„œ ê¸°ì´ˆì ì´ì§€ë§Œ í•„ìˆ˜ì ìœ¼ë¡œ ì‚´í´ë³´ì•„ í•  ì‹œê°í™” ë°©ë²•ì— ëŒ€í•´ ì‚´í´ë³¼ ê²ƒì´ë©°, ëª©ë¡ì€ ì•„ë˜ì™€ ê°™ë‹¤. ë³€ìˆ˜ ë³„ ë°ì´í„° ë¶„í¬(Data Distribution) íƒ€ê²Ÿ ë³„ 2ì°¨ì› ë° 3ì°¨ì› ì‹œê°í™”(2D and 3D plot) ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„(Corrleation) ë³€ìˆ˜ ì¤‘ìš”ë„(Featrue Importances) ì˜ˆì œë¡œ ì‚¬ìš© í•  ë°ì´í„°ë¡œ Breast Cancer Wisconsin Datasetì´ë‹¤. ë§ì´ë“¤ ì•Œë‹¤ì‹œí”¼ ìœ ë°©ì•”ì— ëŒ€í•´ ì–‘ì„±/ìŒì„±ì„ ì˜ˆì¸¡í•˜ê¸° ìœ„í•œ ë°ì´í„°ì…‹ì´ë©°, ì´ 569 rowì™€ 31 columnsì„ ê°€ì§€ê³  ìˆë‹¤. 0. Load Dataìš°ì„  ë°ì´í„°ë¥¼ ë¡œë“œ ì‹œí‚¨ í›„ ë¶„ì„ì— ë¶ˆí•„ìš”í•œ ì¹¼ëŸ¼ì€ ì œì™¸ì‹œí‚¨ë‹¤.ë°ì´í„°ëŠ” ì•„ë˜ì™€ ê°™ì€ í˜•íƒœë¡œ ë˜ì–´ ìˆë‹¤.12345import pandas as pd cancer = pd.read_csv('./input/data.csv')cancer.drop(['id','Unnamed: 32'], axis=1, inplace=True)cancer.head(10) ì´ ë°ì´í„°ì—ì„œ ì˜ˆì¸¡í•´ì•¼í•˜ëŠ” íƒ€ê²Ÿ ì¹¼ëŸ¼ì€ â€˜diagnosisâ€™ì´ë©°, â€˜Mâ€™ì€ malignantë¡œ ì–‘ì„±ì„ ì˜ë¯¸í•˜ë©°, â€˜Bâ€™ëŠ” Benignìœ¼ë¡œ ìŒì„±ì„ ì˜ë¯¸í•œë‹¤.1cancer['diagnosis'].unique() # array(['M', 'B'], dtype=object) 1. Column distribution by targetë¨¼ì € ì‹œê°í™” í•´ ë³¼ ê²ƒì€ ì¹¼ëŸ¼ ë³„ë¡œ ë°ì´í„° ë¶„í¬ë¥¼ ì‹œê°í™”í•´ ë³´ëŠ” ê²ƒì´ë‹¤. ì´ë¥¼ í†µí•´ ê° ì¹¼ëŸ¼ ë³„ë¡œ ë°ì´í„°ê°€ ì–´ë–»ê²Œ ë¶„í¬ë˜ì–´ ìˆëŠ”ì§€ë¥¼ íŒŒì•…í•  ìˆ˜ ìˆê³ , ìš°ë¦¬ê°€ ì˜ˆì¸¡í•˜ê³ ì í•˜ëŠ” íƒ€ê²Ÿ(diagnosis)ë³„ë¡œ ë¶„í¬ê°€ ì–´ë–»ê²Œ ë‹¤ë¥´ê²Œ ë‚˜íƒ€ë‚˜ëŠ”ì§€ë„ íŒŒì•…ì´ ê°€ëŠ¥í•˜ë‹¤. seabornì˜ distplotì„ í†µí•´ íƒ€ê²Ÿ ì¹¼ëŸ¼ì¸ diagnosisë³„ë¡œ 6ë²ˆì§¸ ì¹¼ëŸ¼ê¹Œì§€ë§Œ ì¶œë ¥í•´ë³´ì•˜ë‹¤.1234567891011121314151617181920import seaborn as snsimport matplotlib.pyplot as pltfrom matplotlib import font_manager, rcfont_name = font_manager.FontProperties(fname=\"c:/Windows/Fonts/malgun.ttf\").get_name()rc('font', family=font_name) # í•œê¸€ ì¶œë ¥ ì„¤ì • ë¶€ë¶„for cnt, col in enumerate(cancer): try: plt.figure(figsize=(10, 5)) sns.distplot(cancer[col][cancer['diagnosis']=='M']) sns.distplot(cancer[col][cancer['diagnosis']=='B']) plt.legend(['malignant','benign'], loc='best') plt.title('histogram of features '+str(col)) plt.show() if cnt &gt;= 6: # 6ê°œ ì¹¼ëŸ¼ê¹Œì§€ë§Œ ì¶œë ¥ break except Exception as e: pass ìœ„ì˜ ê·¸ë¦¼ìœ¼ë¡œ ë³´ì•˜ì„ ë•Œ radius_mean, area_mean, perimeter_mean ì¹¼ëŸ¼ì´ ì–‘ì„±ì¼ë•Œì™€ ìŒì„±ì¼ë•Œ ë¶„í¬ê°€ í¬ê²Œ ë‹¤ë¥¸ ê²ƒì„ ì•Œ ìˆ˜ ìˆê³ , íŠ¹íˆ area_mean ì¹¼ëŸ¼ì€ ë¶„í¬ê°€ ë„“ê²Œ í¼ì ¸ìˆëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. 2. 2 Dimension Plotì´ë²ˆì—ëŠ” ì§€ë‚œ ê¸€ì—ì„œ ì‚´í´ë³´ì•˜ë˜ ì°¨ì›ì¶•ì†Œ(Dimensionality Reduction) ê¸°ë²•ì„ ì´ìš©í•˜ì—¬ 2ì°¨ì›ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì‹œê°í™”í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ ì•Œì•„ë³´ê² ë‹¤. ìš°ì„  ë°ì´í„° ìŠ¤ì¼€ì¼ ë° ì°¨ì›ì¶•ì†Œ ê¸°ë²•ì¸ PCA(Principal Component Analysis)ë¥¼ ì ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ 2ì°¨ì›ìœ¼ë¡œ ë³€í™˜ì‹œì¼œì¤€ í›„, íƒ€ê²Ÿ(ìŒì„±/ì–‘ì„±)ë³„ë¡œ ë°ì´í„°ë¥¼ êµ¬ë¶„í•˜ì—¬ ì¶œë ¥í•˜ì˜€ë‹¤. 12345678from sklearn.preprocessing import StandardScaler# Data ScalingX = cancer.drop(['diagnosis'], axis=1)y = cancer['diagnosis']scaler = StandardScaler()cancer_scale = pd.DataFrame(scaler.fit_transform(X), columns=X.columns) 12345678910111213# plot 2Dfrom sklearn.decomposition import PCAimport matplotlib.pyplot as pltpca2 = PCA(n_components=2)data_pca2 = pca2.fit_transform(cancer_scale)plt.figure(figsize=(12, 8))plt.scatter(data_pca2[:,0], data_pca2[:,1], c=cancer['diagnosis'], s=40, edgecolors='white')plt.title(\"2D of Target distribution by diagnosis\")plt.xlabel('pcomp 1')plt.ylabel('pcomp 2')plt.show() 2ì°¨ì›ìœ¼ë¡œ í‘œí˜„í•´ë³¸ ê²°ê³¼ ì–‘ì„±ì¼ë•Œì™€ ìŒì„±ì¼ ë•Œ ê·¹ëª…í•˜ê²Œ ë¶„í¬ê°€ ë‚˜ë‰˜ëŠ” ê²ƒì„ í™•ì¸í•´ë³¼ ìˆ˜ ìˆë‹¤. 3. 3 Dimension Plotìœ„ì™€ ê°™ì€ ë°©ì‹ìœ¼ë¡œ PCAë¥¼ ì´ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ 3ì°¨ì›ìœ¼ë¡œ ë³€í™˜ í›„ ë°ì´í„°ë¥¼ íƒ€ê²Ÿ(ì–‘ì„±/ìŒì„±) ë³„ë¡œ ì‹œê°í™” í•œë‹¤.123456789101112131415from sklearn.decomposition import PCAimport matplotlib.pyplot as pltfrom mpl_toolkits.mplot3d import Axes3Dpca3 = PCA(n_components=3)data_pca3 = pca3.fit_transform(cancer_scale)fig = plt.figure(figsize=(12, 8))ax = fig.add_subplot(111, projection='3d')ax.scatter(data_pca3[:,0], data_pca3[:,1], data_pca3[:,2], c=cancer['diagnosis'], s=60, edgecolors='white')ax.set_title('3D of Target distribution by diagnosis')ax.set_xlabel('pcomp 1')ax.set_ylabel('pcomp 2')ax.set_zlabel('pcomp 3')plt.show() 4. Corrleation Heatmapì´ë²ˆì—ëŠ” ìƒê´€ê´€ê³„ ë¶„ì„ì„ í†µí•´ ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„ê°€ ì–¼ë§ˆë‚˜ ìˆëŠ”ì§€ íŒŒì•…í•´ë³¸ë‹¤. ì´ëŸ¬í•œ ìƒê´€ê´€ê³„ ë¶„ì„ì„ í†µí•´ íƒ€ê²Ÿ ê°’ì„ ì œì™¸í•œ íŠ¹ì • ë‘ ë³€ìˆ˜ê°€ ìƒê´€ê´€ê³„ê°€ 0.9 ì´ìƒì¼ ê²½ìš° ë‘ ë³€ìˆ˜ ì¤‘ í•˜ë‚˜ë¥¼ ì œê±°í•´ì£¼ëŠ” ê²ƒì´ ì¢‹ìœ¼ë©°, ë˜í•œ ì–´ë–¤ ë³€ìˆ˜ê°€ íƒ€ê²Ÿ ê°’ê³¼ ë†’ì€ ìƒê´€ì„±ì„ ê°€ì§€ëŠ”ì§€ íŒŒì•…í•˜ëŠ”ë°ë„ ìœ ìš©í•˜ê²Œ ì‚¬ìš©ëœë‹¤. corr()í•¨ìˆ˜ë¥¼ ì ìš©í•˜ì—¬ ë³€ìˆ˜ê°„ ìƒê´€ê´€ê³„ ë¶„ì„ í›„, ìƒê´€ê´€ê³„ê°€ 0.3ì´ìƒì¸ ë³€ìˆ˜ë§Œ ì¶œë ¥í•˜ì˜€ë‹¤.123456789101112131415import seaborn as snsimport matplotlib.pyplot as pltfrom matplotlib import font_manager, rcfont_name = font_manager.FontProperties(fname=\"c:/Windows/Fonts/malgun.ttf\").get_name()rc('font', family=font_name) # í•œê¸€ ì¶œë ¥ ì„¤ì • ë¶€ë¶„cancer_tmp = cancer.copy()cancer_tmp['diagnosis'] = cancer['diagnosis'].replace(&#123;'M':1, 'B':0&#125;)corrmat = cancer_tmp.corr()top_corr_features = corrmat.index[abs(corrmat[\"diagnosis\"])&gt;=0.3]# plotplt.figure(figsize=(13,10))g = sns.heatmap(cancer[top_corr_features].corr(), annot=True, cmap=\"RdYlGn\")plt.show() 5. Feature Importancesë¨¸ì‹ ëŸ¬ë‹ ë° ë”¥ëŸ¬ë‹ ì˜ˆì¸¡ í›„ ì–´ë–»ê²Œ ì´ëŸ¬í•œ ê²°ê³¼ê°€ ë‚˜ì™”ëŠ”ì§€ ì˜ë¬¸ì´ ë“¤ ë•Œê°€ ìˆë‹¤. ê·¸ëŸ´ ë• ë³€ìˆ˜ ì¤‘ìš”ë„ë¥¼ í†µí•´ ì–´ë–¤ ë³€ìˆ˜ê°€ ì˜ˆì¸¡ ì„±ëŠ¥ì— ì£¼ìš”í•˜ê²Œ ì˜í–¥ì„ ë¯¸ì³¤ëŠ”ì§€ íŒŒì•…í•  ìˆ˜ ìˆë‹¤. RandomForest ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ feature importancesë¥¼ ë½‘ì•„ë‚¸ í›„ ìƒìœ„ ì¤‘ìš”ë„ ë³„ë¡œ ì¤‘ìš”ë„ê°€ 0ì´ìƒë§Œ ì¶œë ¥í•˜ì˜€ë‹¤.12345678910111213141516171819202122from sklearn.ensemble import RandomForestClassifierimport seaborn as snsimport matplotlib.pyplot as pltfrom matplotlib import font_manager, rcfont_name = font_manager.FontProperties(fname=\"c:/Windows/Fonts/malgun.ttf\").get_name()rc('font', family=font_name) # í•œê¸€ ì¶œë ¥ ì„¤ì • ë¶€ë¶„# RandomForestclf = RandomForestClassifier(random_state=42, max_depth=6)clf.fit(X, y)feature_importance = clf.feature_importances_# plotdf_fi = pd.DataFrame(&#123;'columns':X.columns, 'importances':feature_importance&#125;)df_fi = df_fi[df_fi['importances'] &gt; 0] # importanceê°€ 0ì´ìƒì¸ ê²ƒë§Œ df_fi = df_fi.sort_values(by=['importances'], ascending=False)fig = plt.figure(figsize=(15,7))ax = sns.barplot(df_fi['columns'], df_fi['importances'])ax.set_xticklabels(df_fi['columns'], rotation=80, fontsize=13)plt.tight_layout()plt.show() ë³€ìˆ˜ ì¤‘ìš”ë„ ì¶œë ¥ê²°ê³¼ concave_points_worstê°€ 0.175ë¡œ ê°€ì¥ ì¤‘ìš”í•œ ì˜ˆì¸¡ ë³€ìˆ˜ì´ë©°, ê·¸ ë’¤ë¡œ perimeter_worst, perimeter_mean, radius_wordê°€ ì£¼ìš” ì˜ˆì¸¡ ë³€ìˆ˜ë¡œ ë‚˜íƒ€ë‚¬ë‹¤. Outroì´ë²ˆì—ëŠ” ê¸°ì´ˆì ì¸ ë°ì´í„° ì‹œê°í™”ë¥¼ ì•Œì•„ë³´ì•˜ëŠ”ë°, ë°ì´í„° ì‹œê°í™”ëŠ” ë°ì´í„°ì™€ ìƒí™©ì— ë”°ë¼ ê·¸ë•Œ ê·¸ë•Œ ì‹œê°í™”í•´ì•¼í•˜ëŠ” ìš”ì†Œê°€ ë‹¤ë¥´ê³  ë‹¤ì–‘í•˜ê¸° ë•Œë¬¸ì—, ìƒí™©ì— ë”°ë¼ ì›í•˜ëŠ” ê·¸ë¦¼ì„ ê·¸ë¦¬ë©° ìì‹ ë§Œì˜ ì¸ì‚¬ì´íŠ¸ë¥¼ ì°¾ì•„ë‚˜ê°€ë©´ ë  ë“¯ í•˜ë‹¤.","categories":[],"tags":[{"name":"visualization","slug":"visualization","permalink":"https://jaehyeongan.github.io/tags/visualization/"},{"name":"analytics","slug":"analytics","permalink":"https://jaehyeongan.github.io/tags/analytics/"},{"name":"matplotlib","slug":"matplotlib","permalink":"https://jaehyeongan.github.io/tags/matplotlib/"},{"name":"seaborn","slug":"seaborn","permalink":"https://jaehyeongan.github.io/tags/seaborn/"},{"name":"sklearn","slug":"sklearn","permalink":"https://jaehyeongan.github.io/tags/sklearn/"},{"name":"pca","slug":"pca","permalink":"https://jaehyeongan.github.io/tags/pca/"},{"name":"correaltion","slug":"correaltion","permalink":"https://jaehyeongan.github.io/tags/correaltion/"},{"name":"featureimportances","slug":"featureimportances","permalink":"https://jaehyeongan.github.io/tags/featureimportances/"},{"name":"breastcancer","slug":"breastcancer","permalink":"https://jaehyeongan.github.io/tags/breastcancer/"}]},{"title":"[Kaggle] ë³´ìŠ¤í„´ ì£¼íƒ ê°€ê²© ì˜ˆì¸¡(House Prices: Advanced Regression Techniques)","slug":"Kaggle-challenge-ë³´ìŠ¤í„´-ì§‘ê°’-ì˜ˆì¸¡-House-Prices-Advanced-Regression-Techniques","date":"2019-07-08T12:59:56.000Z","updated":"2021-02-07T14:57:00.417Z","comments":true,"path":"2019/07/08/Kaggle-challenge-ë³´ìŠ¤í„´-ì§‘ê°’-ì˜ˆì¸¡-House-Prices-Advanced-Regression-Techniques/","link":"","permalink":"https://jaehyeongan.github.io/2019/07/08/Kaggle-challenge-ë³´ìŠ¤í„´-ì§‘ê°’-ì˜ˆì¸¡-House-Prices-Advanced-Regression-Techniques/","excerpt":"","text":"Introìºê¸€ì˜ ê³ ì „ì ì¸ ë¬¸ì œì´ë©° ë¨¸ì‹ ëŸ¬ë‹ì„ ê³µë¶€í•˜ëŠ” ì‚¬ëŒì´ë¼ë©´ ëˆ„êµ¬ë‚˜ í•œë²ˆì¯¤ ë‹¤ë¤„ë´¤ì„ Boston house price Datasetì„ í†µí•´ regressioní•˜ëŠ” ê³¼ì •ì„ ì†Œê°œí•˜ë ¤ í•œë‹¤. ì •ì‹ competition ëª…ì¹­ì€ â€˜House Prices: Advanced Regression Techniquesâ€™ì´ë©°, í˜„ì¬ ëˆ„êµ¬ë‚˜ submissionì„ ì œì¶œí•  ìˆ˜ ìˆë‹¤. ìœ„ì—ì„œ ë§í–ˆë“¯ì´ boston house priceë°ì´í„°ì…‹ì€ ì™ ë§Œí•œ ë¨¸ì‹ ëŸ¬ë‹ ê³µë¶€í•˜ëŠ” ì‚¬ëŒë“¤ì€ í•œë²ˆì¯¤ ë´¤ì„ ê²ƒì´ë©°, ëŒ€ë¶€ë¶„ì˜ ë¨¸ì‹ ëŸ¬ë‹ ì…ë¬¸ êµì¬ì—ë„ ê¼­ í•œë²ˆì”©ì€ ì†Œê°œê°€ ë˜ëŠ” ë°ì´í„°ì…‹ì´ë‹¤. í•˜ì§€ë§Œ, ëŒ€ë¶€ë¶„ì˜ êµì¬ë‚˜ ê°•ì˜ì—ì„œëŠ” ì´ë¯¸ feature engineeringì„ ê±°ì¹œ ì•„ì£¼ ì˜ ì •í˜•í™” ëœ(ëª¨ë¸ì— ë°”ë¡œ ì ìš© ê°€ëŠ¥í•œ)ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ë©° ë°ì´í„° ì²˜ë¦¬ ê³¼ì •ì€ ìƒëµí•˜ëŠ” ê²½ìš°ê°€ ëŒ€ë¶€ë¶„ì¸ ê²ƒ ê°™ë‹¤. í•˜ì§€ë§Œ boston house price ë°ì´í„°ì…‹ì€ ë¬´ë ¤ 81ê°œì˜ ë‹¤ì–‘í•œ ì¹¼ëŸ¼ ë³€ìˆ˜ë¥¼ ê°€ì§€ê³  ìˆìœ¼ë©°, ê° ì¹¼ëŸ¼ íŠ¹ì„±ì— ë§ëŠ” ì „ì²˜ë¦¬ê°€ í•„ìš”í•˜ë‹¤. ë”°ë¼ì„œ, ì—¬ê¸°ì„œëŠ” boston house price ë°ì´í„°ì…‹ì— ì–´ë–»ê²Œ ì ì ˆí•œ feature engineeringì„ ì ìš©í•˜ê³ , ìµœê·¼ kaggleì—ì„œ ê°€ì¥ ì¸ê¸° ìˆëŠ” ëª¨ë¸ì¸ XGBoost ëª¨ë¸ì„ ì–´ë–»ê²Œ ì ìš©í•˜ì˜€ëŠ”ì§€ ì†Œê°œí•œë‹¤. Import Library1234import numpy as np import pandas as pdimport matplotlib.pyplot as pltimport seaborn as sns 1. Load Data ë°ì´í„°ì˜ ê²½ìš° train, test setì´ ë¶„ë¦¬ë˜ì–´ ì œê³µë˜ë©° test setì— ëŒ€í•œ ì˜ˆì¸¡ê²°ê³¼ê°€ ì¶”í›„ submissionìœ¼ë¡œ ì œì¶œëœë‹¤. train ë°ì´í„°ì˜ ê²½ìš° 1,460ê±´, test ë°ì´í„°ê°€ 1459ê±´ì´ë©° ì´ 81ê°œì˜ ì¹¼ëŸ¼ì„ ê°€ì§„ë‹¤. 12345# laod datatrain_df = pd.read_csv('house_train.csv')test_df = pd.read_csv('house_test.csv')train_df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Id MSSubClass MSZoning LotFrontage LotArea Street Alley LotShape LandContour Utilities ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold YrSold SaleType SaleCondition SalePrice 0 1 60 RL 65.0 8450 Pave NaN Reg Lvl AllPub ... 0 NaN NaN NaN 0 2 2008 WD Normal 208500 1 2 20 RL 80.0 9600 Pave NaN Reg Lvl AllPub ... 0 NaN NaN NaN 0 5 2007 WD Normal 181500 2 3 60 RL 68.0 11250 Pave NaN IR1 Lvl AllPub ... 0 NaN NaN NaN 0 9 2008 WD Normal 223500 3 4 70 RL 60.0 9550 Pave NaN IR1 Lvl AllPub ... 0 NaN NaN NaN 0 2 2006 WD Abnorml 140000 4 5 60 RL 84.0 14260 Pave NaN IR1 Lvl AllPub ... 0 NaN NaN NaN 0 12 2008 WD Normal 250000 5 rows Ã— 81 columns 12345# set indextrain_df.set_index('Id', inplace=True)test_df.set_index('Id', inplace=True)len_train_df = len(train_df)len_test_df = len(test_df) 2. Feature Selection - Variables of Corrleation &gt;= 0.3 ê³ ë ¤í•´ì•¼ í•  ë³€ìˆ˜ê°€ ë§ì„ ë• ê° ë…ë¦½ë³€ìˆ˜ì™€ ì¢…ì†ë³€ìˆ˜ ê°„ì˜ ìƒê´€ê´€ê³„(Corrleation)ì„ ê²€í† í•´ë³´ëŠ” ê²ƒì´ ì¢‹ë‹¤. ëª¨ë“  ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒë„ ì¢‹ì§€ë§Œ ê·¸ ì¤‘ ì¢€ë” ì˜ë¯¸ ìˆëŠ” ë³€ìˆ˜ë§Œì„ ê³¨ë¼ë‚´ì–´ ëª¨ë¸ì„ êµ¬ì¶•í•˜ëŠ” ê²ƒì´ ëª¨ë¸ì˜ ì˜ˆì¸¡ ì •í™•ë„ë¥¼ ë†’ì´ëŠ” ë°©ë²•ì´ë‹¤. corr()í•¨ìˆ˜ë¥¼ í†µí•´ dataframeë‚´ì˜ ëª¨ë“  ë³€ìˆ˜ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ ê·¸ë¦° í›„ ìƒê´€ê´€ê³„ê°€ 0.3ì´ìƒì¸ ë³€ìˆ˜ë§Œ heatmapìœ¼ë¡œ ì¶œë ¥í•˜ì˜€ë‹¤. 123corrmat = train_df.corr()top_corr_features = corrmat.index[abs(corrmat[\"SalePrice\"])&gt;=0.3]top_corr_features Index([&#39;LotFrontage&#39;, &#39;OverallQual&#39;, &#39;YearBuilt&#39;, &#39;YearRemodAdd&#39;, &#39;MasVnrArea&#39;, &#39;BsmtFinSF1&#39;, &#39;TotalBsmtSF&#39;, &#39;1stFlrSF&#39;, &#39;2ndFlrSF&#39;, &#39;GrLivArea&#39;, &#39;FullBath&#39;, &#39;TotRmsAbvGrd&#39;, &#39;Fireplaces&#39;, &#39;GarageYrBlt&#39;, &#39;GarageCars&#39;, &#39;GarageArea&#39;, &#39;WoodDeckSF&#39;, &#39;OpenPorchSF&#39;, &#39;SalePrice&#39;], dtype=&#39;object&#39;) 123# heatmapplt.figure(figsize=(13,10))g = sns.heatmap(train_df[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\") 123# feature selection# train_df = train_df[top_corr_features]# test_df = test_df[top_corr_features.drop(['SalePrice'])] 123# split y_labeltrain_y_label = train_df['SalePrice'] # target ê°’ì„ ë¯¸ë¦¬ ë¶„ë¦¬í•˜ì˜€ìŒ.train_df.drop(['SalePrice'], axis=1, inplace=True) 3. Concat train &amp; test set trainê³¼ test ì…‹ì— ë™ì¼í•œ feature engineeringì„ ì ìš©í•´ì£¼ê¸° ìœ„í•´ ìš°ì„  ë‘ê°œì˜ ë°ì´í„° ì…‹ì„ í•˜ë‚˜ë¡œ í•©ì³ì£¼ì—ˆë‹¤. í•©ì³ì£¼ë‹ˆ 2,919ê°œë¡œ ë°ì´í„°ê°€ ëŠ˜ì—ˆë‹¤. 123456# concat train &amp; testboston_df = pd.concat((train_df, test_df), axis=0)boston_df_index = boston_df.indexprint('Length of Boston Dataset : ',len(boston_df))boston_df.head() Length of Boston Dataset : 2919 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } MSSubClass MSZoning LotFrontage LotArea Street Alley LotShape LandContour Utilities LotConfig ... ScreenPorch PoolArea PoolQC Fence MiscFeature MiscVal MoSold YrSold SaleType SaleCondition Id 1 60 RL 65.0 8450 Pave NaN Reg Lvl AllPub Inside ... 0 0 NaN NaN NaN 0 2 2008 WD Normal 2 20 RL 80.0 9600 Pave NaN Reg Lvl AllPub FR2 ... 0 0 NaN NaN NaN 0 5 2007 WD Normal 3 60 RL 68.0 11250 Pave NaN IR1 Lvl AllPub Inside ... 0 0 NaN NaN NaN 0 9 2008 WD Normal 4 70 RL 60.0 9550 Pave NaN IR1 Lvl AllPub Corner ... 0 0 NaN NaN NaN 0 2 2006 WD Abnorml 5 60 RL 84.0 14260 Pave NaN IR1 Lvl AllPub FR2 ... 0 0 NaN NaN NaN 0 12 2008 WD Normal 5 rows Ã— 79 columns 4. Check NaN ratio and Remove null ratio &gt;= 0.5 ë°ì´í„°ë¥¼ ì²˜ë¦¬í•  ë•Œ í•­ìƒ Nullê°’ì„ ì–´ë–»ê²Œ ì²˜ë¦¬í• ì§€ ê³ ë¯¼í•´ì•¼ í•œë‹¤. ì¶”í›„ ëª¨ë¸ì— ì…ë ¥ë˜ëŠ” inputê°’ì—ëŠ” ì ˆëŒ€ ì–´ë– í•œ Null ê°’ì´ ìˆì–´ì„œëŠ” ì•ˆë˜ë©° ìˆë”ë¼ë„ ì—ëŸ¬ê°€ ë°œìƒí•˜ê¸° ë•Œë¬¸ì— ë¯¸ë¦¬ ê¼­ ì²˜ë¦¬í•´ì£¼ì–´ì•¼ í•œë‹¤. ìš°ì„  ê° ì¹¼ëŸ¼ë³„ë¡œ Nullê°’ ë¹„ìœ¨ì´ 50%ì´ìƒì¸ ì¹¼ëŸ¼ì„ ì°¾ì•„ í•´ë‹¹ ì¹¼ëŸ¼ì„ ì œê±°í•´ì£¼ì—ˆë‹¤. ë³´í†µ nullê°’ ì²˜ë¦¬ë¥¼ ìœ„í•´ í‰ê· , ìµœëŒ€ê°’, ìµœì†Œê°’ ë“±ìœ¼ë¡œ ëŒ€ì²´í•˜ê³¤ í•˜ëŠ”ë° ìœ„ì™€ ê°™ì´ ëŒ€ë¶€ë¶„ì˜ ì¹¼ëŸ¼ì´ Nullì¸ ë°ì´í„°ëŠ” ì°¨ë¼ë¦¬ ì—†ì• ì£¼ëŠ” ê²ƒì´ ì¢‹ë‹¤. 12345# check null check_null = boston_df.isna().sum() / len(boston_df)# columns of null ratio &gt;= 0.5check_null[check_null &gt;= 0.5] Alley 0.932169 PoolQC 0.996574 Fence 0.804385 MiscFeature 0.964029 dtype: float64 12345# remove columns of null ratio &gt;= 0.5remove_cols = check_null[check_null &gt;= 0.5].keys()boston_df = boston_df.drop(remove_cols, axis=1)boston_df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } MSSubClass MSZoning LotFrontage LotArea Street LotShape LandContour Utilities LotConfig LandSlope ... OpenPorchSF EnclosedPorch 3SsnPorch ScreenPorch PoolArea MiscVal MoSold YrSold SaleType SaleCondition Id 1 60 RL 65.0 8450 Pave Reg Lvl AllPub Inside Gtl ... 61 0 0 0 0 0 2 2008 WD Normal 2 20 RL 80.0 9600 Pave Reg Lvl AllPub FR2 Gtl ... 0 0 0 0 0 0 5 2007 WD Normal 3 60 RL 68.0 11250 Pave IR1 Lvl AllPub Inside Gtl ... 42 0 0 0 0 0 9 2008 WD Normal 4 70 RL 60.0 9550 Pave IR1 Lvl AllPub Corner Gtl ... 35 272 0 0 0 0 2 2006 WD Abnorml 5 60 RL 84.0 14260 Pave IR1 Lvl AllPub FR2 Gtl ... 84 0 0 0 0 0 12 2008 WD Normal 5 rows Ã— 75 columns 5. Check Object &amp; Numeric variables í•´ë‹¹ ë°ì´í„° ì…‹ì—ëŠ” ìˆ˜ì¹˜í˜• ë°ì´í„°ë§Œ ìˆëŠ” ê²ƒì´ ì•„ë‹ˆë‹¤. [ì„±ë³„: ë‚¨ì, ì—¬ì], [í•™ê¸‰: í–‡ë‹˜ë°˜, ê½ƒë‹˜ë°˜, ë‹¬ë‹˜ë°˜]ê³¼ ê°™ì€ ì¹´í…Œê³ ë¦¬í˜• ë°ì´í„°ë„ ì¡´ì¬í•œë‹¤. ì´ëŸ¬í•œ ì¹´í…Œê³ ë¦¬í˜• ë°ì´í„°ëŠ” ê° ì¹¼ëŸ¼ì„ 0ê³¼ 1ë¡œ ë³€í™˜í•´ì£¼ëŠ” one-hot encodingì„ ì ìš©í•´ì£¼ì–´ ìˆ˜ì¹˜ê°’ê³¼ ê°€ì¤‘ì¹˜ë¥¼ ë‹¬ë¦¬í•´ì£¼ì–´ì•¼ í•œë‹¤. ìˆ˜ì¹˜í˜• ë°ì´í„°ì™€ ì¹´í…Œê³ ë¦¬í˜• ë°ì´í„°ë¥¼ êµ¬ë¶„í•˜ê¸° ìœ„í•´ select_dtypes()ë¥¼ ì´ìš©í•˜ì˜€ë‹¤. parameterê°’ìœ¼ë¡œ includeì™€ excludeë¥¼ ì ìš©í•  ìˆ˜ ìˆëŠ”ë° ì´ë¥¼ í†µí•´ ë°ì´í„°ë¥¼ ë¶„ë¦¬í•œë‹¤. 123# split object &amp; numericboston_obj_df = boston_df.select_dtypes(include='object') # ì¹´í…Œê³ ë¦¬í˜•boston_num_df = boston_df.select_dtypes(exclude='object') # ìˆ˜ì¹˜í˜• 123print('Object type columns:\\n',boston_obj_df.columns)print('---------------------------------------------------------------------------------')print('Numeric type columns:\\n',boston_num_df.columns) Object type columns: Index([&#39;MSZoning&#39;, &#39;Street&#39;, &#39;LotShape&#39;, &#39;LandContour&#39;, &#39;Utilities&#39;, &#39;LotConfig&#39;, &#39;LandSlope&#39;, &#39;Neighborhood&#39;, &#39;Condition1&#39;, &#39;Condition2&#39;, &#39;BldgType&#39;, &#39;HouseStyle&#39;, &#39;RoofStyle&#39;, &#39;RoofMatl&#39;, &#39;Exterior1st&#39;, &#39;Exterior2nd&#39;, &#39;MasVnrType&#39;, &#39;ExterQual&#39;, &#39;ExterCond&#39;, &#39;Foundation&#39;, &#39;BsmtQual&#39;, &#39;BsmtCond&#39;, &#39;BsmtExposure&#39;, &#39;BsmtFinType1&#39;, &#39;BsmtFinType2&#39;, &#39;Heating&#39;, &#39;HeatingQC&#39;, &#39;CentralAir&#39;, &#39;Electrical&#39;, &#39;KitchenQual&#39;, &#39;Functional&#39;, &#39;FireplaceQu&#39;, &#39;GarageType&#39;, &#39;GarageFinish&#39;, &#39;GarageQual&#39;, &#39;GarageCond&#39;, &#39;PavedDrive&#39;, &#39;SaleType&#39;, &#39;SaleCondition&#39;], dtype=&#39;object&#39;) --------------------------------------------------------------------------------- Numeric type columns: Index([&#39;MSSubClass&#39;, &#39;LotFrontage&#39;, &#39;LotArea&#39;, &#39;OverallQual&#39;, &#39;OverallCond&#39;, &#39;YearBuilt&#39;, &#39;YearRemodAdd&#39;, &#39;MasVnrArea&#39;, &#39;BsmtFinSF1&#39;, &#39;BsmtFinSF2&#39;, &#39;BsmtUnfSF&#39;, &#39;TotalBsmtSF&#39;, &#39;1stFlrSF&#39;, &#39;2ndFlrSF&#39;, &#39;LowQualFinSF&#39;, &#39;GrLivArea&#39;, &#39;BsmtFullBath&#39;, &#39;BsmtHalfBath&#39;, &#39;FullBath&#39;, &#39;HalfBath&#39;, &#39;BedroomAbvGr&#39;, &#39;KitchenAbvGr&#39;, &#39;TotRmsAbvGrd&#39;, &#39;Fireplaces&#39;, &#39;GarageYrBlt&#39;, &#39;GarageCars&#39;, &#39;GarageArea&#39;, &#39;WoodDeckSF&#39;, &#39;OpenPorchSF&#39;, &#39;EnclosedPorch&#39;, &#39;3SsnPorch&#39;, &#39;ScreenPorch&#39;, &#39;PoolArea&#39;, &#39;MiscVal&#39;, &#39;MoSold&#39;, &#39;YrSold&#39;], dtype=&#39;object&#39;) 6. Change object type to dummy variables ìœ„ì—ì„œ ë¶„ë¦¬í•œ ì¹´í…Œê³ ë¦¬í˜• ë°ì´í„°ì— one-hot encodingì„ ì ìš©í•˜ê¸° ìœ„í•´ pandasì˜ pd.get_dummies()ë¥¼ ì ìš©í•˜ì˜€ë‹¤. one-hot encoding ì ìš©ì‹œ [ë‚¨ì, ì—¬ì]ì˜ ê²½ìš° [[1,0], [0,1]]ê³¼ ê°™ì€ í˜•íƒœë¡œ ë³€í™˜ëœë‹¤. 123boston_dummy_df = pd.get_dummies(boston_obj_df, drop_first=True)boston_dummy_df.index = boston_df_indexboston_dummy_df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } MSZoning_FV MSZoning_RH MSZoning_RL MSZoning_RM Street_Pave LotShape_IR2 LotShape_IR3 LotShape_Reg LandContour_HLS LandContour_Low ... SaleType_ConLI SaleType_ConLw SaleType_New SaleType_Oth SaleType_WD SaleCondition_AdjLand SaleCondition_Alloca SaleCondition_Family SaleCondition_Normal SaleCondition_Partial Id 1 0 0 1 0 1 0 0 1 0 0 ... 0 0 0 0 1 0 0 0 1 0 2 0 0 1 0 1 0 0 1 0 0 ... 0 0 0 0 1 0 0 0 1 0 3 0 0 1 0 1 0 0 0 0 0 ... 0 0 0 0 1 0 0 0 1 0 4 0 0 1 0 1 0 0 0 0 0 ... 0 0 0 0 1 0 0 0 0 0 5 0 0 1 0 1 0 0 0 0 0 ... 0 0 0 0 1 0 0 0 1 0 5 rows Ã— 200 columns 7. Impute NaN of numeric data to â€˜meanâ€™ 4ë²ˆì¨° ê³¼ì •ì—ì„œ nullê°’ì´ 50%ì´ìƒì¸ ë³€ìˆ˜ë“¤ì„ ì œê±°í•´ì£¼ì—ˆì—ˆëŠ”ë°, ê·¸ ì´í•˜ë¡œ nullê°’ì´ ìˆëŠ” ë°ì´í„°ë¥¼ ë§ˆì € ì²˜ë¦¬í•´ì£¼ì–´ì•¼ í•œë‹¤. ì—¬ê¸°ì„œëŠ” ê° ì¹¼ëŸ¼ì˜ nullê°’ì„ í•´ë‹¹í•˜ëŠ” ê° ë³€ìˆ˜ë“¤ì˜ í‰ê· (mean)ìœ¼ë¡œ ëŒ€ì²´(imputation)í•´ì£¼ì—ˆë‹¤. í‰ê· ê°’ ëŒ€ì²´ë¥¼ ìœ„í•˜ì—¬ scikit-learnì˜ Imputer í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì˜€ìœ¼ë©°, strategy ê°’ì— ëŒ€ì²´í•´ì£¼ê³ ì í•˜ëŠ” ì´ë¦„ì„ ë„£ì–´ì£¼ë©´ í•´ë‹¹ ê°’ìœ¼ë¡œ ì²˜ë¦¬í•œë‹¤. 12345from sklearn.preprocessing import Imputerimputer = Imputer(strategy='mean')imputer.fit(boston_num_df)boston_num_df_ = imputer.transform(boston_num_df) 12boston_num_df = pd.DataFrame(boston_num_df_, columns=boston_num_df.columns, index=boston_df_index)boston_num_df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } MSSubClass LotFrontage LotArea OverallQual OverallCond YearBuilt YearRemodAdd MasVnrArea BsmtFinSF1 BsmtFinSF2 ... GarageArea WoodDeckSF OpenPorchSF EnclosedPorch 3SsnPorch ScreenPorch PoolArea MiscVal MoSold YrSold Id 1 60.0 65.0 8450.0 7.0 5.0 2003.0 2003.0 196.0 706.0 0.0 ... 548.0 0.0 61.0 0.0 0.0 0.0 0.0 0.0 2.0 2008.0 2 20.0 80.0 9600.0 6.0 8.0 1976.0 1976.0 0.0 978.0 0.0 ... 460.0 298.0 0.0 0.0 0.0 0.0 0.0 0.0 5.0 2007.0 3 60.0 68.0 11250.0 7.0 5.0 2001.0 2002.0 162.0 486.0 0.0 ... 608.0 0.0 42.0 0.0 0.0 0.0 0.0 0.0 9.0 2008.0 4 70.0 60.0 9550.0 7.0 5.0 1915.0 1970.0 0.0 216.0 0.0 ... 642.0 0.0 35.0 272.0 0.0 0.0 0.0 0.0 2.0 2006.0 5 60.0 84.0 14260.0 8.0 5.0 2000.0 2000.0 350.0 655.0 0.0 ... 836.0 192.0 84.0 0.0 0.0 0.0 0.0 0.0 12.0 2008.0 5 rows Ã— 36 columns 8. Merge numeric_df &amp; dummies_df ìœ„ì—ì„œ ê°ê° ì²˜ë¦¬í•œ ì¹´í…Œê³ ë¦¬í˜• ë°ì´í„°ì™€ ìˆ˜ì¹˜í˜• ë°ì´í„°ë¥¼ ì´ì œ ìµœì¢…ì ìœ¼ë¡œ ë‹¤ì‹œ í•˜ë‚˜ë¡œ mergeí•´ì¤€ë‹¤. mergeì‹œ index ìˆœì„œê°€ ê¼¬ì´ì§€ ì•Šê²Œ left_index=True, right_index=Trueë¥¼ ì§€ì •í•˜ì—¬ mergeë¥¼ ìˆ˜í–‰í•œë‹¤. 12boston_df = pd.merge(boston_dummy_df, boston_num_df, left_index=True, right_index=True)boston_df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } MSZoning_FV MSZoning_RH MSZoning_RL MSZoning_RM Street_Pave LotShape_IR2 LotShape_IR3 LotShape_Reg LandContour_HLS LandContour_Low ... GarageArea WoodDeckSF OpenPorchSF EnclosedPorch 3SsnPorch ScreenPorch PoolArea MiscVal MoSold YrSold Id 1 0 0 1 0 1 0 0 1 0 0 ... 548.0 0.0 61.0 0.0 0.0 0.0 0.0 0.0 2.0 2008.0 2 0 0 1 0 1 0 0 1 0 0 ... 460.0 298.0 0.0 0.0 0.0 0.0 0.0 0.0 5.0 2007.0 3 0 0 1 0 1 0 0 0 0 0 ... 608.0 0.0 42.0 0.0 0.0 0.0 0.0 0.0 9.0 2008.0 4 0 0 1 0 1 0 0 0 0 0 ... 642.0 0.0 35.0 272.0 0.0 0.0 0.0 0.0 2.0 2006.0 5 0 0 1 0 1 0 0 0 0 0 ... 836.0 192.0 84.0 0.0 0.0 0.0 0.0 0.0 12.0 2008.0 5 rows Ã— 236 columns 9. Split train &amp; validation &amp; test set ëª¨ë¸ í•™ìŠµ ë° ê²€ì¦ì„ ìœ„í•´ ë°ì´í„°ë¥¼ splití•œë‹¤. ì—¬ê¸°ì„œ test setì˜ ê²½ìš° ì •ë‹µê°’ì´ ì—†ëŠ” ì˜ˆì¸¡í•´ì•¼ í•˜ëŠ” ê°’ì´ë¯€ë¡œ ê²€ì¦ì„ ìœ„í•´ validation setì„ train setì˜ 20%ë§Œí¼ì„ ì§€ì •í•´ì£¼ì—ˆë‹¤. ìµœì¢…ì ìœ¼ë¡œ 1,168ê°œì˜ ë°ì´í„°ë¡œ í•™ìŠµ ë° 292ê°œì˜ ë°ì´í„°ë¡œ ê²€ì¦ í›„ 1,459ê°œì˜ testì…‹ì„ ì˜ˆì¸¡í•œë‹¤. 1234567train_df = boston_df[:len_train_df]test_df = boston_df[len_train_df:]train_df['SalePrice'] = train_y_labelprint('train set length: ',len(train_df))print('test set length: ',len(test_df)) train set length: 1460 test set length: 1459 123456789from sklearn.model_selection import train_test_splitX_train = train_df.drop(['SalePrice'], axis=1)y_train = train_df['SalePrice']X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, shuffle=True)X_test = test_dftest_id_idx = test_df.index 123print('X_train : ',len(X_train))print('X_val : ',len(X_val))print('X_test :',len(X_test)) X_train : 1168 X_val : 292 X_test : 1459 10. Training by XGBRegression Model ëª¨ë¸ í•™ìŠµì„ ìœ„í•´ ìµœê·¼ kaggleì—ì„œ ê°€ì¥ ì¸ê¸° ìˆëŠ” ëª¨ë¸ì¸ XGBoost ëª¨ë¸ì„ ì´ìš©í•˜ì˜€ë‹¤. í•´ë‹¹ ì˜ˆì¸¡ì€ regression ì˜ˆì¸¡ì´ë¯€ë¡œ XGBRegressor() ëª¨ë¸ì„ ì´ìš©í•˜ì˜€ë‹¤. ìµœì ì˜ ëª¨ë¸ íŒŒë¼ë¯¸í„° ì„¤ì •ì„ ìœ„í•˜ì—¬ GridSearchë¥¼ ì´ìš©í•˜ì˜€ìœ¼ë©°, 5ë²ˆì˜ cross-validationìœ¼ë¡œ ê²€ì¦ì„ ì§„í–‰í•˜ì˜€ë‹¤. í•™ìŠµ í›„ bestparamsë¥¼ ì¶œë ¥í•˜ë©´ ìµœì ì˜ íŒŒë¼ë¯¸í„° ê°’ì´ ì¶œë ¥ëœë‹¤. 1234567891011121314151617from sklearn.model_selection import GridSearchCVimport xgboost as xgbparam = &#123; 'max_depth':[2,3,4], 'n_estimators':range(550,700,50), 'colsample_bytree':[0.5,0.7,1], 'colsample_bylevel':[0.5,0.7,1],&#125;model = xgb.XGBRegressor()grid_search = GridSearchCV(estimator=model, param_grid=param, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)grid_search.fit(X_train, y_train)print(grid_search.best_params_)print(grid_search.best_estimator_) {&#39;colsample_bylevel&#39;: 0.5, &#39;colsample_bytree&#39;: 0.7, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 600} XGBRegressor(base_score=0.5, booster=&#39;gbtree&#39;, colsample_bylevel=0.5, colsample_bytree=0.7, gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3, min_child_weight=1, missing=None, n_estimators=600, n_jobs=1, nthread=None, objective=&#39;reg:linear&#39;, random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None, silent=True, subsample=1) 11. Prediction &amp; Score ê²€ì¦ì„ ìœ„í•´ Mean Absolute Error(MAE) ì§€í‘œë¥¼ í™œìš©í•˜ì˜€ë‹¤. MSEë¥¼ í™œìš©í•  ê²½ìš° errorê°’ì´ í´ ê²½ìš° ê·¸ì— ì œê³±ëœ ê°’ì´ ì¶œë ¥ë˜ê¸° ë•Œë¬¸ì— ê°’ì´ ë„ˆë¬´ ì»¤ì ¸ ë³´ê¸° ë¶ˆí¸í•˜ë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤. ê²€ì¦ ê²°ê³¼ validation mae ê°’ì´ 14,000ì •ë„ì¸ë°, ì›Œë‚™ ì§‘ ê°€ê²©ì— ëŒ€í•œ ê°’ì˜ ë²”ìœ„ê°€ ë„“ê¸° ë•Œë¬¸ì— ì´ ì •ë„ errorê°’ì€ ì‹¬ê°í•œ ì •ë„ëŠ” ì•„ë‹ˆë©° ë‚©ë“í• ë§Œí•œ ìˆ˜ì¤€ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤. 123456from sklearn.metrics import mean_squared_error, mean_absolute_errorpred_train = grid_search.predict(X_train)pred_val = grid_search.predict(X_val)print('train mae score: ', mean_absolute_error(y_train, pred_train))print('val mae score:', mean_absolute_error(y_val, pred_val)) train mae score: 4790.379391186858 val mae score: 14178.155835295376 ì´í›„ validation setì„ ëŒ€ìƒìœ¼ë¡œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•œ í›„ ì‹¤ì œ ê°’ê³¼ì˜ ê²°ê³¼ë¥¼ plottingí•˜ì˜€ë‹¤. ì–´ëŠ ì •ë„ ê²½í–¥ì„ ì˜ ì˜ˆì¸¡í•˜ê³  ìˆëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. 123456plt.figure(figsize=(17,7))plt.plot(range(0, len(y_val)), y_val,'o-', label='Validation Actual')plt.plot(range(0, len(pred_val)), pred_val, '-', label='Validation Predict')plt.title('Prediction of House Prices')plt.ylabel('Prices')plt.legend() 12. Predict test set &amp; Submit submission.csv1test_y_pred = grid_search.predict(X_test) 123id_pred_df = pd.DataFrame()id_pred_df['Id'] = test_id_idxid_pred_df['SalePrice'] = test_y_pred 1id_pred_df.to_csv('submission.csv', index=False) Outroì‹¤ì œ submissionì—ì„œ ì„±ëŠ¥ í‰ê°€ ì‹œì—ëŠ” Root Mean Squarred Errorì´ìš©í•˜ëŠ”ë°, ì œì¶œ ê²°ê³¼ 0.12875ë¡œ ì´ 4,465íŒ€ ì¤‘ 1,867ë“±ì„ ê¸°ë¡í•˜ì˜€ë‹¤(ìƒìœ„ 42%). ì´ë²ˆì—ëŠ” ë‹¨ìˆœíˆ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë§Œ ì ìš©í•´ë³´ì•˜ëŠ”ë°, LSTM ëª¨ë¸ì„ ì ìš©í•˜ë©´ í™•ì‹¤íˆ ë” ë‚®ì€ error ê°’ì´ ë‚˜ì˜¬ ê²ƒìœ¼ë¡œ ê¸°ëŒ€ëœë‹¤.","categories":[],"tags":[{"name":"machinelearning","slug":"machinelearning","permalink":"https://jaehyeongan.github.io/tags/machinelearning/"},{"name":"kaggle","slug":"kaggle","permalink":"https://jaehyeongan.github.io/tags/kaggle/"},{"name":"regression","slug":"regression","permalink":"https://jaehyeongan.github.io/tags/regression/"},{"name":"bostonhouse","slug":"bostonhouse","permalink":"https://jaehyeongan.github.io/tags/bostonhouse/"},{"name":"price","slug":"price","permalink":"https://jaehyeongan.github.io/tags/price/"},{"name":"xgboost","slug":"xgboost","permalink":"https://jaehyeongan.github.io/tags/xgboost/"},{"name":"featureengineering","slug":"featureengineering","permalink":"https://jaehyeongan.github.io/tags/featureengineering/"}]},{"title":"[Kaggle] ì§ì†Œ ì•…ì„± ëŒ€í™” ë¶„ë¥˜(Jigsaw Unintended Bias in Toxicity Classification)","slug":"Kaggle-challenge-ì§ì†Œ-ì•…ì„±-ëŒ“ê¸€-ë¶„ë¥˜-Jigsaw-Unintended-Bias-in-Toxicity","date":"2019-07-03T15:11:36.000Z","updated":"2021-02-07T14:57:03.741Z","comments":true,"path":"2019/07/04/Kaggle-challenge-ì§ì†Œ-ì•…ì„±-ëŒ“ê¸€-ë¶„ë¥˜-Jigsaw-Unintended-Bias-in-Toxicity/","link":"","permalink":"https://jaehyeongan.github.io/2019/07/04/Kaggle-challenge-ì§ì†Œ-ì•…ì„±-ëŒ“ê¸€-ë¶„ë¥˜-Jigsaw-Unintended-Bias-in-Toxicity/","excerpt":"","text":"Introì–¼ë§ˆ ì „ ìºê¸€ì—ì„œ êµ¬ê¸€ Jigsaw/Conversation AIíŒ€ì— ì˜í•´ â€˜Jigsaw Unintended Bias in Toxicity Classificationâ€™ë¼ëŠ” ì£¼ì œë¡œ competitionì´ ê°œìµœë˜ì–´ í˜¸ê¸°ì‹¬ì— ë„ì „í•´ë³´ì•˜ë‹¤. Jigsawë¼ëŠ” ê³³ì„ ì²˜ìŒ ë“¤ì–´ë´¤ëŠ”ë° ì•Œì•„ë³´ë‹ˆ êµ¬ê¸€ì˜ ìíšŒì‚¬ë¡œ ì˜¨ë¼ì¸ ìƒì˜ ìš•ì„¤ì´ë‚˜ ì„ ë™ì , í­ë ¥ì  ëŒ€í™”ë¥¼ ì¡ì•„ë‚´ëŠ” ê¸°ìˆ ì„ ì—°êµ¬í•˜ëŠ” ê³³ì´ì—ˆê³ ,Descriptionìƒì— ì˜í•œ ì´ Competitionì˜ ì£¼ìš”ë¬¸ì œëŠ” ë‹¤ìŒê³¼ ê°™ì•˜ë‹¤. í˜„ì¬ Jigsawì˜ Conversation AIíŒ€ì€ Perspectiveë¼ëŠ” ì œí’ˆì„ í†µí•´ ì˜¨ë¼ì¸ ìƒì˜ ì•…ì„± ëŒ€í™”(ìœ„í˜‘, ì™¸ì„¤, ëª¨ìš• ë“±)ë¥¼ ì¡ì•„ë‚´ê³  ìˆëŠ”ë°, ëª¨ë¸ì„ ì¢€ ë” ì •êµí•˜ê²Œ í•˜ì—¬ ë‚®ì€ ì—ëŸ¬ìœ¨ì˜ ë‹¤ì–‘í•œ ì•…ì„± ëŒ€í™”ë¥¼ ì¡ì•„ë‚´ëŠ” ëª¨ë¸ì„ ë§Œë“œëŠ” ê²ƒ. ë°ì´í„°ì˜ ê²½ìš° trainë°ì´í„°ì™€ testë°ì´í„°ë¥¼ ë”°ë¡œ ì œê³µí•˜ë©°, train ë°ì´í„°ì˜ ê²½ìš° 180ë§Œê±´ ì •ë„ ë˜ëŠ”ë° í…ìŠ¤íŠ¸ ë°ì´í„° ìœ„ì£¼ë¡œ ë˜ì–´ìˆë‹¤ë³´ë‹ˆ ì‚¬ì´ì¦ˆê°€ ìƒë‹¹íˆ ì»¸ë‹¤.í•´ë‹¹ competitionì˜ ê²°ê³¼ ì œì¶œì€ Kernelsì— ì˜í•´ì„œë§Œ ê°€ëŠ¥í•œë°, ë°ì´í„° ì‚¬ì´ì¦ˆê°€ í¬ë‹¤ë³´ë‹ˆ ëª¨ë¸ì— ì˜í•œ í•™ìŠµë„ êµ‰ì¥íˆ ì˜¤ë˜ê±¸ë¦¬ê³  kaggleë‚´ì—ì„œë„ kernel í•™ìŠµì‹œê°„ì— ì œí•œì„ ë‘ê¸° ë•Œë¬¸ì— ëª¨ë¸ì„ ì •êµí•˜ê²Œ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒì´ ì‰½ì§€ ì•Šì•˜ë‹¤. ì½”ë“œ ì‘ì„±ì€ Jupyter notebookì„ ì´ìš©í•˜ì˜€ìœ¼ë©°, ì•„ë˜ ì‘ì„±ëœ ì½”ë“œëŠ” ipynbíŒŒì¼ì„ markdownìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì—…ë¡œë“œí•˜ì˜€ë‹¤. Import Library12345678910111213141516171819import os import numpy as npimport pandas as pdimport matplotlib.pyplot as pltfrom collections import Counterimport nltknltk.download('stopwords')nltk.download('punkt')from nltk.corpus import stopwords from nltk.tokenize import word_tokenize stop_words = set(stopwords.words('english')) import tensorflow as tffrom sklearn.metrics import roc_auc_scorefrom keras import models, layers, Modelfrom keras.preprocessing import text, sequencefrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateauimport osprint(os.listdir(\"./input\")) [&#39;test.csv&#39;, &#39;train.csv&#39;] 1. Load Data ë°ì´í„°ëŠ” train ë°ì´í„°ê°€ 180ë§Œê±´, test ë°ì´í„°ê°€ 9ë§Œ7ì²œê±´ ì •ë„ë¡œ ì´ë£¨ì–´ì ¸ ìˆë‹¤. train ë°ì´í„°ëŠ” id, target, comment_textë¥¼ í¬í•¨í•˜ì—¬ ì´ 45ê°œì˜ ì¹¼ëŸ¼ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆì§€ë§Œ, test ë°ì´í„°ì˜ ê²½ìš° id, target, comment_text ì´ 3ê°œì˜ ì¹¼ëŸ¼ìœ¼ë¡œë§Œ ì´ë£¨ì–´ì ¸ ìˆë‹¤. 12345## load datatrain_data = pd.read_csv('./input/train.csv')test_data = pd.read_csv('./input/test.csv')print(train_data.shape)print(test_data.shape) (1804874, 45) (97320, 2) 1train_data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id target comment_text severe_toxicity obscene identity_attack insult threat asian atheist ... article_id rating funny wow sad likes disagree sexual_explicit identity_annotator_count toxicity_annotator_count 0 59848 0.000000 This is so cool. It's like, 'would you want yo... 0.000000 0.0 0.000000 0.00000 0.0 NaN NaN ... 2006 rejected 0 0 0 0 0 0.0 0 4 1 59849 0.000000 Thank you!! This would make my life a lot less... 0.000000 0.0 0.000000 0.00000 0.0 NaN NaN ... 2006 rejected 0 0 0 0 0 0.0 0 4 2 59852 0.000000 This is such an urgent design problem; kudos t... 0.000000 0.0 0.000000 0.00000 0.0 NaN NaN ... 2006 rejected 0 0 0 0 0 0.0 0 4 3 59855 0.000000 Is this something I'll be able to install on m... 0.000000 0.0 0.000000 0.00000 0.0 NaN NaN ... 2006 rejected 0 0 0 0 0 0.0 0 4 4 59856 0.893617 haha you guys are a bunch of losers. 0.021277 0.0 0.021277 0.87234 0.0 0.0 0.0 ... 2006 rejected 0 0 0 1 0 0.0 4 47 5 rows Ã— 45 columns 2. Set index &amp; target label ë‹¤ë¥¸ ì»¤ë„ì„ ë³´ë‹ˆ train ë°ì´í„°ì˜ ë‹¤ì–‘í•œ ì¹¼ëŸ¼ì„ í™œìš©í•˜ëŠ” ê²ƒ ê°™ë˜ë° ì—¬ê¸°ì„  í…ìŠ¤íŠ¸ ë°ì´í„°ì™€ íƒ€ê²Ÿ ê°’ë§Œì„ ì´ìš©í•˜ì—¬ í•™ìŠµ ë° ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ì˜€ë‹¤. id ê°’ì€ indexë¡œ ì§€ì •í•´ë‘ì—ˆìœ¼ë©°, targetê°’ì˜ ê²½ìš° Data Descriptionì˜ ì„¤ëª…ì— ë”°ë¼ 0.5ì´ìƒì€ positive 0.5ë¯¸ë§Œì€ negative ë¼ë²¨ë¡œ ë¶„ë¥˜í•˜ì˜€ë‹¤. 12345678910train_df = train_data[['id','comment_text','target']]test_df = test_data.copy()# set indextrain_df.set_index('id', inplace=True)test_df.set_index('id', inplace=True)# y_labeltrain_y_label = np.where(train_df['target'] &gt;= 0.5, 1, 0) # Label 1 &gt;= 0.5 / Label 0 &lt; 0.5train_df.drop(['target'], axis=1, inplace=True) 12# ratio by ClassCounter(train_y_label) Counter({0: 1660540, 1: 144334}) 3. View text data comment_text ì¹¼ëŸ¼ì„ ì¶œë ¥í•´ë³´ë©´ ì•„ë˜ì™€ ê°™ì´ ë‹¤ì–‘í•œ ì£¼ì œì˜ ëŒ€í™” ë‚´ìš©ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. 1train_data['comment_text'].head(20) 0 This is so cool. It&#39;s like, &#39;would you want yo... 1 Thank you!! This would make my life a lot less... 2 This is such an urgent design problem; kudos t... 3 Is this something I&#39;ll be able to install on m... 4 haha you guys are a bunch of losers. 5 ur a sh*tty comment. 6 hahahahahahahahhha suck it. 7 FFFFUUUUUUUUUUUUUUU 8 The ranchers seem motivated by mostly by greed... 9 It was a great show. Not a combo I&#39;d of expect... 10 Wow, that sounds great. 11 This is a great story. Man. I wonder if the pe... 12 This seems like a step in the right direction. 13 It&#39;s ridiculous that these guys are being call... 14 This story gets more ridiculous by the hour! A... 15 I agree; I don&#39;t want to grant them the legiti... 16 Interesting. I&#39;ll be curious to see how this w... 17 Awesome! I love Civil Comments! 18 I&#39;m glad you&#39;re working on this, and I look fo... 19 Angry trolls, misogynists and Racists&quot;, oh my.... Name: comment_text, dtype: object 4. Remove Punctuation &amp; Stopword ê°€ì¥ ê¸°ë³¸ì ì¸ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ë¥¼ ìœ„í•˜ì—¬ ê°„ë‹¨íˆ í…ìŠ¤íŠ¸ ë‚´ì˜ punctuationê³¼ stopwordsë¥¼ ì œê±°í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì •ì˜í•˜ì˜€ë‹¤. ì›Œë‚™ ë°ì´í„°ê°€ ì»¤ì„œ í•¨ìˆ˜ í˜¸ì¶œ ì‹œ ì²˜ë¦¬ ì†ë„ê°€ ì˜¤ë˜ ê±¸ë¦°ë‹¤. ê·¸ë˜ì„œ ì†ë„ë¥¼ ìœ„í•´ list comprehensionê³¼ lambdaë¡œ ì²˜ë¦¬í•˜ì˜€ëŠ”ë° ê·¸ë˜ë„ ì²˜ë¦¬ê¹Œì§€ ì‹œê°„ì´ ê½¤ ê±¸ë ¸ë‹¤. 123456789101112131415161718## Clean Punctuation &amp; Stopwordsclass clean_text: def __init__(self, text): self.text = text # Remove Punctuation def rm_punct(text): punct = set([p for p in \"/-'?!.,#$%\\'()*+-/:;&lt;=&gt;@[\\\\]^_`&#123;|&#125;~`\" + '\"\"â€œâ€â€™' + 'âˆÎ¸Ã·Î±â€¢Ã âˆ’Î²âˆ…Â³Ï€â€˜â‚¹Â´Â°Â£â‚¬\\Ã—â„¢âˆšÂ²â€”â€“&amp;']) text = [t for t in text if t not in punct] return \"\".join(text) # Remove Stopwords def rm_stopwords(text): word_tokens = word_tokenize(text) result = [w for w in word_tokens if w not in stop_words] return \" \".join(result) 1234567# remove punctuation train_df['comment_text'] = train_df['comment_text'].apply(lambda x: clean_text.rm_punct(x))test_df['comment_text'] = test_df['comment_text'].apply(lambda x: clean_text.rm_punct(x))# remove stopwordsX_train = train_df['comment_text'].apply(lambda x: clean_text.rm_stopwords(x))X_test = test_df['comment_text'].apply(lambda x: clean_text.rm_stopwords(x)) 5. Tokenize ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ keras.Tokenizerë¥¼ ì´ìš©í•˜ì—¬ sequences ë°ì´í„°ë¡œ ë³€í™˜í•œë‹¤. Tokenizerì˜ ì²˜ë¦¬ ìˆœì„œëŠ” ì•„ë˜ì™€ ê°™ë‹¤.â€” Tokenizer ê°ì²´ë¥¼ í†µí•´ ë°ì´í„°ë¥¼ í† í°í™”ì‹œí‚¨ í›„ ê° í† í°ì— ê³ ìœ  indexë¥¼ ë¶€ì—¬í•˜ì—¬ word index ìƒì„±â€” texts_to_sequences()ë¥¼ í†µí•´ word indexë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±â€” pad_sequences()ë¥¼ í†µí•´ padding ì¶”ê°€ 12345678910## tokenizemax_words = 100000tokenizer = text.Tokenizer(num_words=max_words) # Tokenizer ê°ì²´ìƒì„±tokenizer.fit_on_texts(X_train) # í† í° ë³„ word index ìƒì„±# texts_to_sequencessequences_text_train = tokenizer.texts_to_sequences(X_train)sequences_text_test = tokenizer.texts_to_sequences(X_test)print(sequences_text_train[:5]) [[21, 2188, 39, 6, 3, 32, 1115, 116, 48, 91, 277, 26, 138], [323, 21, 3, 25, 107, 142, 144, 105, 7, 159, 125, 9, 28], [21, 9494, 2834, 94, 4342, 340, 1102, 4913], [241, 90, 384, 316, 5764, 1027, 164, 6388], [5230, 586, 998, 2593]] texts_to_sequences()í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ë©´ í† í°í™” ëœ ë¬¸ìë“¤ì´ ìœ„ì™€ ê°™ì´ ê³ ìœ  index ë²ˆí˜¸ë¡œ ë°”ë€ ì±„ sequnce í˜•íƒœë¡œ ì¶œë ¥ëœë‹¤. 123456# add paddingmax_len = max(len(l) for l in sequences_text_train)pad_train = sequence.pad_sequences(sequences_text_train, maxlen=max_len)pad_test = sequence.pad_sequences(sequences_text_test, maxlen=max_len)print(pad_train[:5]) array([[ 0, 0, 0, ..., 277, 26, 138], [ 0, 0, 0, ..., 125, 9, 28], [ 0, 0, 0, ..., 340, 1102, 4913], [ 0, 0, 0, ..., 1027, 164, 6388], [ 0, 0, 0, ..., 586, 998, 2593]]) max_len ê°’ì€ ë°©ê¸ˆ ìœ„ì—ì„œ sequenceë¡œ ë³€í™˜í•œ ë°ì´í„° ì¤‘ ê°€ì¥ ë§ì€ word ìˆ˜ë¥¼ ê°€ì§€ëŠ” ë°ì´í„°ì˜ ê¸¸ì´ë¥¼ ë°›ì€ ê²ƒì´ê³ ,ëª¨ë“  ë°ì´í„°ë¥¼ ê·¸ ê¸¸ì´ ë§Œí¼ ë§ì¶°ì£¼ê¸° ìœ„í•˜ì—¬ pad_seqences()í•¨ìˆ˜ë¥¼ í†µí•´ 0ê°’ì„ ì±„ì›Œì£¼ê²Œ ëœë‹¤. 6. Embedding + LSTM model ì˜ˆì¸¡ì„ ìœ„í•´ì„œ embedding ë ˆì´ì–´ì™€ lstm ë ˆì´ì–´ë¥¼ ì—°ê²°í•˜ì—¬ ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ êµ¬ì¶•í•˜ì˜€ë‹¤. Embedding ë ˆì´ì–´ëŠ” í…ìŠ¤íŠ¸ ë°ì´í„°ì˜ ë‹¨ì–´ ì‚¬ì´ì˜ ì˜ë¯¸ê´€ê³„ë¥¼ í•™ìŠµí•˜ëŠ”ë° íš¨ê³¼ì ì´ë¯€ë¡œ í…ìŠ¤íŠ¸ ë°ì´í„° í•™ìŠµì‹œ ë§ì´ ì‚¬ìš©ë˜ë©°, LSTM ëª¨ë¸ì€ ì–‘ë°©í–¥ LSTM(Bidirectional LSTM)ìœ¼ë¡œ êµ¬ì¶•í•˜ì—¬ ì‹œê°„ì  ì˜ë¯¸ì™€ ìƒê´€ì—†ì´ ë‹¨ì–´ë“¤ ì‚¬ì´ì˜ ì–‘ë°©í–¥ì ìœ¼ë¡œ ì˜ë¯¸ ìˆœì„œë¥¼ í•™ìŠµí•˜ë„ë¡ í•˜ì˜€ë‹¤. 123456789101112131415161718def Embedding_CuDNNLSTM_model(max_words, max_len): sequence_input = layers.Input(shape=(None, )) x = layers.Embedding(max_words, 128, input_length=max_len)(sequence_input) x = layers.SpatialDropout1D(0.3)(x) x = layers.Bidirectional(layers.CuDNNLSTM(64, return_sequences=True))(x) x = layers.Bidirectional(layers.CuDNNLSTM(64, return_sequences=True))(x) avg_pool1d = layers.GlobalAveragePooling1D()(x) max_pool1d = layers.GlobalMaxPool1D()(x) x = layers.concatenate([avg_pool1d, max_pool1d]) x = layers.Dense(32, activation='relu')(x) x = layers.BatchNormalization()(x) output = layers.Dense(1, activation='sigmoid')(x) model = models.Model(sequence_input, output) return model 1234567## embedding_lstm models model = Embedding_CuDNNLSTM_model(max_words, max_len)# model compilemodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc', auroc])model.summary() __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_1 (InputLayer) (None, None) 0 __________________________________________________________________________________________________ embedding_1 (Embedding) (None, 306, 128) 12800000 input_1[0][0] __________________________________________________________________________________________________ spatial_dropout1d_1 (SpatialDro (None, 306, 128) 0 embedding_1[0][0] __________________________________________________________________________________________________ bidirectional_1 (Bidirectional) (None, 306, 128) 99328 spatial_dropout1d_1[0][0] __________________________________________________________________________________________________ bidirectional_2 (Bidirectional) (None, 306, 128) 99328 bidirectional_1[0][0] __________________________________________________________________________________________________ global_average_pooling1d_1 (Glo (None, 128) 0 bidirectional_2[0][0] __________________________________________________________________________________________________ global_max_pooling1d_1 (GlobalM (None, 128) 0 bidirectional_2[0][0] __________________________________________________________________________________________________ concatenate_1 (Concatenate) (None, 256) 0 global_average_pooling1d_1[0][0] global_max_pooling1d_1[0][0] __________________________________________________________________________________________________ dense_1 (Dense) (None, 32) 8224 concatenate_1[0][0] __________________________________________________________________________________________________ batch_normalization_1 (BatchNor (None, 32) 128 dense_1[0][0] __________________________________________________________________________________________________ dense_2 (Dense) (None, 1) 33 batch_normalization_1[0][0] ================================================================================================== Total params: 13,007,041 Trainable params: 13,006,977 Non-trainable params: 64 __________________________________________________________________________________________________ Train model callbackí•¨ìˆ˜ëŠ” ì•„ë˜ì™€ ê°™ì´ ì‚¬ìš©â€” ReduceLROnPlateau() : ì´ˆê¸°ì— í•™ìŠµë¥ ì„ ë†’ê²Œ ì§€ì •í•œ í›„ ì¼ì • epochë™ì•ˆ ì„±ëŠ¥ì´ í–¥ìƒë˜ì§€ ì•Šì„ ì‹œ ì ì°¨ learning rateë¥¼ ì¤„ì—¬ë‚˜ê°â€” EarlyStopping() : ì¼ì • epochë™ì•ˆ ì„±ëŠ¥ í–¥ìƒì´ ì—†ì„ ì‹œ í•™ìŠµì„ ì¡°ê¸° ì¢…ë£Œí•¨.â€” ModelCheckPoint() : epochë§ˆë‹¤ í•™ìŠµ ëœ ëª¨ë¸ì„ ì €ì¥, save_best_only=Trueë¥¼ ì§€ì •í•˜ì—¬ ì„±ëŠ¥ì´ ê°€ì¥ ì¢‹ì€ ëª¨ë¸ë§Œ ì§€ì •í•  ìˆ˜ ìˆìŒ 12def auroc(y_true, y_pred): return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double) í•´ë‹¹ competitionì˜ í‰ê°€ ëª¨ë¸ì˜ ê²½ìš° ROC-AUCë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— í•´ë‹¹ í‰ê°€ì§€í‘œë¡œ ê²€ì¦í•˜ê¸° ìœ„í•´ acrocë¼ëŠ” í•¨ìˆ˜ë¥¼ ì •ì˜. 12345678910111213141516171819# keras.callbackscallbacks_list = [ ReduceLROnPlateau( monitor='val_auroc', patience=2, factor=0.1, mode='max'), # val_lossê°€ patienceë™ì•ˆ í–¥ìƒë˜ì§€ ì•Šìœ¼ë©´ í•™ìŠµë¥ ì„ 0.1ë§Œí¼ ê°ì†Œ (new_lr = lr * factor) EarlyStopping( patience=5, monitor='val_auroc', mode='max', restore_best_weights=True), ModelCheckpoint( filepath='./input/best_embedding_lstm_model.h5', monitor='val_auroc', mode='max', save_best_only=True)]# model fit &amp; savemodel_path = './input/best_embedding_lstm_model.h5'if os.path.exists(model_path): model.load_weights(model_path)else: history = model.fit(pad_train, train_y_label, epochs=7, batch_size=1024, callbacks=callbacks_list, validation_split=0.3, verbose=1) Train on 1263411 samples, validate on 541463 samples Epoch 1/7 1263411/1263411 [==============================] - 579s 458us/step - loss: 0.1831 - acc: 0.9398 - auroc: 0.9263 - val_loss: 0.2086 - val_acc: 0.9169 - val_auroc: 0.9479 Epoch 2/7 1263411/1263411 [==============================] - 577s 457us/step - loss: 0.1187 - acc: 0.9540 - auroc: 0.9600 - val_loss: 0.1792 - val_acc: 0.9356 - val_auroc: 0.9479 Epoch 3/7 1263411/1263411 [==============================] - 577s 456us/step - loss: 0.1017 - acc: 0.9606 - auroc: 0.9717 - val_loss: 0.2070 - val_acc: 0.9359 - val_auroc: 0.9424 Epoch 4/7 1263411/1263411 [==============================] - 576s 456us/step - loss: 0.0707 - acc: 0.9739 - auroc: 0.9866 - val_loss: 0.1806 - val_acc: 0.9386 - val_auroc: 0.9227 Epoch 5/7 1263411/1263411 [==============================] - 576s 456us/step - loss: 0.0639 - acc: 0.9762 - auroc: 0.9890 - val_loss: 0.1942 - val_acc: 0.9345 - val_auroc: 0.9218 Epoch 6/7 1263411/1263411 [==============================] - 577s 457us/step - loss: 0.0584 - acc: 0.9785 - auroc: 0.9908 - val_loss: 0.1988 - val_acc: 0.9374 - val_auroc: 0.9190 12345678# plot score by epochsauroc = history.history['auroc']val_auroc = history.history['val_auroc']epochs = range(1, len(auroc)+1)plt.figure(figsize=(7,3))plt.plot(epochs, auroc, 'b', label='auroc')plt.plot(epochs, val_auroc, 'r', label='validation auroc') [&lt;matplotlib.lines.Line2D at 0x1176f6fdba8&gt;] ê²°ê³¼ë¥¼ ë³´ë‹ˆê²€ì¦ ì„±ëŠ¥ì´ epochì´ ì¦ê°€í•  ìˆ˜ë¡ ë–¨ì–´ì§€ëŠ” ê²ƒìœ¼ë¡œ ë³´ì•„ ëª¨ë¸ì´ ê³¼ëŒ€ì í•© ëœ ë“¯ í•¨. dropout ë¹„ìœ¨ì„ ë” ë†’ì´ê±°ë‚˜, ë ˆì´ì–´ ìˆ˜ë¥¼ ì¤„ì—¬ì•¼ í•  ê²ƒ ê°™ìŒ. Predict test set12## predict test_settest_pred = model.predict(pad_test) 7. submit submission.csv123456sample_result = pd.DataFrame()sample_result['id'] = test_df.indexsample_result['prediction'] = test_pred## submit sample_submission.csvsample_result.to_csv('submission.csv', index=False) Outroìµœì¢… ì œì¶œ ê²°ê³¼ 91.1% ë¼ëŠ” ê²€ì¦ ê²°ê³¼ê°€ ë‚˜ì™€ ìƒìœ„ 84%â€¦. ë¬¸ì œë¥¼ ì œëŒ€ë¡œ ì´í•´ë¥¼ ì•ˆí•˜ê³  ì‹œì‘í•´ì„œ ê·¸ëŸ°ì§€ ëª¨ë¸ ìˆ˜ì •ìœ¼ë¡œëŠ” ì´ ì´ìƒ ì„±ëŠ¥ í–¥ìƒì´ ë˜ì§€ ì•Šì•˜ë‹¤. ë‹¤ë¥¸ ìƒìœ„ ì»¤ë„ì„ ì‚´í´ë³´ë‹ˆ ëŒ€ë¶€ë¶„ feature engineeringë¶€ë¶„ì—ì„œ í…ìŠ¤íŠ¸ ì²˜ë¦¬ì— ë§ì€ ë…¸ë ¥ì„ ê¸°ìš¸ì¸ ê²ƒ ê°™ë‹¤.ë” ìˆ˜ì •í•´ì„œ í•´ë³´ë ¤ê³  í–ˆëŠ”ë°, ì œì¶œ ê¸°ê°„ì´ ì•„ì‰½ê²Œ ì¢…ë£Œê°€ ë˜ì–´ ë” ì§„í–‰í•´ë³´ì§€ëŠ” ì•Šì•˜ë‹¤. ìµœê·¼ ì •ê¶Œìš°ë‹˜ì´ ì“°ì‹  â€˜ë¨¸ì‹ ëŸ¬ë‹ íƒêµ¬ìƒí™œâ€™ì´ë¼ëŠ” ì±…ì„ êµ¬ë§¤í•˜ì˜€ëŠ”ë°, ë‹¤ì–‘í•œ kaggleë¬¸ì œë¥¼ ì–´ë–»ê²Œ ì ‘ê·¼í•´ì•¼ í•˜ëŠ”ì§€, ë˜ ìµœê·¼ kaggleë‚´ì—ì„œ ì–´ë–¤ ëª¨ë¸ì´ ì£¼ë¡œ ì‚¬ìš©ë˜ëŠ”ì§€ íŠ¸ë Œë“œë¥¼ ì‚´í´ë³¼ ìˆ˜ ìˆì„ ê²ƒ ê°™ì•„ ì—´ì‹¬íˆ ì½ì–´ë³´ëŠ” ì¤‘ì´ë‹¤. ì™„ë… í›„ ë‹¤ì‹œ ë‹¤ë¥¸ ìºê¸€ ë¬¸ì œì— ë„ì „í•´ë´ì•¼ê² ë‹¤!","categories":[],"tags":[{"name":"deeplearning","slug":"deeplearning","permalink":"https://jaehyeongan.github.io/tags/deeplearning/"},{"name":"lstm","slug":"lstm","permalink":"https://jaehyeongan.github.io/tags/lstm/"},{"name":"embedding","slug":"embedding","permalink":"https://jaehyeongan.github.io/tags/embedding/"},{"name":"kaggle","slug":"kaggle","permalink":"https://jaehyeongan.github.io/tags/kaggle/"},{"name":"competition","slug":"competition","permalink":"https://jaehyeongan.github.io/tags/competition/"},{"name":"google","slug":"google","permalink":"https://jaehyeongan.github.io/tags/google/"},{"name":"jigsaw","slug":"jigsaw","permalink":"https://jaehyeongan.github.io/tags/jigsaw/"},{"name":"toxicity","slug":"toxicity","permalink":"https://jaehyeongan.github.io/tags/toxicity/"},{"name":"classification","slug":"classification","permalink":"https://jaehyeongan.github.io/tags/classification/"},{"name":"bidirectionallstm","slug":"bidirectionallstm","permalink":"https://jaehyeongan.github.io/tags/bidirectionallstm/"}]},{"title":"[ë”¥ëŸ¬ë‹ì„ ìœ„í•œ ìˆ˜í•™ê¸°ì´ˆ 03] ë¯¸ë¶„, í¸ë¯¸ë¶„","slug":"ë”¥ëŸ¬ë‹ì„-ìœ„í•œ-ìˆ˜í•™ê¸°ì´ˆ-3","date":"2019-06-25T14:59:47.000Z","updated":"2021-02-07T14:58:12.524Z","comments":true,"path":"2019/06/25/ë”¥ëŸ¬ë‹ì„-ìœ„í•œ-ìˆ˜í•™ê¸°ì´ˆ-3/","link":"","permalink":"https://jaehyeongan.github.io/2019/06/25/ë”¥ëŸ¬ë‹ì„-ìœ„í•œ-ìˆ˜í•™ê¸°ì´ˆ-3/","excerpt":"","text":"Introë³¸ ê¸€ì€ â€˜ì²˜ìŒ ë°°ìš°ëŠ” ë”¥ëŸ¬ë‹ ìˆ˜í•™(í•œë¹›ë¯¸ë””ì–´)â€™ì´ë¼ëŠ” ì±…ì˜ â€˜chap.02 ì‹ ê²½ë§ì„ ìœ„í•œ ìˆ˜í•™ê¸°ì´ˆâ€™ ë¥¼ ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤. 1. ë¯¸ë¶„ ë„í•¨ìˆ˜: ì–´ë–¤ í•¨ìˆ˜ ì•ˆì— í¬í•¨ë„ë‹ˆ ê°’ ê°ê°ì´ 0ì— í•œì—†ì´ ê°€ê¹Œì›Œì§€ëŠ” ê·¹í•œê°’(ë¯¸ë¶„ê³„ìˆ˜)ì„ êµ¬í•˜ëŠ” í•¨ìˆ˜ y = f(x)ì˜ ë„í•¨ìˆ˜ fâ€™(x)ëŠ” ì•„ë˜ì™€ ê°™ì´ ì •ì˜ f(x) = 3xì¼ ë•Œ, ë„í•¨ìˆ˜ ê³„ì‚° ê³¼ì • f(x) = x^2ì¼ë–„, ë„í•¨ìˆ˜ ê³„ì‚° ê³¼ì • í•¨ìˆ˜ f(x)ì˜ ë„í•¨ìˆ˜fâ€™(x)ë¥¼ êµ¬í•˜ëŠ” ê²ƒì„ â€œí•¨ìˆ˜ f(x)ë¥¼ ë¯¸ë¶„í•œë‹¤â€ë¼ê³  í•˜ë©°, ìœ„ì™€ ê°™ì´ ê°’ì„ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤ë©´ ë¯¸ë¶„ ê°€ëŠ¥ì´ë¼ê³  í•¨ ë¯¸ë¶„ê¸°í˜¸ y = f(x)ì˜ ë„í•¨ìˆ˜ fâ€™(x)ë¥¼ ê·¹í•œ ê°œë…ì´ ì•„ë‹Œ ë¶„ìˆ˜ë¡œ í‘œí˜„í•˜ëŠ” ë°©ë²• fâ€™(x) = dy/dx ë¯¸ë¶„ ì„±ì§ˆ ë¯¸ë¶„ì˜ ì„ í˜•ì„± í•¨ìˆ˜ í•©ì˜ ë¯¸ë¶„ì€ ê° í•¨ìˆ˜ë¥¼ ë¯¸ë¶„í•œ í•©ê³¼ ê°™ë‹¤.ìƒìˆ˜ë¥¼ ê³±í•œ í•¨ìˆ˜ì˜ ë¯¸ë¶„ì€ ë¯¸ë¶„í•œ í•¨ìˆ˜ì— ìƒìˆ˜ë¥¼ ê³±í•œ ê²ƒê³¼ ê°™ë‹¤. ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ì˜ ë¯¸ë¶„ ìœ„ì˜ ì‹ì„ ì´ìš©í•˜ë©´ ë¯¸ë¶„í•˜ì§€ ì•Šì•„ë„ ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ì˜ ë„í•¨ìˆ˜ê°’ì„ sigma(x)ì˜ ê°’ì—ì„œ ì–»ì„ ìˆ˜ ìˆìŒ. ìµœì†Ÿê°’ì˜ í•„ìš”ì¡°ê±´ í•¨ìˆ˜ f(x)ê°€ x = 0ì¼ ë•Œ ìµœì†Ÿê°’ì´ë¼ë©´ fâ€™(a)=0 fâ€™(a)=0ëŠ” í•¨ìˆ˜ f(x)ê°€ x = aì—ì„œ ìµœì†Ÿê°’ì´ ë˜ê¸° ìœ„í•œ â€œí•„ìš”â€ì¡°ê±´. ì´ëŠ” ì ‘ì„ ì˜ ê¸°ìš¸ê¸°ê°€ 0ì´ë”ë¼ë„ ê¼­ ìµœì†Ÿê°’ì´ë¼ëŠ” ë³´ì¥ì´ ì—†ë‹¤ëŠ” ì˜ë¯¸ ê²½ì‚¬í•˜ê°•ë²•ì€ ì ‘ì„ ì˜ ê¸°ìš¸ê¸°ê°€ ë‚®ì€ ìª½ìœ¼ë¡œ ê³„ì† ì´ë™ì‹œì¼œì„œ ìµœì†Ÿê°’ì„ êµ¬í•¨ í•¨ìˆ˜ ì „ì²´ì˜ ìµœì†Ÿê°’ê³¼ ê°’ì´ ì»¤ì§€ê±°ë‚˜ ì‘ì•„ì§ˆ ë•Œ ë°œìƒí•˜ëŠ” ê·¹ì†Ÿê°’/ê·¹ëŒ“ê°’ì„ í˜¼ë™í•  ìˆ˜ ìˆìœ¼ë‹ˆ, ê²½ì‚¬í•˜ê°•ë²•ìœ¼ë¡œ ìµœì†Ÿê°’ì„ êµ¬í•  ë•Œ ì£¼ì˜í•´ì•¼ í•¨ 2. í¸ë¯¸ë¶„ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ ë…ë¦½ë³€ìˆ˜ê°€ 2ê°œ ì´ìƒì¸ í•¨ìˆ˜ í¸ë¯¸ë¶„ ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ì˜ ê²½ìš° ë³€ìˆ˜ê°€ ì—¬ëŸ¬ ê°œ ìˆìœ¼ë¯€ë¡œ ì–´ë–¤ ë³€ìˆ˜ë¥¼ ë¯¸ë¶„í• ì§€ ëª…ì‹œí•´ì•¼ í•˜ëŠ”ë°, ì´ë ‡ê²Œ íŠ¹ì • ë³€ìˆ˜ë¥¼ ëª…ì‹œí•´ ë¯¸ë¶„í•˜ëŠ” ê²ƒì„ í¸ë¯¸ë¶„ì´ë¼ê³  í•¨ z = f(x, y)ì¼ ë•Œ, ë³€ìˆ˜ xë¥¼ ë¯¸ë¶„í•˜ê³  yë¥¼ ìƒìˆ˜ë¡œ ì·¨ê¸‰í•˜ëŠ” ê²ƒì„ â€˜xì— ê´€í•œ í¸ë¯¸ë¶„â€™ì´ë¼ê³  í•¨ xì— ê´€í•œ í¸ë¯¸ë¶„ yì— ê´€í•œ í¸ë¯¸ë¶„ z = wx+bì— ê´€í•œ í¸ë¯¸ë¶„","categories":[],"tags":[{"name":"deeplearning","slug":"deeplearning","permalink":"https://jaehyeongan.github.io/tags/deeplearning/"},{"name":"math","slug":"math","permalink":"https://jaehyeongan.github.io/tags/math/"},{"name":"neuralnetwork","slug":"neuralnetwork","permalink":"https://jaehyeongan.github.io/tags/neuralnetwork/"},{"name":"derivative","slug":"derivative","permalink":"https://jaehyeongan.github.io/tags/derivative/"},{"name":"partialderivative","slug":"partialderivative","permalink":"https://jaehyeongan.github.io/tags/partialderivative/"}]},{"title":"[ë”¥ëŸ¬ë‹ì„ ìœ„í•œ ìˆ˜í•™ê¸°ì´ˆ 02] ìˆ˜ì—´, ì‹œê·¸ë§ˆ, ë²¡í„°, í–‰ë ¬","slug":"ë”¥ëŸ¬ë‹ì„-ìœ„í•œ-ìˆ˜í•™ê¸°ì´ˆ-2","date":"2019-06-20T09:38:25.000Z","updated":"2021-02-07T14:57:54.698Z","comments":true,"path":"2019/06/20/ë”¥ëŸ¬ë‹ì„-ìœ„í•œ-ìˆ˜í•™ê¸°ì´ˆ-2/","link":"","permalink":"https://jaehyeongan.github.io/2019/06/20/ë”¥ëŸ¬ë‹ì„-ìœ„í•œ-ìˆ˜í•™ê¸°ì´ˆ-2/","excerpt":"","text":"Introë³¸ ê¸€ì€ â€˜ì²˜ìŒ ë°°ìš°ëŠ” ë”¥ëŸ¬ë‹ ìˆ˜í•™(í•œë¹›ë¯¸ë””ì–´)â€™ì´ë¼ëŠ” ì±…ì˜ â€˜chap.02 ì‹ ê²½ë§ì„ ìœ„í•œ ìˆ˜í•™ê¸°ì´ˆâ€™ ë¥¼ ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤. 1. ìˆ˜ì—´ ìˆ˜ì—´ì€ â€˜ìˆ«ì ì—´â€™ì„ ì˜ë¯¸, ì˜ˆë¥¼ ë“¤ì–´ 2,4,6,8,10,.. ë¡œ ì§„í–‰ë˜ë©´ ì§ìˆ˜ì—´ì´ë¼ëŠ” ìˆ˜ì—´ì„ ìˆ˜ì—´ì—ì„œ ì •ë ¬í•˜ëŠ” ìˆ«ì í•˜ë‚˜í•˜ë‚˜ë¥¼ í•­ì´ë¼ê³  í•¨. ì²«ë²ˆì§¸ í•­ì€ 1í•­, ë‘ë²ˆì§¸ í•­ì€ 2í•­, në²ˆì§¸ í•­ì€ ní•­ ì‹ ê²½ë§ì˜ ìˆ˜ì—´ì€ ìœ í•œê°œì˜ í•­ ìˆ˜ë¥¼ ê°–ëŠ” ìˆ˜ì—´ë¡œì„œ ìœ í•œìˆ˜ì—´ì´ë¼ê³  í•¨ ìˆ˜ì—´ê³¼ ì í™”ì‹ ì í™”ì‹ì´ë€ ì´ì›ƒì— ìˆëŠ” í•­ì˜ ê´€ê³„ë¡œ í‘œí˜„í•˜ëŠ” ìˆ˜ì—´ì˜ ê·€ë‚©ì  ì •ì˜. ì¼ë°˜ì ìœ¼ë¡œ 1í•­ A1ê³¼ ì¸ì ‘í•œ 2ê°œì˜ í•­ An, An+1ì˜ ê´€ê³„ì‹ìœ¼ë¡œ ìˆ˜ì—´ {An}ì„ í‘œí˜„ ì—°ë¦½ ì í™”ì‹ ì—¬ëŸ¬ ìˆ˜ì—´ì´ ëª‡ ê°€ì§€ ê´€ê³„ì‹ìœ¼ë¡œ ì—°ê²°ëœ ê²ƒì„ ì—°ë¦½ ì í™”ì‹ì´ë¼ê³  í•¨. ì‹ ê²½ë§ì—ì„œëŠ” ëª¨ë“  ìœ ë‹›ì˜ ì…ë ¥ê³¼ ì¶œë ¥ì´ ì—°ë¦½ ì í™”ì‹ìœ¼ë¡œ ì—°ê²°ë˜ì–´ ìˆìŒ 2. ì‹œê·¸ë§ˆ ê¸°í˜¸ ìœ„ì—ì„œ ì‚´í´ë³¸ ìˆ˜ì—´ì˜ í•©ì„ ê°„ê²°í•˜ê²Œ í‘œí˜„í•˜ëŠ” ê²ƒì´ ì‹œê·¸ë§ˆ ê¸°í˜¸. 1í•­ë¶€í„° ní•­ê¹Œì§€ì˜ ìˆ˜ì—´ì˜ í•©ì„ ì‹œê·¸ë§ˆ ê¸°í˜¸ë¡œ ë‚˜íƒ€ë‚´ë©´ ì•„ë˜ì™€ ê°™ìŒ ì‹œê·¸ë§ˆ ê¸°í˜¸ì— ìˆëŠ” ë¬¸ì këŠ” í•­ ìˆ«ìë¥¼ ì˜ë¯¸. ì‹œê·¸ë§ˆ ê¸°í˜¸ì˜ íŠ¹ì§• - ì„ í˜•ì„± ì‹œê·¸ë§ˆ ê¸°í˜¸ëŠ” â€˜ì„ í˜•ì„±â€™ì´ë¼ëŠ” íŠ¹ì§•ì„ ê°€ì§€ê³  ìˆìŒ. ìœ„ ì‹ì„ ì‹¤ì œ ìˆ˜ì—´ë¡œ ì „ê°œí•˜ë©´ ì•„ë˜ì™€ ê°™ìŒ. 3. í–‰ë ¬í–‰ë ¬(matrix)ì€ ìˆ˜ì™€ ì‹ì„ ì‚¬ê° í˜•íƒœë¡œ ë‚˜ì—´í•œ ê²ƒìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„ ê°€ë¡œì¤„ì€ í–‰, ì„¸ë¡œì¤„ì€ ì—´ ìœ„ ê·¸ë¦¼ê³¼ ê°™ì´ 3í–‰, 3ì—´ë¡œ êµ¬ì„±ëœ í–‰ë ¬ì€ 3x3í–‰ë ¬ ì •ì‚¬ê°í–‰ë ¬ì´ë€ í–‰ê³¼ ì—´ ìˆ˜ê°€ ê°™ì€ í–‰ë ¬ ì•„ë˜ì™€ ê°™ì´ í•˜ë‚˜ì˜ ì—´ì´ë‚˜ í–‰ìœ¼ë¡œ êµ¬ì„±ëœ í–‰ë ¬ X, Yë¥¼ ì°¨ë¡€ë¡œ ì—´ë²¡í„°, í–‰ë°±í„°ë¼ê³  í•¨. í–‰ë ¬ì˜ ìƒë“± ë‘ í–‰ë ¬ A, BëŠ” ëŒ€ì‘í•˜ëŠ” ê° ì„±ë¶„ì´ ê°™ì„ ë•Œ ìƒë“±ì´ë¼ê³  í•˜ë©°, ê¸°í˜¸ë¡œëŠ” A = Bë¡œ í‘œí˜„ ì˜ˆë¥¼ ë“¤ì–´, í–‰ë ¬ Aì™€ Bê°€ ì•„ë˜ì™€ ê°™ì„ ë•Œ, ì´ë•Œ A = Bê°€ ë˜ëŠ” x, y, u, vëŠ” 2, 7, 1, 8 í–‰ë ¬ì˜ í•©ê³¼ ì°¨, ìƒìˆ˜ ë°° ë‘ í–‰ë ¬ A, Bì˜ í•© A+B, ì°¨ A-BëŠ” ê°™ì€ ìœ„ì¹˜ ì„±ë¶„ë¼ë¦¬ì˜ í•©ê³¼ ì°¨ë¡œ ì •ì˜ í–‰ë ¬ì˜ ìƒìˆ˜ ë°°ëŠ” ê° ì„±ë¶„ì— í•´ë‹¹ ìƒìˆ˜ë¥¼ ê³±í•œ ê²ƒìœ¼ë¡œ ì •ì˜ í–‰ë ¬ì˜ ê³±ì…ˆ í–‰ë ¬ì˜ ê³±ì…ˆì€ ì‹ ê²½ë§ ì¸µ ì‚¬ì´ì˜ ì‹ í˜¸ì˜ í•© ë“±ì„ ê³„ì‚°í•  ë•Œ ì´ìš©ë˜ê¸° ë•Œë¬¸ì— íŠ¹íˆ ì¤‘ìš” í–‰ë ¬ì˜ ê³±ì…ˆì€ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ ë‘ í–‰ë ¬ A, Bì˜ ê³± ABëŠ” ií–‰ì„ í–‰ë²¡í„°ë¡œ, Bì˜ jì—´ì„ ì—´ë²¡í„°ë¡œ ìƒê°í–ˆì„ ë•Œ, í–‰ë²¡í„°ì™€ ì—´ë²¡í„°ì˜ ë‚´ì ì„ ií–‰ jì—´ì˜ ì„±ë¶„ìœ¼ë¡œ í•˜ëŠ” í–‰ë ¬ í–‰ë ¬ ê³±ì…ˆì˜ ì˜ˆ ìœ„ ì˜ˆì²˜ëŸ¼ í–‰ë ¬ ê³±ì…ˆì—ì„œëŠ” êµí™˜ë²•ì¹™ì´ ì„±ë¦½í•˜ì§€ ì•ŠìŒ(AB â‰  BA) ì•„ë‹¤ë§ˆë¥´ ê³± ê°™ì€ í–‰ê³¼ ì—´ ìˆ˜ë¥¼ ê°–ëŠ” í–‰ë ¬ A, Bì—ì„œ ê°™ì€ ìœ„ì¹˜ì˜ ì„±ë¶„ì„ ê³±í•œ í–‰ë ¬ì„ â€˜í–‰ë ¬ A, Bì˜ ì•„ë‹¤ë§ˆë¥´ ê³±â€™ì´ë¼ê³  í•¨ ê¸°í˜¸ Aã…‡Bë¡œ í‘œí˜„ ì „ì¹˜í–‰ë ¬ í–‰ë ¬ Aì˜ ií–‰ jì—´ ê°’ì„ jí–‰ iì—´ë¡œ ë°”ê¿” ì–»ëŠ” í–‰ë ¬ì„ í–‰ë ¬ Aì˜ ì „ì¹˜í–‰ë ¬(Transposed Matrix)ë¼ê³  í•¨ í–‰ë ¬ Aì™€ Bê°€ ì•„ë˜ì™€ ê°™ì„ ë•Œ ì „ì¹˜í–‰ë ¬ Aì™€ í–‰ë ¬Bì˜ ê³±ì…ˆêµ¬í•˜ëŠ” ì‹","categories":[],"tags":[{"name":"deeplearning","slug":"deeplearning","permalink":"https://jaehyeongan.github.io/tags/deeplearning/"},{"name":"sigma","slug":"sigma","permalink":"https://jaehyeongan.github.io/tags/sigma/"},{"name":"vector","slug":"vector","permalink":"https://jaehyeongan.github.io/tags/vector/"},{"name":"matrix","slug":"matrix","permalink":"https://jaehyeongan.github.io/tags/matrix/"},{"name":"neuralnetwork","slug":"neuralnetwork","permalink":"https://jaehyeongan.github.io/tags/neuralnetwork/"}]},{"title":"[ë”¥ëŸ¬ë‹ì„ ìœ„í•œ ìˆ˜í•™ê¸°ì´ˆ 01] ì‹ ê²½ë§ì˜ í•„ìˆ˜ í•¨ìˆ˜","slug":"ë”¥ëŸ¬ë‹-ìœ„í•œ-ìˆ˜í•™ê¸°ì´ˆ-1","date":"2019-06-19T14:21:25.000Z","updated":"2021-02-07T14:57:44.318Z","comments":true,"path":"2019/06/19/ë”¥ëŸ¬ë‹-ìœ„í•œ-ìˆ˜í•™ê¸°ì´ˆ-1/","link":"","permalink":"https://jaehyeongan.github.io/2019/06/19/ë”¥ëŸ¬ë‹-ìœ„í•œ-ìˆ˜í•™ê¸°ì´ˆ-1/","excerpt":"","text":"Introë³¸ ê¸€ì€ â€˜ì²˜ìŒ ë°°ìš°ëŠ” ë”¥ëŸ¬ë‹ ìˆ˜í•™(í•œë¹›ë¯¸ë””ì–´)â€™ì´ë¼ëŠ” ì±…ì˜ â€˜chap.02 ì‹ ê²½ë§ì„ ìœ„í•œ ìˆ˜í•™ê¸°ì´ˆâ€™ ë¥¼ ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤. 1. 1ì°¨ í•¨ìˆ˜ aë¥¼ ê¸°ìš¸ê¸°, bë¥¼ ì ˆí¸ì´ë¼ê³  í•˜ë©° ë‘ ë³€ìˆ˜ x, yê°€ ìœ„ ì‹ì˜ ê´€ê³„ë¥¼ ë§Œì¡±í•  ë•Œ ë³€ìˆ˜ yëŠ” ë³€ìˆ˜ xì™€ â€˜1ì°¨ í•¨ìˆ˜ ê´€ê³„â€™ë¼ê³  í•¨ 1ì°¨í•¨ìˆ˜ë¥¼ ê·¸ë˜í”„ë¡œ ê·¸ë¦¬ë©´ ì•„ë˜ì™€ ê°™ì€ ì§ì„ ìœ¼ë¡œ ë‚˜íƒ€ë‚¨ y = 2x+1ì˜ ê·¸ë˜í”„ë¼ë©´, ì ˆí¸ì€ 1, ê¸°ìš¸ê¸°ëŠ” 2(ì•„ë˜ ì™¼ìª½) y = -2x-1ì˜ ê·¸ë˜í”„ë¼ë©´, ì ˆí¸ì€ -1, ê¸°ìš¸ê¸°ëŠ” -2(ì•„ë˜ ì˜¤ë¥¸ìª½) 1ì°¨ í•¨ìˆ˜ëŠ” ë…ë¦½ë³€ìˆ˜ê°€ ì—¬ëŸ¬ ê°œì¼ ë•Œë„ ìˆìŒ ì‹ ê²½ë§ì—ì„œ ìœ ë‹›ì´ ë°›ëŠ” â€˜ê°€ì¤‘ ì…ë ¥â€™ì€ 1ì°¨ í•¨ìˆ˜ ê´€ê³„ë¡œ í‘œí˜„. ì˜ˆë¥¼ ë“¤ì–´ ì•„ë˜ì¸µì—ì„œ 3ê°œì˜ ì…ë ¥ ì‹ í˜¸ë¥¼ ë°›ì€ ìœ ë‹›ì˜ ê°€ì¤‘ ì…ë ¥ zëŠ” ì•„ë˜ì™€ ê°™ì´ í‘œí˜„. ê°€ì¤‘ì¹˜ w1, w2, w3ì™€ í¸í–¥ bë¥¼ ìƒìˆ˜ íŒŒë¼ë¯¸í„°ë¼ê³  ìƒê°í•˜ë©´ ê°€ì¤‘ ì…ë ¥ zëŠ” ì…ë ¥ x1, x2, x3ê³¼ 1ì°¨í•¨ìˆ˜ ê´€ê³„ ë˜í•œ ìœ ë‹›ì´ ë°›ëŠ” x1, x2, x3ì„ ì…ë ¥ ë°ì´í„°ê°’ìœ¼ë¡œ í™•ì •í–ˆë‹¤ë©´ ê°€ì¤‘ ì…ë ¥ zëŠ” ê°€ì¤‘ì¹˜ w1, w2, w3 ë° í¸í–¥ bì™€ 1ì°¨ í•¨ìˆ˜ ê´€ê³„ 2. 2ì°¨ í•¨ìˆ˜ 2ì°¨ í•¨ìˆ˜ ê·¸ë˜í”„ì—ì„œ ì¤‘ìš”í•œ ê²ƒì€ aê°€ ì–‘ìˆ˜ì¼ ë•ŒëŠ” ì•„ë˜ë¡œ ë³¼ë¡í•œ ê·¸ë˜í”„ê³ , ìµœì†Ÿê°’ì´ ì¡´ì¬í•œë‹¤ëŠ” ì . 3. ë‹¨ìœ„ ê³„ë‹¨ í•¨ìˆ˜ u(-1) = 0, u(1) = 1, u(0) = 1 ë‹¨ìœ„ ê³„ë‹¨ í•¨ìˆ˜ëŠ” ì›ì ì—ì„œ ë¶ˆì—°ì† ì¦‰, â€˜ë¯¸ë¶„ ë¶ˆê°€ëŠ¥â€™ ë¯¸ë¶„ ë¶ˆê°€ëŠ¥í•œ íŠ¹ì„±ìœ¼ë¡œ ì¸í•´ ì‹ ê²½ë§ì˜ í™œì„±í™” í•¨ìˆ˜ë¡œ ì˜ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ. 4. ì§€ìˆ˜í•¨ìˆ˜ì™€ ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ ìœ„ì™€ ê°™ì€ ì‹ì„ ì§€ìˆ˜í•¨ìˆ˜ë¼ê³  í•¨. ìƒìˆ˜ aëŠ” ì§€ìˆ˜í•¨ìˆ˜ì˜ ë°‘(base)ë¼ê³  í•˜ë©°, ë°‘ì˜ ê°’ìœ¼ë¡œ íŠ¹íˆ ì¤‘ìš”í•œ ê²ƒì€ ìì—°ìƒìˆ˜ e e = 2.718281828â€¦ ìì—°ìƒìˆ˜ë¥¼ í¬í•¨í•˜ëŠ” ì§€ìˆ˜í•¨ìˆ˜ë¥¼ ë¶„ëª¨ë¡œ ê°–ëŠ” í•¨ìˆ˜ê°€ ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ ì‹ ê²½ë§ì—ì„œ ì‚¬ìš©ë˜ëŠ” ëŒ€í‘œì ì¸ í™œì„±í™” í•¨ìˆ˜ ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ëŠ” ì•„ë˜ì™€ ê°™ì´ Sìí˜•íƒœì˜ ê·¸ë˜í”„ë¡œ ê·¸ë ¤ì§ 5. ì •ê·œë¶„í¬ì˜ í™•ë¥ ë°€ë„í•¨ìˆ˜ ì‹ ê²½ë§ì„ ì»´í“¨í„°ì—ì„œ ì„¤ì •í•  ë•Œ ê°€ì¤‘ì¹˜ ë° í¸í–¥ì˜ ì´ˆê¹ƒê°’ì„ ì„¤ì •í•´ì•¼ í•˜ëŠ”ë° ì´ ì´ˆê¸°ê°’ì„ êµ¬í•  ë•Œ ë„ì›€ì´ ë˜ëŠ” ê²ƒì´ ì •ê·œ ë¶„í¬ì„. ì´ ë¶„í¬ë¥¼ ë”°ë¥´ëŠ” ì •ê·œë¶„í¬ ë‚œìˆ˜ë¥¼ ì´ˆê¹ƒê°’ìœ¼ë¡œ ì‚¬ìš©í•˜ë©´ ì‹ ê²½ë§ ê³„ì‚° ì‹œ ì¢‹ì€ ê²°ê³¼ë¥¼ ì–»ëŠ”ë‹¤ê³  ì•Œë ¤ì ¸ ìˆìŒ. ì •ê·œë¶„í¬ëŠ” í™•ë¥ ë°€ë„í•¨ìˆ˜ f(x)ë¥¼ ë”°ë¥´ëŠ” í™•ë¥ ë¶„í¬ë¥¼ ë§í•¨(ì•„ë˜ ì‹) muì€ ê¸°ëŒ“ê°’(í‰ê· ê°’), sigmaëŠ” í‘œì¤€í¸ì°¨ë¼ê³  í•˜ë©° ëª¨ë‘ ìƒìˆ˜ì„. ê·¸ë˜í”„ëŠ” ì¢… ëª¨ì–‘","categories":[],"tags":[{"name":"deeplearning","slug":"deeplearning","permalink":"https://jaehyeongan.github.io/tags/deeplearning/"},{"name":"math","slug":"math","permalink":"https://jaehyeongan.github.io/tags/math/"},{"name":"sigmoid","slug":"sigmoid","permalink":"https://jaehyeongan.github.io/tags/sigmoid/"},{"name":"sigma","slug":"sigma","permalink":"https://jaehyeongan.github.io/tags/sigma/"},{"name":"vector","slug":"vector","permalink":"https://jaehyeongan.github.io/tags/vector/"},{"name":"matrix","slug":"matrix","permalink":"https://jaehyeongan.github.io/tags/matrix/"}]},{"title":"ì•„ë‚˜ì½˜ë‹¤(Anaconda) ê°€ìƒí™˜ê²½ ë§Œë“¤ê¸°","slug":"ì•„ë‚˜ì½˜ë‹¤-ê°€ìƒí™˜ê²½-ë§Œë“¤ê¸°","date":"2019-06-13T15:29:09.000Z","updated":"2020-10-28T15:14:34.000Z","comments":true,"path":"2019/06/14/ì•„ë‚˜ì½˜ë‹¤-ê°€ìƒí™˜ê²½-ë§Œë“¤ê¸°/","link":"","permalink":"https://jaehyeongan.github.io/2019/06/14/ì•„ë‚˜ì½˜ë‹¤-ê°€ìƒí™˜ê²½-ë§Œë“¤ê¸°/","excerpt":"","text":"Introì§€ë‚œë²ˆì— data scienceë¥¼ ìœ„í•´ ì•„ë‚˜ì½˜ë‹¤ë¥¼ ì„¤ì¹˜í•˜ëŠ” ë²•ì„ ì•Œì•„ë³´ì•˜ëŠ”ë°, ì‹¤ì œ ì—…ë¬´ì—ì„œ ì—¬ëŸ¬ ë¨¸ì‹ ëŸ¬ë‹ í”„ë¡œì íŠ¸ë¥¼ ë™ì‹œì— ìˆ˜í–‰í•  ê²½ìš° ê° í”„ë¡œì íŠ¸ê°€ ìš”êµ¬í•˜ëŠ” í™˜ê²½ì´ ë‹¤ë¥¼ ìˆ˜ê°€ ìˆë‹¤. ê·¸ëŸ°ë° í•œ ê°€ì§€ í™˜ê²½ì—ì„œ ëª¨ë‘ ì§„í–‰í•  ê²½ìš° ì„œë¡œ dependency ì—ëŸ¬ê°€ ë°œìƒí•˜ê±°ë‚˜ í™˜ê²½ì´ ê¼¬ì—¬ë²„ë¦´ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ê°ê° ë…ë¦½ëœ í™˜ê²½ì„ ë§Œë“¤ì–´ì£¼ëŠ” ê²ƒì´ ì¢‹ë‹¤.ì´ë¥¼ ìœ„í•´ ì•„ë‚˜ì½˜ë‹¤ì—ì„œ ë…ë¦½ëœ ê°€ìƒí™˜ê²½ì„ ì–´ë–»ê²Œ ë§Œë“œëŠ”ì§€ ê°„ë‹¨íˆ ì•Œì•„ë³´ì. Anaconda ê°€ìƒí™˜ê²½ ìƒì„±anaconda promptë¥¼ ì‹¤í–‰í•˜ì—¬ ì•„ë˜ì™€ ê°™ì´ ì‹¤í–‰í•´ë³´ì.12345&gt; conda info --envs# conda environments:#base * C:\\Users\\nonam\\Miniconda3 ìœ„ ëª…ë ¹ì–´ëŠ” í˜„ì¬ ë³¸ì¸ì˜ ì•„ë‚˜ì½˜ë‹¤ì— ì¡´ì¬í•˜ëŠ” í™˜ê²½ ëª©ë¡ì„ ë³´ì—¬ì£¼ëŠ” ëª…ë ¹ì–´ì´ë‹¤. baseëŠ” ê¸°ë³¸ì ì¸ ì•„ë‚˜ì½˜ë‹¤ í™˜ê²½ì„ ë§í•˜ë©° ì•„ì§ ê°€ìƒí™˜ê²½ì´ ì¶”ê°€ë˜ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì— base í™˜ê²½ë§Œ ë‚˜ì˜¤ê²Œ ëœë‹¤. 1. ê°€ìƒí™˜ê²½ ì¶”ê°€1&gt; conda create -n test_envs python=3.6 â€˜conda create -nâ€™ ì´ë¼ëŠ” ëª…ë ¹ì–´ë¥¼ í†µí•´ ê°€ìƒí™˜ê²½ì„ ì¶”ê°€í•  ìˆ˜ ìˆìœ¼ë©° ë°”ë¡œ ë’¤ì— ì›í•˜ëŠ” ê°€ìƒí™˜ê²½ ì´ë¦„ì„ ì ëŠ”ë‹¤.ì´í›„ â€˜python= x.xâ€™ ì„ í†µí•´ ê°€ìƒí™˜ê²½ì˜ python versionì„ ì„¤ì •í•´ì¤„ ìˆ˜ ìˆë‹¤. 2. ê°€ìƒí™˜ê²½ ëª©ë¡ í™•ì¸123456&gt; conda info --envs# conda environments:#base * C:\\Users\\nonam\\Miniconda3test_envs C:\\Users\\nonam\\Miniconda3\\envs\\test_envs ì´í›„ ë‹¤ì‹œ ëª©ë¡ì„ í™•ì¸í•˜ë©´ test_envsë¼ëŠ” ì´ë¦„ì˜ í™˜ê²½ì´ ìƒˆë¡œ ì¶”ê°€ëœ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. 3. ê°€ìƒí™˜ê²½ í™œì„±í™”1&gt; conda activate test_envs â€˜activate test_envsâ€™ì™€ ê°™ì´ ìˆ˜í–‰í•˜ë©´ í•´ë‹¹ ê°€ìƒí™˜ê²½ì´ í™œì„±í™”ê°€ ë˜ë©°,ì›ë˜ í™œì„±í™” ë˜ì–´ìˆë˜ (base)ê°€ (test_envs)ë¡œ ë°”ë€ŒëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. 4. ê°€ìƒí™˜ê²½ ë¹„í™œì„±í™”1&gt; conda deactivate conda deactivateë¥¼ í†µí•´ í˜„ì¬ í™œì„±í™” ëœ ê°€ìƒí™˜ê²½ì„ ë¹„í™œì„±í™” ì‹œí‚¨ë‹¤. 5. ê°€ìƒí™˜ê²½ ì œê±°1&gt; conda env remove -n test_env Outroê°€ë” í”„ë¡œì íŠ¸ë¥¼ ìˆ˜í–‰í•˜ë‹¤ë³´ë©´ ì‹¤í–‰ë˜ëŠ” ëª¨ë“ˆë“¤ì´ python ë²„ì „ì— ë”°ë¼ ì‹¤í–‰ì´ ë˜ëŠ” ê²ƒì´ ìˆê³  ì•ˆë˜ëŠ” ê²ƒì´ ìˆê¸°ë„ í•˜ê³ , ì˜ì¡´ì„± ë¬¸ì œë¡œ ì¸í•´ ì˜ ì‚¬ìš©í•˜ë˜ ë‹¤ë¥¸ ëª¨ë“ˆë“¤ì´ ê°‘ìê¸° ë‹¤ìš´ê·¸ë ˆì´ë“œ ë˜ê±°ë‚˜ ì‚­ì œë˜ëŠ” í˜„ìƒë„ ìˆìœ¼ë¯€ë¡œ ì´ë ‡ê²Œ ê°€ìƒí™˜ê²½ì„ ë§Œë“¤ì–´ ì£¼ì–´ ë…ë¦½ëœ í™˜ê²½ì—ì„œ í”„ë¡œì íŠ¸ë¥¼ ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ ì¢‹ì„ ê²ƒ ê°™ë‹¤.","categories":[],"tags":[{"name":"machinelearning","slug":"machinelearning","permalink":"https://jaehyeongan.github.io/tags/machinelearning/"},{"name":"python","slug":"python","permalink":"https://jaehyeongan.github.io/tags/python/"},{"name":"anaconda","slug":"anaconda","permalink":"https://jaehyeongan.github.io/tags/anaconda/"},{"name":"miniconda","slug":"miniconda","permalink":"https://jaehyeongan.github.io/tags/miniconda/"},{"name":"datascience","slug":"datascience","permalink":"https://jaehyeongan.github.io/tags/datascience/"},{"name":"envs","slug":"envs","permalink":"https://jaehyeongan.github.io/tags/envs/"}]},{"title":"ì°¨ì›ì¶•ì†Œ(Dimensionality Reduction)","slug":"Dimension-Reduction","date":"2019-05-27T12:53:37.000Z","updated":"2020-12-10T14:51:17.000Z","comments":true,"path":"2019/05/27/Dimension-Reduction/","link":"","permalink":"https://jaehyeongan.github.io/2019/05/27/Dimension-Reduction/","excerpt":"","text":"Introë§ì€ ê²½ìš° ë¨¸ì‹ ëŸ¬ë‹ ë¬¸ì œëŠ” í›ˆë ¨ ìƒ˜í”Œì´ ìˆ˜ì²œ ì‹¬ì§€ì–´ ìˆ˜ë°±ë§Œ ê°œì˜ íŠ¹ì„±ì„ ê°€ì§€ê³  ìˆë‹¤. ì´ëŠ” ëª¨ë¸ì˜ í•™ìŠµì„ ëŠë¦¬ê²Œ í•  ë¿ë§Œ ì•„ë‹ˆë¼ ì •êµí•œ ëª¨ë¸ì„ ë§Œë“¤ê¸° ì–´ë µê²Œ í•˜ëŠ”ë° ì´ëŸ¬í•œ ë¬¸ì œë¥¼ ì°¨ì›ì˜ ì €ì£¼(curse of dimensionality)ë¼ê³  í•œë‹¤.ê·¸ë¦¬ê³  ì´ëŸ¬í•œ ì°¨ì›ì˜ ì €ì£¼ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì°¨ì› ì¶•ì†Œ ê¸°ë²•ì´ ì´ìš©ëœë‹¤. ì°¨ì›ì˜ ì €ì£¼(curse of dimensionality)í˜„ì¬ ìš°ë¦¬ëŠ” 3ì°¨ì›ì˜ ê³µê°„ì— ì‚´ê³  ìˆì–´ ê·¸ ë³´ë‹¤ í° 4ì°¨ì›, 5ì°¨ì› ì´ìƒì˜ ê³µê°„ì„ ë¨¸ë¦¬ì†ìœ¼ë¡œ ë– ì˜¬ë¦¬ê¸° í˜ë“¤ë‹¤. ì¦‰, ì°¨ì›(Dimensionality)ì´ë¼ëŠ” ê²ƒì€ ê³µê°„ì„ ëœ»í•˜ê³  ìœ„ ê·¸ë¦¼ê³¼ ê°™ì´ 1ê°œì˜ ì ì¸ 0ì°¨ì› ë¶€í„° ì‹œì‘í•˜ì—¬ 4ì°¨ì›ê¹Œì§€ ê³µê°„ì€ ëª‡ ê°œì˜ ì ê³¼ ì„ ì„ ê·¸ë¦¬ëŠëƒì— ë”°ë¼ ë¬´ìˆ˜íˆ ë§ì€ ì°¨ì›ì„ ê°€ì§€ê²Œ ëœë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ë‹¨ìœ„ ë©´ì ì—ì„œ ì„ì˜ì˜ ë‘ ì ì„ ì„ íƒí•˜ì˜€ì„ ê²½ìš° ë‘ ì  ì‚¬ì´ì˜ ê±°ë¦¬ëŠ” ëŒ€ëµ 0.52ê°€ ëœë‹¤. ì´ë¥¼ 3ì°¨ì› íë¸Œì— ë‚˜íƒ€ë‚¼ ê²½ìš° ë‘ ì ì‚¬ì´ì˜ ê±°ë¦¬ëŠ” 0.66ì •ë„ê°€ ëœë‹¤. í•˜ì§€ë§Œ ë§Œì•½ 1,000,000ì°¨ì›ì˜ ì´ˆì…ë°©ì²´ì—ì„œ ë‘ ì ì„ ë¬´ì‘ìœ„ë¡œ ì„ íƒí•  ê²½ìš°ëŠ” ì–´ë–¨ê¹Œ? í‰ê·  ê±°ë¦¬ëŠ” ëŒ€ëµ 428.25ê°€ ëœë‹¤. ì°¨ì›ì˜ ë†’ì•„ì§ìœ¼ë¡œì¨ ë‘ ì  ì‚¬ì´ë¥¼ í‘œí˜„í•˜ëŠ” ê±°ë¦¬ê°€ ê³ ë¬´ì¤„ì²˜ëŸ¼ ëŠ˜ì–´ë‚˜ë²„ë ¸ëŠ”ë°, ì´ë ‡ë“¯ ê³ ì°¨ì›ì˜ ê³µê°„ì€ ì‚¬ì‹¤ìƒ ë°ì´í„° ê°„ ê±°ë¦¬ê°€ ë¨¼ êµ‰ì¥íˆ í¬ë°•í•œ ìƒíƒœë¼ í•  ìˆ˜ ìˆë‹¤. ì´ê²ƒì„ ë°ì´í„° ê´€ì ì—ì„œ ë³´ìë©´ ë°ì´í„°ì˜ ì‚¬ì´ì¦ˆì™€ í¬ê¸°ê°€ ë°”ë¡œ ì°¨ì›ì´ ë˜ë©°, ë°ì´í„°ì˜ ë³€ìˆ˜ì˜ í¬ê¸°ê°€ ì°¨ì›ì˜ í¬ê¸°ê°€ ë˜ë©° ë³€ìˆ˜ê°€ ë§ìœ¼ë©´ ë§ì„ìˆ˜ë¡ ë°ì´í„°ì˜ ì°¨ì›ì€ ê³„ì†í•´ì„œ ì»¤ì§€ëŠ” ê²ƒì´ë‹¤. ê³ ì°¨ì› ë°ì´í„° ì…‹ì˜ ëª¨ë¸ í•™ìŠµ ë¬¸ì œë°ì´í„°ë¥¼ í‘œí˜„í•˜ëŠ” íŠ¹ì§•ê³¼ ìˆ˜ê°€ ë§ì„ ê²½ìš° ëª¨ë¸ì´ ë” ì˜ í•™ìŠµí•˜ëŠ” ê²ƒì´ ì•„ë‹ê¹Œë¼ëŠ” ìƒê°ì´ ë“¤ì§€ë§Œ, ìœ„ì—ì„œ ë§í–ˆë“¯ì´ ì°¨ì›ì˜ ì •ë„ê°€ ë„ˆë¬´ í´ ê²½ìš° ì˜¤íˆë ¤ ë°ì´í„°ì˜ ì£¼ìš” íŠ¹ì§•ë“¤ì´ í¬ë°•í•´ì§€ëŠ” í˜„ìƒì´ ë°œìƒí•˜ê²Œ ë˜ì–´ ëª¨ë¸ì´ ê³¼ëŒ€ì í•©í•˜ê²Œ ë˜ëŠ” ë¬¸ì œê°€ ë°œìƒí•œë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ í•œê°€ì§€ í•´ê²°ì±…ì€ í›ˆë ¨ ìƒ˜í”Œì˜ ë°€ë„ê°€ ì¶©ë¶„íˆ ë†’ì•„ì§ˆ ë•Œê¹Œì§€ í›ˆë ¨ ì„¸íŠ¸ë¥¼ í‚¤ìš°ëŠ” ê²ƒì¸ë°, ì‹¤ì œë¡œëŠ” ì¼ì • ë°€ë„ì— ë„ë‹¬í•˜ê¸° ìœ„í•´ í•„ìš”í•œ í›ˆë ¨ ìƒ˜í”Œ ìˆ˜ëŠ” ì°¨ì› ìˆ˜ê°€ ì»¤ì§ì— ë”°ë¼ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ëŠ˜ì–´ë‚˜ê²Œë˜ëŠ” ë¬¸ì œê°€ ìˆë‹¤. ë”°ë¼ì„œ, ë‹¤ë¥¸ í•´ê²°ì±…ìœ¼ë¡œ ì°¨ì›ì„ ì €ì°¨ì› ê³µê°„ì— í¼ì¹˜ëŠ” íˆ¬ì˜(projection)ì´ë‚˜ ì°¨ì›ì„ ì¶•ì†Œí•˜ëŠ” ì£¼ì„±ë¶„ ë¶„ì„(PCA)ë“±ì´ ì´ìš©ëœë‹¤. ì°¨ì› ì¶•ì†Œë¥¼ ìœ„í•œ ë°©ë²•1. íˆ¬ì˜(projection)ê³ ì°¨ì› ê³µê°„ì— ìˆëŠ” í›ˆë ´ ìƒ˜í”Œì„ ì €ì°¨ì› ê³µê°„ìœ¼ë¡œ ê·¸ëŒ€ë¡œ ìˆ˜ì§ìœ¼ë¡œ íˆ¬ì˜í•˜ëŠ” ë°©ë²•ì´ë©°, ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ 3ì°¨ì› ê³µê°„ì— ìˆëŠ” ìƒ˜í”Œë“¤ì€ ì‚¬ì‹¤ 2ì°¨ì› ê³µê°„ì— ë†“ì•„ë„ ë°ì´í„°ë“¤ì˜ íŠ¹ì„±ì´ ë§ì´ ë­‰ê°œì§€ì§€ ì•Šê²Œ ëœë‹¤. í•˜ì§€ë§Œ, íˆ¬ì˜í•˜ëŠ” ê²ƒì´ ëª¨ë“  ìƒí™©ì— ìµœì ì¸ ê²ƒì€ ì•„ë‹ˆë‹¤.ìœ„ ê·¸ë¦¼ê³¼ ê°™ì´ ë°ì´í„°ê°€ ë§ë ¤ìˆì„ ê²½ìš° ë°ì´í„°ë¥¼ ê·¸ëŒ€ë¡œ íˆ¬ì˜í•˜ê²Œ ë˜ë©´ ì–´ë–»ê²Œ ë ê¹Œ? ìœ„ ë°ì´í„° ìƒ˜í”Œì„ ê·¸ëŒ€ë¡œ ìˆ˜ì§ìœ¼ë¡œ íˆ¬ì˜í•  ê²½ìš°, ì•„ë˜ ì™¼ìª½ê³¼ ê°™ì€ ê·¸ë¦¼ì´ ëœë‹¤. ê²€ì •, ë¹¨ê°•, ë…¸ë‘ ìƒ˜í”Œì´ ë­‰ê°œì ¸ë²„ë ¸ê¸° ë•Œë¬¸ì— 2ì°¨ì›ì—ì„œëŠ” í‘œí˜„ì„ ì˜ ë‚˜íƒ€ë‚´ì§€ë¥¼ ëª»í•˜ê³  ìˆë‹¤. í•˜ì§€ë§Œ ìš°ë¦¬ê°€ ì›í•˜ëŠ” ê²ƒì€ ë°”ë¡œ ì˜¤ë¥¸ìª½ê³¼ ê°™ì´ íŠ¹ì„±ì´ ë­‰ê°œì§€ì§€ ì•Šê²Œ í¼ì³ì§„ ê·¸ë¦¼ì¼ ê²ƒì´ë‹¤. ê·¸ë¦¬ê³  ì´ë ‡ê²Œ êµ¬ë¶€ë ¤ì ¸ ìˆëŠ” ë°ì´í„°ë¥¼ ë°˜ë“¯ì´ í´ê¸°ìœ„í•´ ì‚¬ìš©ë˜ëŠ” ê²ƒì´ ë°”ë¡œ ë§¤ë‹ˆí´ë“œ í•™ìŠµì´ë‹¤ 2. ë§¤ë‹ˆí´ë“œ í•™ìŠµ(manifold learning)ìœ„ì—ì„œ ë³´ì•˜ë˜ ìŠ¤ìœ„ìŠ¤ ë¡¤(swiss roll)ë°ì´í„°ëŠ” 2D ë§¤ë‹ˆí´ë“œì˜ í•œ ì˜ˆì˜€ë‹¤. í•œ ê°€ì§€ ì˜ˆë¥¼ ë” ë“¤ì–´ ì•„ë˜ì™€ ê°™ì€ ë°ì´í„°ê°€ ìˆë‹¤ê³  í•´ë³´ì. ìœ„ ë°ì´í„°ë“¤ ê°„ì˜ ê±°ë¦¬ë¥¼ ì§ì„ ìƒì˜ ê±°ë¦¬ë¡œ ë³´ì•˜ì„ ë•Œ Aì™€ Cê°€ ì„œë¡œ ê°€ê¹Œìš¸ê¹Œ, ì•„ë‹ˆë©´ Aì™€ Gê°€ ì„œë¡œ ê°€ê¹Œìš¸ê¹Œ?ìœ„ ê·¸ë¦¼ëŒ€ë¡œ ë³´ì•˜ì„ ë•ŒëŠ” Aì™€ Cë³´ë‹¤ëŠ” Aì™€ Gì‚¬ì´ì˜ ê±°ë¦¬ê°€ ë” ê°€ê¹Œì›Œ ë³´ì¸ë‹¤. í•˜ì§€ë§Œ ìœ„ ë°ì´í„°ê°€ ì‹¤ì€ ì•„ë˜ì˜ ê·¸ë¦¼ì„ êµ¬ë¶€ë ¤ ë†“ì€ ê·¸ë¦¼ì´ì—ˆë‹¤ë©´ ì–´ë–¨ê¹Œ? ì‹¤ì œë¡œëŠ” ì–´ë–¤ ì ì´ ë” ê°€ê¹Œìš´ê°€? ìœ„ì™€ ê°™ì´, ì €ì°¨ì›ì˜ ë°ì´í„°ê°€ ê³ ì°¨ì›ì˜ ê³µê°„ì—ì„œ íœ˜ì–´ì§€ê±°ë‚˜ ë’¤í‹€ë ¤ ìˆëŠ” ê²ƒì„ ë§¤ë‹ˆí´ë“œ(manifold)ë¼ê³  í•˜ë©°, ê³ ì°¨ì› ê³µê°„ë‚´ì—ì„œ ë’¤í‹€ë ¤ìˆëŠ” ë°ì´í„°ë¥¼ ê³§ê²Œ í´ ìœ í´ë¦¬ë””ì•ˆ ê±°ë¦¬(euclidean distance) ê³„ì‚°ì„ í†µí•´ ë°ì´í„°ë“¤ ê°„ì˜ ê±°ë¦¬ë¥¼ ì°¾ëŠ” í•™ìŠµì„ ë§¤ë‹ˆí´ë“œ í•™ìŠµ(manifold learning)ì´ë¼ê³  í•œë‹¤. 3. ì£¼ì„±ë¶„ ë¶„ì„(PCA)ì£¼ì„±ë¶„ ë¶„ì„(Principal Component Analysis)ì€ ë°ì´í„°ì˜ ì°¨ì›ì„ ì¶•ì†Œí•˜ê³ ì í•  ë–„ ê°€ì¥ ì¸ê¸° ìˆê²Œ ì‚¬ìš©ë˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì´ë‹¤. ì£¼ì„±ë¶„ ë¶„ì„ì´ë€ ë°ì´í„°ë¥¼ ê°€ì¥ ì˜ í‘œí˜„í•˜ëŠ” ì´ˆí‰ë©´ì„ ì°¾ì•„ ë¶„ì‚°ì„ ìµœëŒ€ë¡œ ë³´ì¡´í•˜ëŠ” ì¶•ì„ ì°¾ëŠ” ê²ƒì´ë‹¤. ì¦‰, ë°ì´í„°ë¥¼ ê°€ì¥ ì˜ í‘œí˜„í•˜ëŠ” nê°œì˜ êµ¬ê°„ì„ ì°¾ì•„ ê·¸ê²ƒì„ nê°œì˜ ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œí•˜ì—¬ í‘œí˜„í•˜ëŠ” ë°©ë²•ì´ë‹¤. ì£¼ì„±ë¶„ì„ ì°¾ëŠ” ê³¼ì • ë°ì´í„°ì— ê°€ì¥ ê°€ê¹Œìš´ ì´ˆí‰ë©´ì„ ì •ì˜í•œ í›„, ë°ì´í„°ë¥¼ ì´ í‰ë©´ì— íˆ¬ì˜ ì„ì˜ì˜ ì¶•ì„ ì„ íƒ í›„, ë°ì´í„°ì˜ ë¶„ì‚°ì„ ìµœëŒ€í•œ ë³´ì¡´í•˜ëŠ” ì¶•ì„ ì„ íƒ ì„ íƒí•œ ì¶•ì„ ê¸°ì¤€ìœ¼ë¡œ ì§êµí•˜ëŠ” ì¶•ì„ ì„ íƒ(ë‘ë²ˆì§¸ë¡œ ë¶„ì‚°ì„ ìµœëŒ€í•œ ë³´ì¡´í•˜ëŠ” ì¶•) ìœ„ ê³¼ì •ì„ ë°˜ë³µí•˜ë©° ì°¾ìœ¼ë ¤ëŠ” ì°¨ì› ìˆ˜ë§Œí¼ ìˆ˜í–‰ ìœ„ì˜ ê³¼ì •ì„ í†µí•´ ì°¾ì€ ië²ˆì§¸ ì¶•ì„ ì •ì˜í•˜ëŠ” ë‹¨ìœ„ ë²¡í„°ë¥¼ ië²ˆì§¸ ì£¼ì„±ë¶„(PC, principal component)ë¼ê³  í•˜ë©°, ì´ëŸ¬í•œ ì£¼ì„±ë¶„ì„ ì°¾ëŠ” ê³¼ì •ì€ íŠ¹ì´ê°’ ë¶„í•´(SVD, Singular Value Decomposition)ë¼ëŠ” í‘œì¤€ í–‰ë ¬ ë¶„í•´ ê¸°ìˆ ì„ í†µí•´ ì´ë£¨ì–´ ì§„ë‹¤. scikit-learnì—ì„œ PCA ì‚¬ìš©í•˜ê¸°ì‚¬ì´í‚·ëŸ°ì—ì„œ pcaë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” sklearnì˜ preprocessingëª¨ë“ˆì—ì„œ PCAëª¨ë¸ì„ ì´ìš©í•˜ë©´ ëœë‹¤.1234from sklearn.decomposition import PCApca = PCA(n_components=3)data_3d = pca.fit_transform(data) PCAëª¨ë¸ì˜ ì¤‘ìš” íŒŒë¼ë¯¸í„°ëŠ” n_componentsì¸ë° ì´ê²ƒì´ ë°”ë¡œ ì¶•ì†Œí•  ì°¨ì›ì˜ ìˆ˜ì´ë‹¤. ìœ„ì˜ ê³¼ì •ì€ 3ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œí•˜ê²Œ ëœë‹¤. breast cancer ë°ì´í„°ì…‹ì— PCAë¥¼ ì ìš©123456import pandas as pd cancer = pd.read_csv('breast_caner.csv')cancer.drop(['id','diagnosis'], axis=1, inplace=True) # ì¼ë‹¨ í•„ìš”ì—†ìœ¼ë¯€ë¡œ ì œì™¸print(cancer.shape)print(cancer.head()) ìœ„ ë°ì´í„°ëŠ” ì´ 30ê°œì˜ ì°¨ì›ì„ ê°€ì§„ ë°ì´í„°ì…‹ì´ë‹¤. ì´ë¥¼ 3ê°œì˜ ì°¨ì›ì„ ê°€ì§„ ë°ì´í„°ë¡œ ì°¨ì› ì¶•ì†Œë¥¼ í•´ë³´ë ¤ê³  í•œë‹¤. ì—¬ê¸°ì„œ ì•Œì•„ë‘ì–´ì•¼ í•  ê²ƒì€ ë³´í†µ ì°¨ì› ì¶•ì†Œë¥¼ í•˜ê¸° ì „ì—ëŠ” ë¨¼ì € ë°ì´í„°ë¥¼ ì •ê·œí™” í•œë‹¤ëŠ” ê²ƒì´ë‹¤. ë°ì´í„° ë²”ìœ„ë¥¼ ì •ê·œí™” í•¨ìœ¼ë¡œì¨ ë°ì´í„°ê°„ íŠ¹ì„± ë¹„êµë¥¼ ì‰½ê²Œí•˜ê¸° ìœ„í•´ì„œë‹¤. ë¨¼ì € Standard Scalerë¥¼ í†µí•´ ë°ì´í„°ë¥¼ ì •ê·œí™” í•œë‹¤.12345from sklearn.preprocessing import StandardScalerscaler = StandardScaler()cancer_scaled = pd.DataFrame(scaler.fit_transform(cancer), columns=cancer.columns)print(cancer_scaled.head()) ì´ì œ PCAë¥¼ í†µí•´ 3ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œ í›„ ê²°ê³¼ë¥¼ ì‚´í´ë³¸ë‹¤.12345from sklearn.decomposition import PCApca = PCA(n_components=3)cancer_pca = pca.fit_transform(cancer_scaled)print(cancer_pca[:1] ë°ì´í„°ê°€ numpy array í˜•íƒœë¡œ 3ê°œì˜ ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œê°€ ëœ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.","categories":[],"tags":[{"name":"pca","slug":"pca","permalink":"https://jaehyeongan.github.io/tags/pca/"},{"name":"machinelearning","slug":"machinelearning","permalink":"https://jaehyeongan.github.io/tags/machinelearning/"},{"name":"dimensionality","slug":"dimensionality","permalink":"https://jaehyeongan.github.io/tags/dimensionality/"},{"name":"manifold","slug":"manifold","permalink":"https://jaehyeongan.github.io/tags/manifold/"}]},{"title":"í…ì„œí”Œë¡œìš°(Tensorflow 2.0) GPUë²„ì „ ì‚¬ìš©í•˜ê¸°","slug":"tensorflow GPUë²„ì „ ì‚¬ìš©í•˜ê¸°","date":"2019-05-01T09:29:10.000Z","updated":"2020-12-10T14:54:57.000Z","comments":true,"path":"2019/05/01/tensorflow GPUë²„ì „ ì‚¬ìš©í•˜ê¸°/","link":"","permalink":"https://jaehyeongan.github.io/2019/05/01/tensorflow GPUë²„ì „ ì‚¬ìš©í•˜ê¸°/","excerpt":"","text":"Introë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ í•™ìŠµí• ë•ŒëŠ” í¬ê²Œ ì™€ë‹¿ì§€ ì•Šì§€ë§Œ ë³µì¡í•œ ë”¥ëŸ¬ë‹ ì—°ì‚°ì„ í•  ë•Œ í¬ê²Œ ëŠê»´ì§€ëŠ” ê²ƒì´ ë°”ë¡œ GPUì˜ ìœ ë¬´ì´ë‹¤. ë”¥ëŸ¬ë‹ê³¼ ê°™ì€ ë³µì¡í•œ matrix ì—°ì‚°ì„ í•˜ê¸° ìœ„í•´ CPUë¡œ ëª¨ë¸ì„ ëŒë ¸ë‹¤ê°€ëŠ” ì»´í“¨í„°ê°€ ìš´ëª…ì„ ë‹¤ í•  ìˆ˜ ìˆë‹¤.ì´ì „ì— í…ìŠ¤íŠ¸ ì²˜ë¦¬ ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ CPUì™€ GPUë¡œ ëŒë ¸ì„ ë•Œ ì–¼ë§ˆë‚˜ ì°¨ì´ë‚˜ëŠ”ì§€ ë³´ë ¤ê³  ì‹¤í—˜ì„ í–ˆì—ˆëŠ”ë°, GPUì˜ ê²½ìš° 3ì‹œê°„ ì •ë„ë§Œì— í•™ìŠµì´ ëë‚œ ë°˜ë©´ CPUì˜ ê²½ìš° ê±°ì˜ í•œë‚˜ì ˆì„ ëŒì•„ê°€ê³ ë„ ê²°ê³¼ê°€ ë‚˜ì˜¤ì§€ ì•Šì•„ ì¤‘ê°„ì— ëŠì€ ì ì´ ìˆì—ˆë‹¤. ë³¸ì¸ ì»´í“¨í„°ì— ì™¸ì¥ ê·¸ë˜í”½ì´ ì—†ë‹¤ë©´ í•  ìˆ˜ ì—†ì§€ë§Œ GPUê°€ ê°–ì¶°ì ¸ ìˆì„ ê²½ìš° ì´ë¥¼ ì ê·¹ í™œìš©í•˜ëŠ” ê²ƒì´ ì •ì‹ ê±´ê°•ì— ì¢‹ì„ ê²ƒ ê°™ë‹¤.í•˜ì§€ë§Œ, GPUë„ ë‹¤ ê°™ì€ GPUê°€ ì•„ë‹ˆë‹¤.í˜„ì¬ tensorflowì—ì„œ ì§€ì›í•˜ëŠ” GPUëŠ” Nvidiaë¥¼ ê¸°ë³¸ìœ¼ë¡œ í•˜ë©° AMDì˜ ê²½ìš° ì•„ì§ ì´ìš©í•˜ê¸°ì— ë§ì´ ë¶ˆí¸í•˜ë‹¤. tensorflow-gpu ì„¤ì¹˜1. CUDA ì„¤ì¹˜ìš°ì„  CUDAë¥¼ ì„¤ì¹˜í•´ì•¼ í•œë‹¤. í˜„ì¬ CUDAì˜ ê²½ìš° ìµœì‹  ë²„ì „ì´ 10.2ì´ì§€ë§Œ, í™•ì¸ ê²°ê³¼ ì•„ì§ê¹Œì§€ëŠ” ê³µì‹ì ìœ¼ë¡œ tensorflowê°€ CUDA 10.0ë²„ì „ê¹Œì§€ë§Œ ì§€ì›í•œë‹¤. CUDA Toolkit Arcive(https://developer.nvidia.com/cuda-toolkit-archive)ë¡œ ì´ë™í•˜ì—¬ ì•„ë˜ í™”ë©´ê³¼ ê°™ì´ CUDA Toolkit 10.0ë²„ì „ì„ í´ë¦­í•œë‹¤. í´ë¦­ í›„ ì•„ë˜ì™€ ê°™ì´ ìì‹ ì˜ ìš´ì˜ì²´ì œ ë§ëŠ” ê²ƒì„ ì„ íƒí•œ í›„ ë‹¤ìš´ë¡œë“œë¥¼ ì‹¤ì‹œí•˜ê³  ë‹¤ìš´ë¡œë“œ ëœ ì„¤ì¹˜íŒŒì¼ì„ ë‹¤ë¥¸ ì¡°ê±´ ë³€ê²½ì—†ì´ ê·¸ëŒ€ë¡œ ì„¤ì¹˜í•˜ë©´ ëœë‹¤. 2. cuDNN ë‹¤ìš´ë¡œë“œCUDA ì„¤ì¹˜ë¥¼ ì™„ë£Œí•˜ì˜€ë‹¤ë©´ ì´ì œ cuDNN(https://developer.nvidia.com/rdp/cudnn-download)ì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ CUDA ë””ë ‰í† ë¦¬ì— ë„£ì–´ì¤˜ì•¼ í•œë‹¤.cuDNNì„ ì„¤ì¹˜í•˜ê¸° ìœ„í•´ì„œëŠ” nvidiaì— ë¡œê·¸ì¸ì„ í•´ì•¼í•˜ë¯€ë¡œ ê°€ì…ì´ ì•ˆë˜ì–´ìˆë‹¤ë©´ ê°€ì…ì„ í•œ í›„ ì ‘ì†í•˜ë©´ ëœë‹¤. ì£¼ì˜í•  ì ì€ ìœ„ì—ì„œ ì„¤ì¹˜í•œ CUDAë²„ì „ì— í˜¸í™˜ë˜ëŠ” cuDNNì„ ë‹¤ìš´ë¡œë“œ í•´ì•¼ í•œë‹¤ëŠ” ê²ƒì´ë‹¤. ìœ„ì—ì„œ CUDA 10.0ë²„ì „ì„ ì„¤ì¹˜í•´ì£¼ì—ˆê¸° ë•Œë¬¸ì— cuDNNë„ CUDA 10.0ì— í˜¸í™˜ë˜ëŠ” ë²„ì „(for CUDA 10.0)ìœ¼ë¡œ ë‹¤ìš´ë°›ëŠ”ë‹¤. ìœ„ íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œ í•˜ë©´ cudnn-10.0-windows10-x64-v7.5.1.10 ë¼ëŠ” ì••ì¶•íŒŒì¼ì´ ë‹¤ìš´ë¡œë“œ ë˜ëŠ”ë°, ì••ì¶•íŒŒì¼ì„ í’€ê²Œ ë˜ë©´ ê·¸ ì•ˆì— ì•„ë˜ì™€ ê°™ì€ íŒŒì¼ì´ ë“¤ì–´ìˆë‹¤. 3. cuDNNíŒŒì¼ CUDA í´ë”ë¡œ ë³µì‚¬ì´ì œë¶€í„°ê°€ ì¤‘ìš”í•œë°,ë°©ê¸ˆ ì „ ì••ì¶•í•´ì œ í•œ í´ë”ì˜ íŒŒì¼ì„ ëª¨ë‘ ë³µì‚¬í•˜ì—¬ ê·¸ëŒ€ë¡œ ì²˜ìŒ ì„¤ì¹˜í•œ CUDA í´ë”ë¡œ ì „ë¶€ ë³µì‚¬í•´ì£¼ì–´ì•¼ í•œë‹¤.ìš°ì„  ì••ì¶•í•´ì œ í•œ íŒŒì¼ë“¤ì„ ì „ë¶€ ë³µì‚¬í•œ í›„, C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0 ì´ ê²½ë¡œë¡œ ê°€ì„œ ë³µì‚¬í•œ íŒŒì¼ì„ ê·¸ëŒ€ë¡œ ë¶™ì—¬ ë„£ê¸° í•´ì¤€ë‹¤.(ì•ˆì— ë™ì¼í•œ íŒŒì¼ì´ ìˆëŠ”ë° ê·¸ëƒ¥ ë®ì–´ì”Œì›Œì£¼ëŠ” ê²ƒì´ë‹¤.) (ìœ„ ê²½ë¡œì— ê·¸ëŒ€ë¡œ ë³µì‚¬í•œ íŒŒì¼ì„ ë®ì–´ì”Œìš´ë‹¤.) 4. í™˜ê²½ë³€ìˆ˜ ì§€ì •ë³´í†µ ë‹¤ë¥¸ ì„¤ì •ì„ ê±´ë“œë¦¬ì§€ ì•Šê³  ì§„í–‰í•˜ì˜€ì„ ê²½ìš°, í™˜ê²½ë³€ìˆ˜ì— ì•„ë˜ì™€ ê°™ì€ CUDA ê²½ë¡œê°€ ë“¤ì–´ìˆì„ ê²ƒì´ë‹¤. ì—†ë‹¤ë©´ ì•„ë˜ì™€ ê°™ì€ ê²½ë¡œë¥¼ ê·¸ëŒ€ë¡œ í™˜ê²½ë³€ìˆ˜ì— ì§€ì •í•´ì¤€ë‹¤. 5. tensorflow-gpu ë²„ì „ ì„¤ì¹˜ì´í›„ Anacoda prompt í˜¹ì€ CMD ì°½ì„ ì—´ì–´ ì•„ë˜ì™€ ê°™ì€ ëª…ë ¹ì–´ë¡œ tensorflow-gpuë²„ì „ì„ ì„¤ì¹˜í•œë‹¤. &gt; pip install tensorflow-gpuí˜¹ì€&gt; conda install tensorflow-gpu (ì´ë¯¸ ì„¤ì¹˜ë˜ì–´ ìˆì–´ì„œ ìœ„ì™€ ê°™ì´ ë‚˜ì˜´.) 6. tensorflow ì‹¤í–‰ ë° í™•ì¸promtì°½ì„ ì—´ì–´ ì•„ë˜ì™€ ê°™ì´ tensorflowë¥¼ importí•˜ì˜€ì„ ë•Œ errorê°€ ë‚˜ì§€ ì•ŠëŠ”ë‹¤ë©´ ìš°ì„  tensorflow ì„¤ì¹˜ì— ì„±ê³µí•œ ê²ƒì´ë‹¤.ì„¤ì¹˜ ëœ tensorflow ë²„ì „ì„ í™•ì¸í•˜ê³  ì‹¶ì„ ë•ŒëŠ” tf._version_ ì„ í†µí•´ í™•ì¸í•  ìˆ˜ ìˆë‹¤.12import tensorflow as tf tf.__version__ tensorflowê°€ GPUë²„ì „ìœ¼ë¡œ ì˜ ì„¤ì¹˜ë˜ì—ˆê³ , ë‚˜ì˜ GPUë¥¼ ì˜ ì¸ì‹í•˜ê³  ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ì™€ ê°™ì€ ì½”ë“œë¥¼ í†µí•´ í™•ì¸í•  ìˆ˜ ìˆë‹¤. tensorflowê°€ ì¸ì‹í•˜ëŠ” ë¡œì»¬ device ëª©ë¡ì„ ë³´ì—¬ì£¼ê²Œ ëœë‹¤.12from tensorflow.python.client import device_libdevice_lib.list_local_devices() ë‚´ ì»´í“¨í„°ì˜ GPUì˜ ê²½ìš° GeForce GTX 1050 with MAX-Qì¸ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.","categories":[],"tags":[{"name":"keras","slug":"keras","permalink":"https://jaehyeongan.github.io/tags/keras/"},{"name":"tensorflow","slug":"tensorflow","permalink":"https://jaehyeongan.github.io/tags/tensorflow/"},{"name":"python","slug":"python","permalink":"https://jaehyeongan.github.io/tags/python/"},{"name":"nvidia","slug":"nvidia","permalink":"https://jaehyeongan.github.io/tags/nvidia/"},{"name":"cuda","slug":"cuda","permalink":"https://jaehyeongan.github.io/tags/cuda/"},{"name":"cudnn","slug":"cudnn","permalink":"https://jaehyeongan.github.io/tags/cudnn/"}]},{"title":"ë¡œì§€ìŠ¤í‹± íšŒê·€(logistic regression)","slug":"logistic-regression","date":"2019-04-27T05:54:01.000Z","updated":"2020-12-10T14:53:15.000Z","comments":true,"path":"2019/04/27/logistic-regression/","link":"","permalink":"https://jaehyeongan.github.io/2019/04/27/logistic-regression/","excerpt":"","text":"Introì´ë¦„ì€ regressionì´ë‚˜ ë¶„ë¥˜ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ì£¼ë¡œ ì‚¬ìš©ë˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì´ ìˆë‹¤. ë°”ë¡œ ë¡œì§€ìŠ¤í‹± íšŒê·€(Logistic Regression)ì´ë‹¤. ë¡œì§€ìŠ¤í‹± íšŒê·€ëŠ” ìƒ˜í”Œì´ íŠ¹ì • í´ë˜ìŠ¤ì— ì†í•  í™•ë¥ ì„ ì¶”ì •í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë™ì‘í•˜ëŠ” ì´ì§„ ë¶„ë¥˜(Binary Classification) ëª¨ë¸ì´ë‹¤. Logistic Regression(ë¡œì§€ìŠ¤í‹± íšŒê·€)ë¡œì§€ìŠ¤í‹± íšŒê·€(logistic regression)ëŠ” ìƒ˜í”Œì´ íŠ¹ì • í´ë˜ìŠ¤ì— ì†í•  í™•ë¥ ì„ ì¶”ì •í•˜ëŠ” ë° ë„ë¦¬ ì‚¬ìš©ëœë‹¤.(ì˜ˆë¥¼ ë“¤ì–´ í•´ë‹¹ ì´ë©”ì¼ì´ spamì¼ í™•ë¥ ê³¼ spamì´ ì•„ë‹ í™•ë¥ )ì¶”ì • í™•ë¥ ì´ 50% ì´ìƒì´ë©´ ëª¨ë¸ì€ ê·¸ ìƒ˜í”Œì„ í•´ë‹¹ í´ë˜ìŠ¤(label: 1)ì— ì†í•œë‹¤ê³  ì˜ˆì¸¡í•˜ê³  50%ì´í•˜ì´ë©´ í´ë˜ìŠ¤ì— ì†í•˜ì§€ ì•ŠëŠ”ë‹¤ê³ (label: 0) ì˜ˆì¸¡í•œë‹¤. í™•ë¥  ì¶”ì •ê¸°ì¡´ Regresionìœ¼ë¡œ ë¶„ë¥˜ë¬¸ì œë¥¼ í•´ê²°í•˜ê³ ì í•  ê²½ìš° ê°€ì¥ í° ë¬¸ì œì ì€ ë°”ë¡œ 1ì´ìƒ ë˜ëŠ” 0ì´í•˜ì˜ ìˆ˜ë¡œ ë‚˜ì˜¤ëŠ” ì˜ˆì¸¡ê°’ì„ í•´ì„í•˜ëŠ” ì¼ì´ë‹¤. ë”°ë¼ì„œ 0ê³¼ 1ì‚¬ì´ì˜ í™•ë¥ ë¡œ í‘œí˜„í•˜ì—¬ 0.5ë³´ë‹¤ í¬ë©´ positive, 0.5ë³´ë‹¤ ì‘ìœ¼ë©´ negativeë¡œ ì§ê´€ì ìœ¼ë¡œ í‘œí˜„í•˜ê³ ì í•˜ëŠ” ê²ƒì´ í™•ë¥  ì¶”ì •ì´ë‹¤.ì–´ë–¤ ì‚¬ê±´ì´ ì¼ì–´ë‚  í™•ë¥ ì€ ì•„ë˜ì™€ ê°™ì´ í‘œí˜„ëœë‹¤. ì„ í˜• íšŒê·€ ëª¨ë¸ê³¼ ê°™ì´ ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ì€ ì…ë ¥ íŠ¹ì„±ì˜ ê°€ì¤‘ì¹˜ í•©ì„ ê³„ì‚°í•˜ê³  í¸í–¥ì„ ë”í•œë‹¤. ëŒ€ì‹  ì„ í˜• íšŒê·€ì²˜ëŸ¼ ë°”ë¡œ ê²°ê³¼ë¥¼ ì¶œë ¥í•˜ì§€ ì•Šê³  ê²°ê´ê°’ì˜ ë¡œì§€ìŠ¤í‹±(logistic)ì„ ì•„ë˜ì˜ ì‹ì„ í†µí•´ ì¶œë ¥í•œë‹¤. Logit Functionë¡œì§“ í•¨ìˆ˜ëŠ” Xì˜ ê°’ì´ ì£¼ì–´ì¡Œì„ ë•Œ Yì˜ í™•ë¥ ì„ ì´ìš©í•œ log oddsì´ë©° ì•„ë˜ì™€ ê°™ì´ ë‚˜íƒ€ë‚¸ë‹¤. Sigmoid(=logistic) Fuctionë¡œì§€ìŠ¤í‹±(ë˜ëŠ” ë¡œì§“)ì€ 0ê³¼ 1ì‚¬ì´ì˜ ê°’ì„ ì¶œë ¥í•˜ëŠ” ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜(Sigmoid Function)ì´ë‹¤.(ì¦‰, Sì í˜•íƒœ)ë¡œì§€ìŠ¤í‹± í•¨ìˆ˜ëŠ” logit í•¨ìˆ˜ì˜ ì—­í•¨ìˆ˜ í˜•íƒœë¡œ zì— ê´€í™˜ í™•ë¥ ì„ ì‚°ì¶œí•˜ë©° ì•„ë˜ì™€ ê°™ì€ ì‹ìœ¼ë¡œ í‘œí˜„ëœë‹¤. ìœ„ ê·¸ë˜í”„ì™€ ê°™ì´ sigmoid functionì€ Sì í˜•íƒœë¡œ ì´ë£¨ì–´ì ¸ ìˆìœ¼ë©° ê° í´ë˜ìŠ¤ì— ì†í•  í™•ë¥ ì„ 0.5ë¥¼ ê¸°ì¤€ìœ¼ë¡œ 0.5ì´ìƒì´ë©´ ì–‘ì„± í´ë˜ìŠ¤(1)ë¡œ ì˜ˆì¸¡í•˜ê³ , 0.5ì´í•˜ì´ë©´ ìŒì„± í´ë˜ìŠ¤(0)ìœ¼ë¡œ ì˜ˆì¸¡í•œë‹¤. ë¡œì§€ìŠ¤í‹± íšŒê·€ì˜ ë¹„ìš© í•¨ìˆ˜(Cost Function)ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ì˜ í›ˆë ¨ ëª©ì ì€ ì–‘ì„± ìƒ˜í”Œ(y=1)ì— ëŒ€í•´ì„œëŠ” ë†’ì€ í™•ë¥ ì„ ì¶”ì •í•˜ê³  ìŒìƒ ìƒ˜í”Œ(y=0)ì— ëŒ€í•´ì„œëŠ” ë‚®ì€ í™•ë¥ ì„ ì¶”ì •í•˜ëŠ” ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„° ë²¡í„° thetaë¥¼ ì°¾ëŠ” ê²ƒì´ë‹¤. ì „ì²´ í›ˆë ¨ ì„¸íŠ¸ì— ëŒ€í•œ ë¹„ìš© í•¨ìˆ˜ëŠ” ëª¨ë“  í›ˆë ¨ ìƒ˜í”Œì˜ ë¹„ìš©ì„ í‰ê· í•œ ê²ƒì´ë‹¤. ì´ë¥¼ ë¡œê·¸ ì†ì‹¤(log loss)ë¼ê³  ë¶€ë¥´ë©° ì•„ë˜ì™€ ê°™ì€ ì‹ìœ¼ë¡œ í‘œí˜„ëœë‹¤. ë¡œì§€ìŠ¤í‹± íšŒê·€ ì ìš© breast cancer ë°ì´í„°ì…‹ì— ë¡œì§€ìŠ¤í‹± íšŒê·€ë¥¼ ì ìš©í•´ë³¸ë‹¤.ìš°ì„  breast cancer ë°ì´í„° ì…‹ì€ ì´ 30ê°œì˜ featuresì™€ ì•” ì—¬ë¶€ì— í•´ë‹¹í•˜ëŠ” 1ê°œì˜ y labelê°’ì„ ê°€ì§€ê³  ìˆë‹¤.scikit-learnì˜ ë°ì´í„° ì…‹ì„ í™œìš©í•˜ì˜€ëŠ”ë° ì´ ë°ì´í„°ì˜ ê²½ìš° ì´ë¯¸ z-score ì •ê·œí™”ê°€ ë˜ì–´ìˆëŠ” ë°ì´í„° ì…‹ì´ë‹¤.123456789from sklearn.datasets import load_breast_cancerfrom sklearn.model_selection import train_test_splitbreast_cancer = load_breast_cancer()X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, stratify=cancer.target, random_state=42)print(X_train.shape) # (426, 30)print(y_train.shape) # (426, ) í›ˆë ¨ ì…‹ê³¼ í…ŒìŠ¤íŠ¸ ì…‹ì„ ë‚˜ëˆˆ ë°ì´í„°ì— scikit-learnì—ì„œ ì œê³µí•˜ëŠ” ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ì„ ì ìš©í•œë‹¤.12345from sklearn.linear_model import LogisticRegressionlogreg = LogisticRegression(C=0.01).fit(X_train, y_train)print(\"train set score : \", logreg.score(X_train, y_train)) # 0.934print(\"test set score : \", logreg.score(X_test, y_test)) # 0.930 ê²°ê³¼ë¥¼ ë³´ì•˜ì„ ë•Œ ëª¨ë¸ì´ train setê³¼ test setì— ëª¨ë‘ 93%ì˜ ì •í™•ë„ë¡œ ì•”ì„ ë¶„ë¥˜í•˜ì˜€ëŠ”ë°, ì‹¤ì œë¡œ ì˜ ë¶„ë¥˜ë˜ì—ˆëŠ”ì§€ test data ì¤‘ ì• 10ê°œë§Œ ì‹¤ì œ ê°’ê³¼ ë¹„êµí•´ë³´ì•˜ë‹¤.1234pred_10 = logreg.predict(X_test)print('ì‹¤ì œ ê°’: ',y_test[:10]) # ì‹¤ì œ ê°’: [1 0 1 1 0 0 0 0 0 1]print('ì˜ˆì¸¡ ê°’: ',pred_10[:10]) # ì˜ˆì¸¡ ê°’: [1 0 1 1 1 0 1 0 0 1] ë‘ ê°œì˜ ì˜ˆì¸¡ê°’ë§Œ ë¹¼ê³ (ì•” ì•„ë‹˜ì„ ì•”ì´ë¼ê³  ì˜ˆì¸¡) ì‹¤ì œ ê°’ê³¼ ê°™ì€ ì˜ˆì¸¡ê²°ê³¼ë¥¼ ë³´ì—¬ì¤€ë‹¤. Softmax Regression(ì†Œí”„íŠ¸ë§¥ìŠ¤ íšŒê·€)ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ì€ ì—¬ëŸ¬ ê°œì˜ ì´ì§„ ë¶„ë¥˜ê¸°ë¥¼ í›ˆë ¨ì‹œì¼œ ì—°ê²°í•˜ì§€ ì•Šê³ ë„ ì§ì ‘ ë‹¤ì¤‘ í´ë˜ìŠ¤ë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ í•  ìˆ˜ ìˆë‹¤.ì´ë¥¼ ì†Œí”„íŠ¸ë§¥ìŠ¤ íšŒê·€(Softmax Regression) ë˜ëŠ” ë‹¤í•­ ë¡œì§€ìŠ¤í‹± íšŒê·€(Multinomial Logistic Regression)ë¼ê³  í•œë‹¤. â€” ìƒ˜í”Œ xê°€ ì£¼ì–´ì§€ë©´ ì†Œí”„íŠ¸ë§¥ìŠ¤ íšŒê·€ ëª¨ë¸ì´ ê° í´ë˜ìŠ¤ kì— ëŒ€í•œ ì ìˆ˜ë¥¼ ê³„ì‚° â€” ê·¸ ì ìˆ˜ì— ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜(softmax function)ì„ ì ìš©í•˜ì—¬ ê° í´ë˜ìŠ¤ì˜ í™•ë¥ ì„ ì¶”ì • â€” ë¡œì§€ìŠ¤í‹± íšŒê·€ì™€ ë§ˆì°¬ê°€ì§€ë¡œ ì¶”ì • í™•ë¥ ì´ ê°€ì¥ í° í´ë˜ìŠ¤ë¥¼ ì„ íƒ â€” ê° í´ë˜ìŠ¤ê°€ ë  í™•ë¥  ê°’ì„ ëª¨ë‘ ë”í•˜ë©´ 1ì´ ë¨ ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜â€” këŠ” í´ë˜ìŠ¤ì˜ ìˆ˜ì´ë©°, s(x)ëŠ” ìƒ˜í”Œ xì— ëŒ€í•œ ê° í´ë˜ìŠ¤ì˜ ì ìˆ˜ë¥¼ ë‹´ê³  ìˆëŠ” ë²¡í„° ì†Œí”„íŠ¸ë§¥ìŠ¤ íšŒê·€ ë¶„ë¥˜ê¸°ì˜ ì˜ˆì¸¡ í¬ë¡œìŠ¤ ì—”íŠ¸ë¡œí”¼(cross-entropy) ë¹„ìš© í•¨ìˆ˜ â€” í¬ë¡œìŠ¤ ì—”íŠ¸ë¡œí”¼ëŠ” ì¶”ì •ëœ í´ë˜ìŠ¤ì˜ í™•ë¥ ì´ íƒ€ê¹ƒ í´ë˜ìŠ¤ì— ì–¼ë§ˆë‚˜ ì˜ ë§ëŠ”ì§€ ì¸¡ì •í•˜ëŠ” ìš©ë„ë¡œ ì‚¬ìš© Softmax Regression ì ìš©scikit-learnì˜ LogisticRegression() í´ë˜ìŠ¤ì— multi_class ë§¤ê°œë³€ìˆ˜ë¥¼ â€œmultinomialâ€ë¡œ ë°”ê¾¸ë©´ ì†Œí”„íŠ¸ë§¥ìŠ¤ íšŒê·€ë¥¼ ì‚¬ìš© â€” ì†Œí”„íŠ¸ë§¥ìŠ¤ íšŒê·€ ì‚¬ìš© ì‹œ solver ë§¤ê°œë³€ìˆ˜ë¥¼ â€œlbfgsâ€ ì§€ì • â€” Cë¥¼ ì‚¬ìš©í•˜ì—¬ l2ê·œì œ ì ìš© 1234from sklearn.linear_model import LogisticRegressionsoftmax_reg = LogisticRegression(multi_class='multinomial', solver='lbfgs', C=10)spftmax_reg.fit(X_train, y_train)","categories":[],"tags":[{"name":"machinelearning","slug":"machinelearning","permalink":"https://jaehyeongan.github.io/tags/machinelearning/"},{"name":"classification","slug":"classification","permalink":"https://jaehyeongan.github.io/tags/classification/"},{"name":"regression","slug":"regression","permalink":"https://jaehyeongan.github.io/tags/regression/"},{"name":"python","slug":"python","permalink":"https://jaehyeongan.github.io/tags/python/"},{"name":"logistic","slug":"logistic","permalink":"https://jaehyeongan.github.io/tags/logistic/"},{"name":"scikitlearn","slug":"scikitlearn","permalink":"https://jaehyeongan.github.io/tags/scikitlearn/"}]},{"title":"ì„ í˜• ëª¨ë¸(Linear Model)","slug":"Linear-Regression","date":"2019-04-25T06:27:13.000Z","updated":"2020-12-10T14:52:45.000Z","comments":true,"path":"2019/04/25/Linear-Regression/","link":"","permalink":"https://jaehyeongan.github.io/2019/04/25/Linear-Regression/","excerpt":"","text":"Introë¨¸ì‹ ëŸ¬ë‹ì„ ì›ë¦¬ë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ ê°€ì¥ ë¨¼ì € ë°°ìš°ê²Œ ë˜ëŠ” ì„ í˜• ëª¨ë¸(linear models)ì— ëŒ€í•œ ê¸€ì´ë‹¤. ì„ í˜• ëª¨ë¸ì€ 100ì—¬ ë…„ ì „ê²Œ ê°œë°œë˜ì—ˆê³ , ì§€ë‚œ ëª‡ì‹­ ë…„ ë™ì•ˆ í­ë„“ê²Œ ì—°êµ¬ë˜ê³  í˜„ì¬ë„ ë„ë¦¬ ì“°ì´ê³  ìˆë‹¤. ê¸°ë³¸ì ìœ¼ë¡œ ì„ í˜• ëª¨ë¸ì€ ì…ë ¥ íŠ¹ì„±ì— ëŒ€í•œ ì„ í˜• í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•œë‹¤. Linear Regression(ì„ í˜• íšŒê·€)íšŒê·€ì˜ ê²½ìš° ì„ í˜• ëª¨ë¸ì„ ìœ„í•œ ì¼ë°˜í™”ëœ ì˜ˆì¸¡ í•¨ìˆ˜ëŠ” ì•„ë˜ì™€ ê°™ë‹¤. ìœ„ ì‹ì—ì„œ x[0]ë¶€í„° x[n]ê¹Œì§€ëŠ” í•˜ë‚˜ì˜ ë°ì´í„° í¬ì¸íŠ¸ì— ëŒ€í•œ íŠ¹ì„±ì„ ë‚˜íƒ€ë‚´ë©°(íŠ¹ì„±ì˜ ê°œìˆ˜ëŠ” n+1), wì™€ bëŠ” ëª¨ë¸ì´ í•™ìŠµí•  íŒŒë¼ë¯¸í„°ì´ë‹¤.ê·¸ë¦¬ê³  y^ì€ ëª¨ë¸ì´ ë§Œë“¤ì–´ë‚¸ ì˜ˆì¸¡ê°’ì´ë‹¤.ìœ„ ì‹ì€ íŠ¹ì„±ì´ í•˜ë‚˜ì¸ ë°ì´í„° ì…‹ì´ë¼ë©´ ì•„ë˜ì™€ ê°™ì´ 1ì°¨ ë°©ì •ì‹ìœ¼ë¡œ ë‹¨ìˆœí•˜ê²Œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤. w[0]ëŠ” ê¸°ìš¸ê¸°ì´ê³ , bëŠ” yì¶•ê³¼ ë§Œë‚˜ëŠ” ì ˆí¸(ë˜ëŠ” í¸í–¥)ì´ë‹¤. íŠ¹ì„±ì´ ë§ì•„ì§€ë©´ wëŠ” ê° íŠ¹ì„±ì— í•´ë‹¹í•˜ëŠ” ê¸°ìš¸ê¸°ë¥¼ ëª¨ë‘ ê°€ì§„ë‹¤. [ì„ í˜• íšŒê·€] ì„ í˜• íšŒê·€ëŠ” ê°€ì¥ ê°„ë‹¨í•˜ê³  ì˜¤ë˜ëœ íšŒê·€ìš© ì„ í˜• ì•Œê³ ë¦¬ì¦˜ì´ë‹¤. ì„ í˜• íšŒê·€ëŠ” ì˜ˆì¸¡ ê°’ y^ê³¼ ì‹¤ì œ ê°’ y ì‚¬ì´ì˜ í‰ê· ì œê³±ì˜¤ì°¨(mean squared error)ë¥¼ ìµœì†Œí™” í•˜ëŠ” íŒŒë¼ë¯¸í„° wì™€ bë¥¼ ì°¾ëŠ”ë‹¤. í‰ê· ì œê³± ì˜¤ì°¨ëŠ” ì˜ˆì¸¡ê°’(y^)ê³¼ ì‹¤ì œê°’(y)ì˜ ì°¨ì´ë¥¼ ì œê³±í•˜ì—¬ ë”í•œ í›„ì— ìƒ˜í”Œ ê°œìˆ˜ë¡œ ë‚˜ëˆˆ ê°’ì´ë‹¤. ì•„ë˜ëŠ” scikit-learnì˜ LinearRegressionì„ í†µí•´ boston house priceë¥¼ í†µí•œ ì§‘ ê°’ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ëŠ” ì½”ë“œì´ë‹¤. 12345678910111213141516import numpy as npfrom sklearn.datasets import load_bostonfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import mean_squared_error# datasetX, y = load_boston(True)X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)####### Linear Regression #######lin_reg = LinearRegression().fit(X_train, y_train)train_pred = lin_reg.predict(X_train)test_pred = lin_reg.predict(X_test)print('MSE of train set: ', mean_squared_error(y_train, train_pred)) # 19.640print('MSE of test set: ', mean_squared_error(y_test, test_pred)) # 29.782 training setê³¼ test setì— ëŒ€í•œ MSEê°€ ê°ê° 19.6, 29.7ë¡œ ë‚˜íƒ€ë‚¬ë‹¤. ì´ëŠ” ëª¨ë¸ì´ training setì— ê³¼ëŒ€ì í•©(Overfitting)ë˜ì—ˆë‹¤ëŠ” ì´ì•¼ê¸°ë‹¤. í•˜ì§€ë§Œ ì„ í˜•íšŒê·€ì—ì„œëŠ” ì´ëŸ° ê³¼ëŒ€ì í•©ì„ ë°©ì§€í•  ê·œì œ(regularization)ë°©ì•ˆì´ ì—†ë‹¤. ë•Œë¬¸ì— ê·œì œ ë°©ì•ˆì´ í¬í•¨ë˜ì–´ ìˆëŠ” ì•Œê³ ë¦¬ì¦˜(Ridge, Lasso, ElasticNet ë“±)ì„ ì´ìš©í•˜ëŠ” ê²ƒì´ íš¨ìœ¨ì ì¼ ìˆ˜ ìˆë‹¤. ì´ëŠ” ì¢€ ë” ì•„ë˜ì—ì„œ ì‚´í´ë³¼ ê²ƒì´ë‹¤. Polynomial Regression(ë‹¤í•­ íšŒê·€)ê°€ì§€ê³  ìˆëŠ” ë°ì´í„°ê°€ ë‹¨ìˆœí•œ ì§ì„ ë³´ë‹¤ ë³µì¡í•œ í˜•íƒœë¼ë©´ ì–´ë–»ê²Œ ì„ í˜•íšŒê·€ë¥¼ ì ìš©í•´ì•¼ í• ê¹Œ? ì‹ ê¸°í•˜ê²Œë„ ë¹„ì„ í˜•(Non-lieaner) ë°ì´í„°ë¥¼ í•™ìŠµí•˜ëŠ” ë° ì„ í˜• ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. ë°”ë¡œ ê° íŠ¹ì„±ì˜ ê±°ë“­ì œê³±ì„ ìƒˆë¡œìš´ íŠ¹ì„±ìœ¼ë¡œ ì¶”ê°€í•˜ê³ , ì´ í™•ì¥ëœ íŠ¹ì„±ì„ í¬í•¨í•œ ë°ì´í„°ì…‹ì— ì„ í˜• ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¤ëŠ” ê²ƒì´ë‹¤. ì´ëŸ° ê¸°ë²•ì„ ë‹¤í•­ íšŒê·€(Polynomial Regression)ì´ë¼ê³  í•œë‹¤. ë§Œì•½ ì•„ë˜ì™€ ê°™ì´ ì„ì˜ë¡œ ë§Œë“  2ì°¨ ë°©ì •ì‹ì˜ ë¹„ì„ í˜• ë°ì´í„°ê°€ ìˆë‹¤ê³  í•´ë³´ì.123m = 100X = 6 * np.random.rand(m,1)-3y = 0.5*X**2+X+2+np.random.randn(m,1) ìœ„ì™€ ê°™ì€ ë°ì´í„°ê°€ non-linear ë°ì´í„°ì¸ë° ì„ í˜•íšŒê·€ ëª¨ë¸ì„ ìœ„ ë°ì´í„°ì— ì ìš©í•´ë³´ë©´,123lin_reg = LinearRegression()lin_reg.fit(X, y)pred = lin_reg.predict(X) ìœ„ ê·¸ë¦¼ê³¼ ê°™ì´ ë°ì´í„°ì˜ ë¹„ì„ í˜•ì  íŒ¨í„´ì„ ì „í˜€ íŒŒì•…í•˜ì§€ ëª»í•œ ì±„ 1ì°¨ ì§ì„ ìœ¼ë¡œë§Œ ì˜ˆì¸¡ì„ í•˜ê²Œ ëœë‹¤.ì´ì œ ì´ëŸ¬í•œ ë¹„ì„ í˜• ë°ì´í„°ë¥¼ ì„ í˜• ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ ìœ„ ë°ì´í„°ì— ê° íŠ¹ì„±ì„ ì œê³±í•˜ì—¬ ìƒˆë¡œìš´ íŠ¹ì„±ì„ ì¶”ê°€í•œë‹¤. 1234567from sklearn.preprocessing import PolynomialFeaturespoly_features = PolynomialFeatures(degree=2, include_bias=False)X_poly = poly_features.fit_transform(X)lin_reg = LinearRegression()lin_reg.fit(X_poly, y)pred = lin_reg.predict(X_poly) ë°ì´í„°ì— ìƒˆë¡œìš´ ë‹¤í•­ íŠ¹ì„±ì„ ì¶”ê°€í•˜ì˜€ì„ ë•Œ ì„ í˜• ëª¨ë¸ì´ ë°ì´í„°ì˜ íŒ¨í„´ì„ íŒŒì•…í•˜ì—¬ ì˜ˆì¸¡í•˜ëŠ” íŠ¹ì„±ì„ ë³´ì—¬ì£¼ê³  ìˆë‹¤.ìœ„ ë°ì´í„°ì˜ ì‹¤ì œ í•¨ìˆ˜ëŠ”,ì´ê³ , ì˜ˆì¸¡ ëª¨ë¸ì˜ í•¨ìˆ˜ëŠ”,ì´ë¯€ë¡œ ì‹¤ì œ ê°’ê³¼ ì˜ˆì¸¡ ê°’ì˜ ì°¨ì´ê°€ ë§ì§€ ì•ŠìŒì„ ì•Œ ìˆ˜ ìˆë‹¤. Regularized Linear Regression(ê·œì œê°€ ìˆëŠ” ì„ í˜• ëª¨ë¸)ìœ„ì—ì„œ ì‚´í´ë³´ì•˜ë“¯ ì´ ì„ í˜• ëª¨ë¸ì˜ ê²½ìš° ëª¨ë¸ì´ í›ˆë ¨ ë°ì´í„° ì…‹ì— ê³¼ëŒ€ì í•©(overfitting)ë˜ë”ë¼ë„ ëª¨ë¸ì„ ê·œì œí•  ë°©ì•ˆì´ ì—†ë‹¤. ë”°ë¼ì„œ, ê³¼ëŒ€ì í•©ì„ ê°ì†Œì‹œí‚¤ê¸° ìœ„í•´ì„œëŠ” ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜(weight)ë¥¼ ê·œì œ(ì œí•œ)í•¨ìœ¼ë¡œì¨ ê³¼ëŒ€ì í•©ë˜ê¸° ì–´ë µê²Œ ë§Œë“¤ì–´ì•¼ í•œë‹¤. ì´ë ‡ê²Œ ê°€ì¤‘ì¹˜ë¥¼ ì œí•œí•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ë¦¿ì§€(Ridge), ë¼ì˜(Lasso), ì—˜ë¼ìŠ¤íŒƒë„·(ElasticNet) íšŒê·€ì— ëŒ€í•´ ì‚´í´ë³´ë ¤ê³  í•œë‹¤. 1. Ridge Regression(ë¦¿ì§€ íšŒê·€)ì„ í˜• íšŒê·€ì— ê·œì œê°€ ì¶”ê°€ëœ íšŒê·€ ëª¨ë¸ì´ë‹¤. ê·œì œí•­ì´ ë¹„ìš©í•¨ìˆ˜ì— ì¶”ê°€ ë˜ë©° ì´ëŠ” í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì„ ë°ì´í„°ì— ë§ì¶”ëŠ” ê²ƒë¿ë§Œ ì•„ë‹ˆë¼ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ê°€ ê°€ëŠ¥í•œ í•œ ì‘ê²Œ ìœ ì§€ë˜ë„ë¡ í•œë‹¤. ê·œì œí•­ì€ í›ˆë ¨í•˜ëŠ” ë™ì•ˆì—ë§Œ ë¹„ìš©í•¨ìˆ˜ì— ì¶”ê°€ë˜ê³ , í›ˆë ¨ì´ ëë‚˜ë©´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ê·œì œê°€ ì—†ëŠ” ì„±ëŠ¥ ì§€í‘œë¡œ í‰ê°€í•œë‹¤. ì„ í˜•íšŒê·€ì— ê·œì œ(L2: ê°€ì¤‘ì¹˜ë“¤ì˜ ì œê³±í•©ì„ ìµœì†Œí™”)ë¥¼ ê±¸ì–´ ê³¼ëŒ€ì í•©ì„ ë°©ì§€ í•˜ì´í¼íŒŒë¼ë¯¸í„° a(alpha)ëŠ” ëª¨ë¸ì„ ì–¼ë§ˆë‚˜ ê·œì œí• ì§€ ì¡°ì ˆ a = 0 ì´ë©´, ë¦¿ì§€ íšŒì‰¬ëŠ” ì„ í˜•íšŒê·€ì™€ ê°™ìŒ a ê°€ ì•„ì£¼ í¬ë©´ ëª¨ë“  ê°€ì¤‘ì¹˜ê°€ ê±°ì˜ 0ì— ê°€ê¹Œì›Œì§€ê³  ê²°êµ­ ë°ì´í„°ì˜ í‰ê· ì„ ì§€ë‚˜ëŠ” ìˆ˜í‰ì„ ì´ ë¨ë¦¿ì§€ íšŒê·€ì˜ ë¹„ìš©í•¨ìˆ˜ëŠ” ì•„ë˜ ìˆ˜ì‹ê³¼ ê°™ë‹¤. ìœ„ì—ì„œ ë³´ì•˜ë˜ boston house price ì˜ˆì¸¡ì— ridge íšŒê·€ë¥¼ ì ìš©í•  ê²½ìš° ì„ í˜• íšŒê·€ë³´ë‹¤ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ì–»ì„ ìˆ˜ ìˆë‹¤. 123456789####### Ridge Regression ####### from sklearn.linear_model import Ridgeridge = Ridge(alpha=0.1).fit(X_train, y_train)train_pred = ridge.predict(X_train)test_pred = ridge.predict(X_test)print('MSE of train set: ', mean_squared_error(y_train, train_pred)) # 19.645print('MSE of test set: ', mean_squared_error(y_test, test_pred)) # 29.878 2. Lasso Regression(ë¼ì˜ íšŒê·€)ë¼ì˜ íšŒê·€ ì—­ì‹œ ì„ í˜• íšŒê·€ì— ê·œì œê°€ ì¶”ê°€ëœ ëª¨ë¸ì´ë©°, ë¦¿ì§€ íšŒê·€ì—ì„œ ì‚¬ìš©ëœ L2 ê·œì œê°€ ì•„ë‹Œ ê°€ì¤‘ì¹˜ ë²¡í„°ì˜ L1 ë…¸ë¦„ì„ ì‚¬ìš©í•œë‹¤. ë¼ì˜ íšŒê·€ì˜ ê°€ì¥ ì¤‘íš¨í•œ íŠ¹ì§•ì€ ëœ ì¤‘ìš”í•œ íŠ¹ì„±ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì™„ì „íˆ ì œê±°í•˜ë ¤ê³  í•œë‹¤ëŠ” ê²ƒì´ë‹¤. ìë™ìœ¼ë¡œ ëœ ì¤‘ìš”í•œ íŠ¹ì„±ì„ ì œê±°í•˜ëŠ” íŠ¹ì„± ì„ íƒ(feature selection)ì„ ìˆ˜í–‰í•˜ê³  í¬ì†Œ ëª¨ë¸(spare model)ì„ ë§Œë“¬(ì¦‰, 0ì´ì•„ë‹Œ íŠ¹ì„±ì˜ ê°€ì¤‘ì¹˜ê°€ ì‘ìŒ) ì´ë¥¼ í†µí•´ ëª¨ë¸ì„ ì´í•´í•˜ê¸° ì‰¬ì›Œì§€ê³  ëª¨ë¸ì˜ ê°€ì¥ ì¤‘ìš”í•œ íŠ¹ì„±ì´ ë¬´ì—‡ì¸ì§€ íŒŒì•… ê°€ëŠ¥ë¼ì˜ íšŒê·€ì˜ ë¹„ìš©í•¨ìˆ˜ëŠ” ì•„ë˜ ìˆ˜ì‹ê³¼ ê°™ë‹¤. ë§ˆì°¬ê°€ì§€ë¡œ boston houseì— Lasso ëª¨ë¸ì„ ì ìš©í•œë‹¤. lassoëª¨ë¸ì˜ coef_ íŒŒë¼ë¯¸í„°ë¥¼ ì´ìš©í•˜ë©´ ëª‡ ê°œì˜ íŠ¹ì„±ì´ ì œì™¸ë˜ê³  ì‚¬ìš©ë˜ì—ˆëŠ”ì§€ ì•Œ ìˆ˜ ìˆë‹¤. 12345678910####### Lasso Regression #######from sklearn.linear_model import Lassolasso = Lasso(alpha=0.01, max_iter=100000).fit(X_train, y_train)train_pred = lasso.predict(X_train)test_pred = lasso.predict(X_test)print('MSE of train set: ', mean_squared_error(y_train, train_pred)) # 19.678print('MSE of test set: ', mean_squared_error(y_test, test_pred)) # 30.091print('ì‚¬ìš©í•œ íŠ¹ì„±ì˜ ìˆ˜ : ',np.sum(lasso.coef_ != 0)) # 13 3. Elastic Net(ì—˜ë¼ìŠ¤í‹±ë„·)ì—˜ë¼ìŠ¤í‹±ë„·ì€ ë¦¿ì§€ íšŒê·€ì™€ ë¼ì˜ íšŒê·€ë¥¼ ì ˆì¶©í•œ ëª¨ë¸ì´ë‹¤. ê·œì œí•­ì€ ë¦¿ì§€ì™€ íšŒê·€ì˜ ê·œì œí•­ì„ ë‹¨ìˆœíˆ ë”í•´ì„œ ì‚¬ìš©í•˜ë©°, í˜¼í•© ì •ë„ëŠ” í˜¼í•© ë¹„ìœ¨ rì„ ì‚¬ìš©í•´ ì¡°ì ˆí•œë‹¤. r = 0ì´ë©´, ì—˜ë¼ìŠ¤í‹±ë„·ì€ ë¦¿ì§€ íšŒê·€ì™€ ê°™ê³ , r = 1ì´ë©´, ë¼ì˜ íšŒê·€ì™€ ê°™ì•„ì§ ì—˜ë¼ìŠ¤í‹±ë„·ì˜ ë¹„ìš©í•¨ìˆ˜ëŠ” ì•„ë˜ ìˆ˜ì‹ê³¼ ê°™ë‹¤. elastic netë„ boston house priceì— ì ìš©. 123456789####### ElasticNet #######from sklearn.linear_model import ElasticNetelastic = ElasticNet(alpha=0.001, max_iter=10000000).fit(X_train, y_train)train_pred = elastic.predict(X_train)test_pred = elastic.predict(X_test)print('MSE of train set: ', mean_squared_error(y_train, train_pred)) # 19.657print('MSE of test set: ', mean_squared_error(y_test, test_pred)) # 29.974 Outroê·¸ë ‡ë‹¤ë©´ ì„ í˜•íšŒê·€, ë¦¿ì§€, ë¼ì˜, ì—˜ë¼ìŠ¤í‹±ë„·ì„ ê°ê° ì–¸ì œ, ì–´ë–¤ ìƒí™©ì— ì‚¬ìš©í•´ì•¼ ì¢‹ì„ê¹Œ?ì ì–´ë„ ê·œì œê°€ ìˆëŠ” ëª¨ë¸ì´ ëŒ€ë¶€ë¶„ì˜ ìƒí™©ì—ì„œ ì¢‹ìœ¼ë¯€ë¡œ ì¼ë°˜ì ìœ¼ë¡œ ì„ í˜•íšŒê·€ ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ í”¼í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤. ê¸°ë³¸ì ìœ¼ë¡œ ridge íšŒê·€ê°€ ê¸°ë³¸ì´ ë˜ì–´ ì‚¬ìš© ë¨ í•˜ì§€ë§Œ, íŠ¹ì„±ì´ ë§ê³  ê·¸ ì¤‘ ì¼ë¶€ë¶„ë§Œ ì¤‘ìš”í•˜ë‹¤ë©´ lassoë‚˜ elastic netì´ ë” ì¢‹ì€ ì„ íƒì¼ ìˆ˜ ìˆìŒ ë˜í•œ, íŠ¹ì„± ìˆ˜ê°€ í›ˆë ¨ ìƒ˜í”Œ ìˆ˜ë³´ë‹¤ ë§ê±°ë‚˜ íŠ¹ì„± ëª‡ ê°œê°€ ê°•í•˜ê²Œ ì—°ê´€ë˜ì–´ ìˆì„ ë•ŒëŠ” lassoë³´ë‹¤ëŠ” elastic netì´ ì„ í˜¸ ë¨","categories":[],"tags":[{"name":"regression","slug":"regression","permalink":"https://jaehyeongan.github.io/tags/regression/"},{"name":"linear","slug":"linear","permalink":"https://jaehyeongan.github.io/tags/linear/"},{"name":"ridge","slug":"ridge","permalink":"https://jaehyeongan.github.io/tags/ridge/"},{"name":"lasso","slug":"lasso","permalink":"https://jaehyeongan.github.io/tags/lasso/"},{"name":"elsasticnet","slug":"elsasticnet","permalink":"https://jaehyeongan.github.io/tags/elsasticnet/"}]},{"title":"ê²½ì‚¬í•˜ê°•ë²•(Gradient Descent)","slug":"ê²½ì‚¬í•˜ê°•ë²•-Gradient-Descent","date":"2019-04-23T12:52:20.000Z","updated":"2020-12-10T14:55:24.000Z","comments":true,"path":"2019/04/23/ê²½ì‚¬í•˜ê°•ë²•-Gradient-Descent/","link":"","permalink":"https://jaehyeongan.github.io/2019/04/23/ê²½ì‚¬í•˜ê°•ë²•-Gradient-Descent/","excerpt":"","text":"Introìµœì ì˜ ì˜ˆì¸¡ ëª¨ë¸ì„ ë§Œë“¤ê¸° ìœ„í•´ì„œëŠ” ì‹¤ì œê°’(true)ê³¼ ì˜ˆì¸¡ê°’(predict)ê³¼ì˜ Error(cost function)ê°€ ìµœì†Œê°€ ë˜ëŠ” ëª¨ë¸ì„ ì°¾ëŠ” ê²ƒì´ë‹¤. í•˜ì§€ë§Œ ë¶„ì„ìê°€ ì§ì ‘ ëª¨ë¸ì˜ Cost functionì„ ìµœì†Œí™”ì‹œí‚¤ëŠ” íŒŒë¼ë¯¸í„° ê°’ì„ ì°¾ê¸° ìœ„í•´ì„œëŠ” ìˆ˜ì‹­ ë²ˆì˜ íŒŒë¼ë¯¸í„° ë³€ê²½ì´ í•„ìš”í•˜ê¸° ë•Œë¬¸ì— ëª¨ë¸ì´ í•™ìŠµê³¼ì •ì—ì„œ ìŠ¤ìŠ¤ë¡œ cost functionì´ ìµœì†Œê°€ ë˜ë„ë¡ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•´ë‚˜ê°€ëŠ” ê²½ì‚¬í•˜ê°•ë²•(Gradient Decent)ì´ ì‚¬ìš©ëœë‹¤. Gradient Descentê²½ì‚¬í•˜ê°•ë²•(Gradient Descent) ê²½ì‚¬í•˜ê°•ë²•ì´ë€ ë¹„ìš©í•¨ìˆ˜(Cost Function)ì„ ìµœì†Œí™”í•˜ê¸° ìœ„í•˜ì—¬ ë°˜ë³µì í•´ì„œ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•´ë‚˜ê°€ëŠ” ê²ƒì„ ë§í•œë‹¤.ë§Œì•½ í•œ ë°¤ ì¤‘ì— ì‚°ì—ì„œ ê¸¸ì„ ìƒì—ˆì„ ë•Œ, ì‚° ë°‘ìœ¼ë¡œ ë‚´ë ¤ê°€ëŠ” ê°€ì¥ ì¢‹ì€ ë°©ë²•ì€ ë¬´ì—‡ì¼ê¹Œ? ë°”ë¡œ ê°€ì¥ ê°€íŒŒë¥¸ ê¸¸ì„ ë”°ë¼ ì‚° ì•„ë˜ë¡œ ë‚´ë ¤ê°€ëŠ” ê²ƒì´ë‹¤. ì´ì™€ ê°™ì´ ìµœì ì˜ ê°’ì— ë„ë‹¬í•˜ê¸° ìœ„í•´ ê°€ì¥ ë¹ ë¥¸ ê¸¸ì„ ì°¾ëŠ” ê³¼ì •ì„ ê²½ì‚¬ í•˜ê°•ë²•ì˜ ê¸°ë³¸ì›ë¦¬ë¼ê³  í•  ìˆ˜ ìˆë‹¤.â€” íŒŒë¼ë¯¸í„° ë²¡í„° theta()ì— ëŒ€í•´ cost functionì˜ í˜„ì¬ gradientë¥¼ ê³„ì‚°â€” theta()ì˜ ê²½ìš° ì„ì˜ì˜ ê°’ìœ¼ë¡œ ì‹œì‘í•´ì„œ(random initialization) ì¡°ê¸ˆì”© cost functionì´ ê°ì†Œë˜ëŠ” ë°©í–¥ìœ¼ë¡œ ì§„í–‰ í•™ìŠµë¥ (learning rate)ê²½ì‚¬ í•˜ê°•ë²•ì—ì„œ ì¤‘ìš”í•œ íŒŒë¼ë¯¸í„°ë¡œì„œ í•™ìŠµ ì‹œ ìŠ¤í…œ(step)ì˜ í¬ê¸° í•™ìŠµë¥ ì´ ë„ˆë¬´ ì‘ì„ ê²½ìš°â€” ì•Œê³ ë¦¬ì¦˜ì´ ìˆ˜ë ´í•˜ê¸° ìœ„í•´ ë°˜ë³µì„ ë§ì´ ì§„í–‰í•´ì•¼ í•˜ë¯€ë¡œ í•™ìŠµ ì‹œê°„ì´ ì˜¤ë˜ê±¸ë¦¼â€” ì§€ì—­ ìµœì†Ÿê°’(local minimum)ì— ìˆ˜ë ´í•  ìˆ˜ ìˆìŒ í•™ìŠµë¥ ì´ ë„ˆë¬´ í´ ê²½ìš°â€” í•™ìŠµ ì‹œê°„ì´ ì ê²Œ ê±¸ë¦¬ë‚˜â€” ìŠ¤í…ì´ ë„ˆë¬´ ì»¤ ì „ì—­ ìµœì†Ÿê°’(global minimum)ì„ ê°€ë¡œì§ˆëŸ¬ ë°˜ëŒ€í¸ìœ¼ë¡œ ê±´ë„ˆë›°ì–´ ìµœì†Ÿê°’ì—ì„œ ë©€ì–´ì§ˆ ìˆ˜ ìˆìŒ ê²½ì‚¬í•˜ê°•ë²•ì˜ ë¬¸ì œì â€” ë¬´ì‘ìœ„ ì´ˆê¸°í™”(random initialization)ìœ¼ë¡œ ì¸í•´ ì•Œê³ ë¦¬ì¦˜ì´ ì „ì—­ ìµœì†Ÿê°’ì´ ì•„ë‹Œ ì§€ì—­ ìµœì†Ÿê°’ì— ìˆ˜ë ´í•  ìˆ˜ ìˆìŒâ€” í‰íƒ„í•œ ì§€ì—­ì„ ì§€ë‚˜ê¸° ìœ„í•´ì„  ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ê³  ì¼ì° ë©ˆì¶”ê²Œ ë˜ì–´ ì „ì—­ ìµœì†Ÿê°’ì— ë„ë‹¬í•˜ì§€ ëª»í•  ìˆ˜ ìˆìŒâ€” í•˜ì§€ë§Œ ì„ í˜• íšŒê·€(Linear Regression)ë¥¼ ìœ„í•œ MSE(Mean Squared Error) cost functionì€ ì–´ë–¤ ë‘ì ì„ ì„ íƒí•´ ì–´ë””ì—ì„œ ì„ ì„ ê·¸ì–´ë„ ê³¡ì„ ì„ ê°€ë¡œì§€ë¥´ì§€ ì•ŠëŠ” ë³¼ë¡ í•¨ìˆ˜(convex function)ì„â€” ì´ëŠ” ì§€ì—­ ìµœì†Ÿê°’ì´ ì—†ê³  í•˜ë‚˜ì˜ ì „ì—­ ìµœì†Ÿê°’ë§Œì„ ê°€ì§€ëŠ” ê²ƒì„ ëœ»í•˜ë©°, ì—°ì†ëœ í•¨ìˆ˜ì´ê³  ê¸°ìš¸ê¸°ê°€ ê°‘ìê¸° ë³€í•˜ì§€ ì•ŠìŒ Batch Gradient Descentâ€” ê²½ì‚¬í•˜ê°•ë²•ì„ êµ¬í˜„í•˜ë ¤ë©´ ê° ëª¨ë¸ íŒŒë¼ë¯¸í„° theta()ì— ëŒ€í•´ ë¹„ìš© í•¨ìˆ˜ì˜ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ê³„ì‚°í•´ì•¼ í•¨.â€” ì¦‰, theta()ê°€ ì¡°ê¸ˆ ë³€ê²½ë  ë•Œ ë¹„ìš©í•¨ìˆ˜ê°€ ì–¼ë§ˆë‚˜ ë³€í•˜ëŠ”ì§€ ê³„ì‚°í•´ì•¼ í•˜ëŠ”ë° ì´ë¥¼ í¸ë„ í•¨ìˆ˜(partial derivative)ë¼ê³  í•¨.â€” ë§¤ gradient descent stepì—ì„œ í›ˆë ¨ ë°ì´í„° ì „ì²´ë¥¼ ì‚¬ìš©â€” ê·¸ë ‡ê¸° ë•Œë¬¸ì— ë§¤ìš° í° training setì—ì„œëŠ” í•™ìŠµì´ ë§¤ìš° ëŠë¦¼ Stochastic Gradient Descent(SGD) â€” ë§¤ stepì—ì„œ ë”± í•œ ê°œì˜ ìƒ˜í”Œì„ ë¬´ì‘ìœ„ë¡œ ì„ íƒí•˜ê³  ê·¸ í•˜ë‚˜ì˜ ìƒ˜í”Œì— ëŒ€í•œ gradientë¥¼ ê³„ì‚°â€” ë§¤ìš° ì ì€ ë°ì´í„°ë¥¼ ì²˜ë¦¬ í•˜ê¸° ë•Œë¬¸ì— í•™ìŠµ ì†ë„ê°€ ë¹ ë¥´ê³ , í•˜ë‚˜ì˜ ìƒ˜í’€ë§Œ ë©”ëª¨ë¦¬ì— ìˆìœ¼ë©´ ë˜ë¯€ë¡œ ë§¤ìš° í° training setë„ í›ˆë ¨ì´ ê°€ëŠ¥â€” cost functionì´ ë§¤ìš° ë¶ˆê·œì¹™í•  ê²½ìš° ì•Œê³ ë¦¬ì¦˜ì´ local minimumì„ ê±´ë„ˆë›°ë„ë¡ ë„ì™€ì£¼ë¯€ë¡œ global minimumì„ ì°¾ì„ ê°€ëŠ¥ì„±ì´ ë†’ìŒâ€” í•˜ì§€ë§Œ ìƒ˜í”Œ ì„ íƒì´ í™•ë¥ ì (Stochastic)ì´ê¸° ë•Œë¬¸ì— ë°°ì¹˜ ê²½ì‚¬ í•˜ê°•ë²•ì— ë¹„í•´ ë¶ˆì•ˆì •â€” cost functionì´ local minimumì— ë‹¤ë‹¤ë¥¼ ë•Œê¹Œì§€ ë¶€ë“œëŸ½ê²Œ ê°ì†Œí•˜ì§€ ì•Šê³  ìœ„ì•„ë˜ë¡œ ìš”ë™ì¹˜ë©´ì„œ í‰ê· ì ìœ¼ë¡œ ê°ì†Œ Mini-batch Gradient Descentâ€” mini-batchë¼ ë¶ˆë¦¬ëŠ” ì„ì˜ì˜ ì‘ì€ ìƒ˜í”Œ ì„¸íŠ¸ì— ëŒ€í•´ gradientë¥¼ ê³„ì‚°â€” SGDì— ë¹„í•´ matrix ì—°ì‚°ì— ìµœì í™”ë˜ì–´ ìˆìœ¼ë©°, íŒŒë¼ë¯¸í„° ê³µê°„ì—ì„œ ëœ ë¶ˆê·œì¹™í•˜ê²Œ í•™ìŠµâ€” í•˜ì§€ë§Œ, local minimumì— ë¹ ì§€ë©´ ë¹ ì ¸ë‚˜ì˜¤ê¸° í˜ë“¬ Outroì•„ë˜ëŠ” batch, mini-bath, stochastic gradient descentì˜ ê²½ì‚¬ í•˜ê°•ë²• ì§„ë¡œë¥¼ ì‚´í´ë³¸ ê·¸ë¦¼ì´ë‹¤.(ì¶œì²˜ : í•¸ì¦ˆì˜¨ ë¨¸ì‹ ëŸ¬ë‹) ëª¨ë‘ ìµœì†Ÿê°’ ê·¼ì²˜ì— ë„ë‹¬í–ˆì§€ë§Œ ë°°ì¹˜ ê²½ì‚¬ í•˜ê°•ë²•ì˜ ê²½ë¡œê°€ ì‹¤ì œë¡œ ìµœì†Ÿê°’ì—ì„œ ë©ˆì¶˜ ë°˜ë©´ í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²• ë° ë¯¸ë‹ˆë°°ì¹˜ ê²½ì‚¬í•˜ê°•ë²•ì€ ê·¼ì²˜ì—ì„œ ë§´ëŒê³  ìˆë‹¤. ê·¸ë ‡ì§€ë§Œ ë°°ì¹˜ ê²½ì‚¬ í•˜ê°•ë²•ì—ëŠ” ë§¤ ìŠ¤í…ì—ì„œ ë§ì€ ì‹œê°„ì´ ì†Œìš”ë˜ê³ , í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•ê³¼ ë¯¸ë‹ˆë°°ì¹˜ ê²½ì‚¬ í•˜ê°•ë²•ë„ ì ì ˆí•œ í•™ìŠµ ìŠ¤ì¼€ì¥´(learning schedule)ì„ ì‚¬ìš©í•˜ë©´ ìµœì†Ÿê°’ì— ë„ë‹¬í•  ìˆ˜ ìˆë‹¤.","categories":[],"tags":[{"name":"deeplearning","slug":"deeplearning","permalink":"https://jaehyeongan.github.io/tags/deeplearning/"},{"name":"machinelearning","slug":"machinelearning","permalink":"https://jaehyeongan.github.io/tags/machinelearning/"},{"name":"learningrate","slug":"learningrate","permalink":"https://jaehyeongan.github.io/tags/learningrate/"},{"name":"gradientdescent","slug":"gradientdescent","permalink":"https://jaehyeongan.github.io/tags/gradientdescent/"},{"name":"batchgd","slug":"batchgd","permalink":"https://jaehyeongan.github.io/tags/batchgd/"},{"name":"minibatchgd","slug":"minibatchgd","permalink":"https://jaehyeongan.github.io/tags/minibatchgd/"},{"name":"sgd","slug":"sgd","permalink":"https://jaehyeongan.github.io/tags/sgd/"},{"name":"meansquarederror","slug":"meansquarederror","permalink":"https://jaehyeongan.github.io/tags/meansquarederror/"}]},{"title":"ë¨¸ì‹ ëŸ¬ë‹ì„ ìœ„í•œ ì•„ë‚˜ì½˜ë‹¤(Anaconda) ê°œë°œí™˜ê²½ êµ¬ì¶•","slug":"machine-learning-ê°œë°œí™˜ê²½-êµ¬ì¶•í•˜ê¸°","date":"2019-04-09T13:06:02.000Z","updated":"2020-12-10T14:54:03.000Z","comments":true,"path":"2019/04/09/machine-learning-ê°œë°œí™˜ê²½-êµ¬ì¶•í•˜ê¸°/","link":"","permalink":"https://jaehyeongan.github.io/2019/04/09/machine-learning-ê°œë°œí™˜ê²½-êµ¬ì¶•í•˜ê¸°/","excerpt":"","text":"IntroëŒ€ë¶€ë¶„ ì²˜ìŒ ë¨¸ì‹ ëŸ¬ë‹ë¥¼ í•˜ê³ ì ë§ˆìŒë¨¹ê³  ì‹œë„í•˜ëŠ” ê²ƒì´ ê°œë°œí™˜ê²½ì„ ë§Œë“œëŠ” ê²ƒ ì…ë‹ˆë‹¤.ë¬¼ë¡  ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ì— ëŒ€í•œ ê¸°ë³¸ì  ì´í•´ê°€ ë¨¼ì € ë’·ë°›ì¹¨ì´ ë˜ì–´ ìˆì–´ì•¼ í•˜ê² ì§€ë§Œ ì²˜ìŒë¶€í„° ë„ˆë¬´ ì–´ë µê²Œ ì‹œì‘í•˜ë©´ ì¬ë¯¸ì—†ìë‚˜ìš”? ë¨¼ì € pythonì˜ ì„¸ê³„ì—ì„œ hello world ë¶€í„° ì°ì–´ë´ì•¼ì ¸. ì˜¤ëŠ˜ì€ Machine learning, data science ë­ ë“±ë“±ì„ í•˜ê¸° ìœ„í•´ ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ê³  ìˆëŠ” ì–¸ì–´ì¸ Python ê°œë°œí™˜ê²½ì„ êµ¬ì¶•í•˜ëŠ” ë°©ë²•ì„ ì†Œê°œí•©ë‹ˆë‹¤.(ë‚´ ê°œë°œí™˜ê²½ ê¸°ì¤€ìœ¼ë¡œ)(JDKëŠ” ì„¤ì¹˜ë˜ì–´ìˆì–´ì•¼ í•©ë‹ˆë‹¤.) êµ³ì´ pythonì„ ì„¤ì¹˜í•  í•„ìš” ì—†ì´, data science íŒ¨í‚¤ì§€ ëª¨ìŒì¸ Anacondaë¥¼ ì„¤ì¹˜í•  ê²ƒì…ë‹ˆë‹¤.AnacondaëŠ” ìˆ˜í•™ ë° ê³¼í•™ ê´€ë ¨ numpy, scipy, pandas, matplotlibê³¼ ê°™ì€ ìœ ìš©í•œ python packageë¥¼ ëª¨ì•„ë†“ì€ ë°°í¬íŒì…ë‹ˆë‹¤. ë¶„ëª…íˆ ì¥ì ì´ê¸´ í•˜ì§€ë§Œ ì˜ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” íŒ¨í‚¤ì§€ê¹Œì§€ ëª¨ë‘ í¬í•¨í•˜ê³  ìˆì–´ ë¬´ê²ë‹¤ëŠ” ë‹¨ì ì´ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ, ì €ëŠ” Anacondaì˜ ì¶•ì†ŒíŒì¸ Minicondaë¥¼ ê¹”ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.MinicondaëŠ” Anacondaì™€ ë‹¤ë¥´ê²Œ ì‚¬ìš©í•˜ê³ ì í•˜ëŠ” íŒ¨í‚¤ì§€ë¥¼ ìŠ¤ìŠ¤ë¡œ ì„¤ì¹˜í•´ì•¼í•˜ëŠ” ë²ˆê±°ë¡œì›€ì´ ìˆì§€ë§Œ ê°€ë³ë‹¤ëŠ” ì¥ì ì´ ìˆìŠµë‹ˆë‹¤. Anaconda vs MinicondaAnacondaâ€” pythonì´ë‚˜ condaë¥¼ ì²˜ìŒ ì ‘í•˜ëŠ” ê²½ìš° ì¢‹ìŒâ€” pythonê³¼ 150ê°œ ì´ìƒì˜ ê³¼í•™ íŒ¨í‚¤ì§€ë¥¼ í•œ ë²ˆì— ìë™ ì„¤ì¹˜í•˜ì—¬ í¸ë¦¬â€” ê°•ë ¥í•œ script editorì¸ Jupyter notebookì´ í¬í•¨ë˜ì–´ ìˆìŒâ€” 3gbì˜ ì—¬ìœ  ìš©ëŸ‰ì´ í•„ìš” Minicondaâ€” ì ì€ ìš©ëŸ‰(100mb ì´í•˜)â€” ìŠ¤ìŠ¤ë¡œ ì›í•˜ëŠ” íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•˜ê³ ì í•  ê²½ìš° ì¢‹ìŒ(ì¢‹ì€ ìŠµê´€) Install Miniconda(+python)ìš°ì„  Miniconda í™ˆí˜ì´ì§€ë¥¼ ê°€ë©´ ì„¤ì¹˜ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤. ìœ„ ë§í¬ì—ì„œëŠ” í˜„ì¬ python 2.7ë²„ì „ê³¼ 3.7ë²„ì „ì„ ì œê³µí•˜ê³  ìˆìŠµë‹ˆë‹¤. ìµœì‹ ë²„ì „ì´ë¯€ë¡œ ë‹¤ìš´ë°›ìœ¼ì…”ë„ ë¬´ë°©í•˜ì§€ë§Œ í•œ ê°€ì§€ ê³ ë ¤í•  ê²ƒì´ ìˆìŠµë‹ˆë‹¤. ë§Œì•½ ì¶”í›„ deep learningì„ í•˜ê³ ì í•œë‹¤ë©´ 3.7ë²„ì „ ë³´ë‹¤ ì•„ë˜ ë²„ì „ì„ ì‚¬ìš©í•˜ì‹œëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. tensorflowê°€ ì•„ì§ 3.7ë²„ì „ì— í˜¸í™˜ë˜ì§€ ì•Šê±°ë“ ì—¬ã…œã…œê·¸ë˜ì„œ Miniconda installer archiveì—ì„œ python3.6ë²„ì „ìœ¼ë¡œ ë˜ì–´ìˆëŠ” ê²ƒì„ ì°¾ìœ¼ì‹œë©´ ë©ë‹ˆë‹¤.ì €ëŠ” ì•ˆì •ì„± ë¬¸ì œë¥¼ ê³ ë ¤í•˜ì—¬ Miniconda2-4.5.4-Windows-x86_64.exeì„ ì´ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤. (ìˆ˜ì •)ë‹¤ì‹œ ì•Œì•„ë³´ë‹ˆ ì´ì œ python 3.7ë²„ì „ë„ tensorflowì™€ í˜¸í™˜ì´ ëœë‹¤ë„¤ìš”! ê·¸ëƒ¥ Miniconda í™ˆí˜ì´ì§€ì—ì„œ ìµœì‹ ë²„ì „ì„ ë‹¤ìš´ë°›ìœ¼ì…”ë„ ë¬´ë°©í•  ê²ƒ ê°™ìŠµë‹ˆë‹¤. ë‹¤ìš´ë°›ì€ í›„ ì„¤ì¹˜íŒŒì¼ì„ ë”ë¸”í´ë¦­í•˜ë©´ ì•„ë˜ì™€ ê°™ì€ í™”ë©´ì´ ë‚˜íƒ€ë‚©ë‹ˆë‹¤. ì´í›„, nextë²„íŠ¼ì„ ëˆŒëŸ¬ ì„¤ì¹˜ë¥¼ ì§„í–‰í•˜ì‹œë©´ ë©ë‹ˆë‹¤. ì„¤ì¹˜ê°€ ì™„ë£Œë˜ë©´ ì„¤ì¹˜ëœ ê²½ë¡œì— ì•„ë˜ì™€ ê°™ì´ minicondaê°€ ê¹”ë¦½ë‹ˆë‹¤.(ë™ì‹œì— pythonë„ ê°™ì€ ê²½ë¡œì— ì„¤ì¹˜ê°€ ë©ë‹ˆë‹¤.) Enroll System pathì, ì´ì œ miniconda ë° pythonì„ ì„¤ì¹˜í•˜ì˜€ìœ¼ë‹ˆ í™˜ê²½ë³€ìˆ˜ì— ë“±ë¡í•´ì¤˜ì•¼ í•©ë‹ˆë‹¤. java jdk ë“±ë¡í•˜ëŠ”ê±°ì™€ ê°™ìŠµë‹ˆë‹¤.ìš°ì„  í™˜ê²½ë³€ìˆ˜ì— ë“±ë¡í•´ì•¼ í•˜ëŠ” pathëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.(ë³¸ì¸ì´ ì„¤ì¹˜í•œ pathì— ë§ê²Œ ë„£ì–´ì£¼ì‹œë©´ ë©ë‹ˆë‹¤.) C:\\Users\\nonam\\Miniconda3 C:\\Users\\nonam\\Miniconda3\\python.exe C:\\Users\\nonam\\Miniconda3\\Scripts C:\\Users\\nonam\\Miniconda3\\Library\\bin anaconda ë° minicondaë¥¼ ì„¤ì¹˜í•˜ê²Œ ë˜ë©´ anaconda promptì™€ ê°™ì€ ì½˜ì†”ì°½ì´ í•¨ê»˜ ì„¤ì¹˜ë©ë‹ˆë‹¤.ê²€ìƒ‰ì°½ì—ì„œ anaconda promptë¥¼ ì‹¤í–‰í•˜ì—¬ ì•„ë˜ì™€ ê°™ì´ pythonì´ ì‹¤í–‰ëœë‹¤ë©´ ì„¤ì¹˜ê°€ ì™„ë£Œëœ ê²ƒì…ë‹ˆë‹¤. Install Jupyter labì´ì œ ê°•ë ¥í•œ data science ì—ë””í„°ì¸ jupyter labì„ ì„¤ì¹˜í•´ë³´ê² ìŠµë‹ˆë‹¤.(jupyter labì€ jupyter notebookë³´ë‹¤ ë‹¤ì–‘í•œ ê¸°ëŠ¥ì„ ê°€ì§€ê³  ìˆê³  íŒŒì¼ ê´€ë¦¬ê°€ ì‰¬ì›Œ ì• ìš©í•©ë‹ˆë‹¤.)anaconda promptë¥¼ ì—´ì–´ ì•„ë˜ì™€ ê°™ì´ ëª…ë ¹ì–´(jupyter lab ë° í•´ë‹¹ kernelì„ ì„¤ì¹˜)ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.123&gt; conda install -c conda-forge jupyterlab&gt; python -m ipykernel install --user ì‹¤í–‰ì´ ë˜ë©´ http://localhost:8888/ ì£¼ì†Œë¡œ jupyter labì´ ì‹¤í–‰ë˜ë©° ì•„ë˜ì™€ ê°™ì€ í™”ë©´ì´ ë‚˜íƒ€ë‚©ë‹ˆë‹¤. Outrojupyter notebookì€ communication computing shellì´ë¼ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. pythonì˜ ê²°ê³¼ë¥¼ ë°”ë¡œë°”ë¡œ í™•ì¸ê°€ëŠ¥í•˜ê¸° ë•Œë¬¸ì— ë°ì´í„° ë¶„ì„ ë° ì‹œê°í™”ì—ì„œ ì•„ì£¼ ê°•ë ¥í•œ íˆ´ì´ì§€ìš”.í•˜ì§€ë§Œ, ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•´ì•¼ í•˜ê³  loopë° ì¡°ê±´ë¬¸ì´ ìì£¼ ì½”ë“œì— í¬í•¨ëœë‹¤ë©´ ì¡°ê¸ˆ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.jupyter notebookì€ ë³€ìˆ˜ì— ë©”ëª¨ë¦¬ë¥¼ ì ì¬ í›„ ì§€ì†ì ìœ¼ë¡œ ë©”ëª¨ë¦¬ë¥¼ ì°¨ì§€í•˜ê¸° ë•Œë¬¸ì— ëŒ€ìš©ëŸ‰ ë°ì´í„°ì™€ ê°™ì€ ê³ ì„±ëŠ¥ ë°ì´í„° ì²˜ë¦¬ì—ëŠ” ê·¸ë‹¤ì§€ ì¶”ì²œë“œë¦¬ì§€ ì•ŠìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì €ëŠ” ê°„ë‹¨íˆ ë°ì´í„°ì˜ ë¶„í¬ ë° ë¶„ì„ì„ ìœ„í•´ì„œë§Œ jupyterë¥¼ ì‚¬ìš©í•˜ëŠ” í¸ì…ë‹ˆë‹¤. ê·¸ ì™¸ ì „ì²´ì ì¸ ì½”ë”©ì€ sublime text3ë¼ëŠ” editorë¥¼ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤.ì¶”í›„ python ì½”ë”©ì„ ìœ„í•œ ê°•ë ¥í•œ ë˜ë‹¤ë¥¸ editorì¸ sublime text3ì— ëŒ€í•´ ì†Œê°œí•´ë“œë¦¬ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.","categories":[],"tags":[{"name":"machinelearning","slug":"machinelearning","permalink":"https://jaehyeongan.github.io/tags/machinelearning/"},{"name":"python","slug":"python","permalink":"https://jaehyeongan.github.io/tags/python/"},{"name":"anaconda","slug":"anaconda","permalink":"https://jaehyeongan.github.io/tags/anaconda/"},{"name":"miniconda","slug":"miniconda","permalink":"https://jaehyeongan.github.io/tags/miniconda/"},{"name":"jupyter","slug":"jupyter","permalink":"https://jaehyeongan.github.io/tags/jupyter/"},{"name":"sublimetext3","slug":"sublimetext3","permalink":"https://jaehyeongan.github.io/tags/sublimetext3/"},{"name":"datascience","slug":"datascience","permalink":"https://jaehyeongan.github.io/tags/datascience/"}]},{"title":"Keras functional api - Multi-input ëª¨ë¸ êµ¬ì¶•í•˜ê¸°","slug":"KERAS-FUNCTIONAL-API-MULTI-INPUT-ëª¨ë¸-êµ¬ì¶•í•˜ê¸°","date":"2019-03-25T15:40:27.000Z","updated":"2020-12-10T14:52:26.000Z","comments":true,"path":"2019/03/26/KERAS-FUNCTIONAL-API-MULTI-INPUT-ëª¨ë¸-êµ¬ì¶•í•˜ê¸°/","link":"","permalink":"https://jaehyeongan.github.io/2019/03/26/KERAS-FUNCTIONAL-API-MULTI-INPUT-ëª¨ë¸-êµ¬ì¶•í•˜ê¸°/","excerpt":"","text":"Introì§€ë‚œ í•œë‹¬ê°„ íšŒì‚¬ í”„ë¡œì íŠ¸ë¥¼ ìœ„í•´ ê³µë¶€í•œ ë‚´ìš©ì„ ì •ë¦¬í•  ê²¸ ì˜¤ëŠ˜ì€ keras functional api(í•¨ìˆ˜í˜• api)ì— ëŒ€í•œ ì†Œê°œì™€ ì´ê²ƒì„ ì–´ë–»ê²Œ ì ìš©í•˜ëŠ”ì§€ë¥¼ LSTMëª¨ë¸ê³¼ embeddingëª¨ë¸ì„ í†µí•´ ê°„ë‹¨íˆ ì†Œê°œí•˜ë ¤ê³  í•œë‹¤. ê·¸ë™ì•ˆ kerasë¥¼ í†µí•´ ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ êµ¬ì¶•í•˜ê¸° ìœ„í•´ì„œëŠ” Sequential ëª¨ë¸ì„ ì´ìš©í•˜ì˜€ì„ ê²ƒì´ë‹¤.Sequential ëª¨ë¸ì€ ë„¤íŠ¸ì›Œí¬ì˜ ì…ë ¥ê³¼ ì¶œë ¥ì´ í•˜ë‚˜ë¼ê³  ê°€ì •í•˜ê³  ì¸µì„ ì°¨ë ˆëŒ€ë¡œ ìŒ“ì•„ êµ¬ì„±í•œë‹¤. ë”°ë¼ì„œ ìœ„ì™€ ê°™ì€ Sequential ëª¨ë¸ì— ë°ì´í„°ë¥¼ í•™ìŠµí•˜ê¸° ìœ„í•´ì„œëŠ” ëª¨ë“  ë°ì´í„°ë¥¼ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ì „ì²˜ë¦¬í•˜ì—¬ ëª¨ë¸ì— ë§ê²Œ shapeì„ êµ¬ì„±í•´ì£¼ì–´ì•¼ í•œë‹¤.í•˜ì§€ë§Œ, ìœ„ì™€ ê°™ì€ êµ¬ì„±ì´ ë§ì§€ ì•ŠëŠ” ê²½ìš°ë„ ì¡´ì¬í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì¤‘ê³  ì˜ë¥˜ ì‹œì¥ ê°€ê²©ì„ ì˜ˆì¸¡í•˜ëŠ” ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ë§Œë“ ë‹¤ê³  ê°€ì •í•´ë³´ê² ë‹¤. ì´ ëª¨ë¸ì€ ì‹œì¥ ê°€ê²© ì˜ˆì¸¡ì„ ìœ„í•´ ì˜ë¥˜ ë¸Œëœë“œ, ì œì‘ ì—°ë„ì™€ ê°™ì€ ì •ë³´(ë©”íƒ€ ë°ì´í„°), ì‚¬ìš©ìê°€ ì œê³µí•œ ì œí’ˆ ë¦¬ë·°(í…ìŠ¤íŠ¸ ë°ì´í„°), í•´ë‹¹ ì˜ë¥˜ ì‚¬ì§„(ì´ë¯¸ì§€ ë°ì´í„°)ê³¼ ê°™ì€ ë°ì´í„°ë¥¼ ë°›ëŠ”ë‹¤. ëª¨ë¸ì€ ë°ì´í„°ì˜ íŠ¹ì„±ì— ë§ê²Œ ì ì ˆíˆ ì‚¬ìš©ë˜ì–´ì•¼ í•˜ëŠ”ë°, í•´ë‹¹ ë°ì´í„°ê°€ textì¸ì§€, imageì¸ì§€, time-seriesì¸ì§€ì— ë”°ë¼ í•™ìŠµí•˜ëŠ” ëª¨ë¸ë„ ë‹¬ë¼ì§„ë‹¤.ìœ„ì™€ ê°™ì€ ê²½ìš°,ë©”íƒ€ ë°ì´í„°ë§Œ ìˆë‹¤ë©´ ì´ë¥¼ one-hot encodingí•˜ì—¬ ë‹¨ìˆœí•œ DenseNetëª¨ë¸ì„ êµ¬í˜„í•  ìˆ˜ ìˆì„ ê²ƒì´ê³ ,í…ìŠ¤íŠ¸ ë°ì´í„°ì˜ ê²½ìš° ì´ë¥¼ word2vec ê°™ì€ ê¸°ë²•ì„ í†µí•´ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ Embedding ëª¨ë¸ì´ë‚˜ í˜¹ì€ RNNëª¨ë¸ì„ êµ¬í˜„í•  ìˆ˜ ìˆì„ ê²ƒì´ê³ ,ì´ë¯¸ì§€ ë°ì´í„°ì˜ ê²½ìš° CNNê³¼ ê°™ì€ ConveNet ëª¨ë“ˆì„ ì´ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. keras functional apií•˜ì§€ë§Œ ë°©ê¸ˆ ì‚´í´ë³¸ ê²ƒê³¼ ê°™ì´ ì˜ˆì¸¡ì— ì‚¬ìš©ë˜ëŠ” ë°ì´í„°ê°€ ì—¬ëŸ¬ í˜•íƒœë¡œ ì¡´ì¬í•œë‹¤ë©´ ì–´ë–¤ ëª¨ë¸ì„ ì‚¬ìš©í•´ì•¼ í• ê¹Œ?ë‹¨ìˆœíˆ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ë¥¼ vectorizeí•˜ì—¬ ì˜ˆì¸¡ ë³€ìˆ˜ë¡œ ì¶”ê°€í•˜ì—¬ ì‚¬ìš©í•´ì•¼ í• ê¹Œ?ë°ì´í„° íŠ¹ì„±ì— ë”°ë¼ ê°ê° ëª¨ë¸ì„ í•™ìŠµì‹œí‚¬ ìˆœ ì—†ì„ê¹Œ? ì´ëŸ¬í•œ ì˜ë¬¸ì„ í•´ê²°í•´ì¤„ ê²ƒì´ ë°”ë¡œ ì˜¤ëŠ˜ ì‚´í´ë³¼ Keras Functional APIì´ë‹¤í•¨ìˆ˜í˜• apië¼ê³  ë¶ˆë¦¬ë©°, ë§ ê·¸ëŒ€ë¡œ ëª¨ë¸ì„ í•¨ìˆ˜ì²˜ëŸ¼ í•„ìš”í•  ë•Œ í˜¸ì¶œí•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤. ì¦‰, ëª¨ë¸ì„ í•¨ìˆ˜ë¡œ êµ¬í˜„í•˜ì—¬ ëª¨ë“ˆì‹ìœ¼ë¡œ ì´ìš©í•œë‹¤ëŠ” ë§ì´ë‹¤. ë‹¤ì‹œ ìœ„ì˜ ì˜ˆë¡œ ëŒì•„ê°€ í•¨ìˆ˜í˜• APIë¥¼ í™œìš©í•˜ë©´ ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ ëª¨ë¸ë³„ í•™ìŠµ ë° ì˜ˆì¸¡ì´ ê°€ëŠ¥í•´ì§„ë‹¤. ìœ„ ê·¸ë¦¼ê³¼ ê°™ì€ ëª¨ë¸ì„ ë‹¤ì¤‘ì…ë ¥ëª¨ë¸(multi-input model)ì´ë¼ê³  í•˜ë©° ì´ ì™¸ì—ë„ ë‹¤ì¤‘ì¶œë ¥ëª¨ë¸(multi-output model)ì´ ì¡´ì¬í•©ë‹ˆë‹¤. ë‹¤ì¤‘ì…ë ¥ëª¨ë¸: ë°ì´í„° íŠ¹ì„±ì— ë”°ë¥¸ ì„œë¡œ ë‹¤ë¥¸ ì—¬ëŸ¬ê°œì˜ ëª¨ë¸ì´ inputìœ¼ë¡œ ì‚¬ìš©ë˜ì–´ í•˜ë‚˜ì˜ outputì„ ë‚´ëŠ” ë„¤íŠ¸ì›Œí¬ ë‹¤ì¤‘ì¶œë ¥ëª¨ë¸: í•˜ë‚˜ì˜ outputì´ ì•„ë‹Œ ë°ì´í„°ì— ìˆëŠ” ì—¬ëŸ¬ ì†ì„±ì„ ë™ì‹œì— ì˜ˆì¸¡í•˜ëŠ” ë„¤íŠ¸ì›Œí¬ í•¨ìˆ˜í˜• APIëŠ” ê¸°ì¡´ êµ¬í˜„ë°©ë²•ê³¼ êµ¬ì¡°ì ìœ¼ë¡œ ì°¨ì´ê°€ ìˆë‹¤.ë³´í†µ ëª¨ë¸ì„ êµ¬í˜„í•  ë•Œ Sequential()ê°ì²´ë¥¼ ìƒì„± í›„ ì‹œí€€ìŠ¤ í˜•íƒœë¡œ ìˆœì°¨ì ìœ¼ë¡œ layerë¥¼ ìŒ“ì•„ê°€ì§€ë§Œ í•¨ìˆ˜í˜• apiëŠ” Model()ê°ì²´ë¥¼ í†µí•´ ëª¨ë¸ì„ êµ¬í˜„í•œë‹¤. ê¸°ì¡´ Sequential() ì‚¬ìš© ì‹œ 123456789from keras import models, layersmodel = models.Sequential()model.add(layers.Dense(64, activation='relu', input_shape=(784,)))model.add(layers.Dense(64, activation='relu'))model.add(layers.Dense(10, activation='softmax'))model.fit(data, labels) # starts training funciontal api() ì‚¬ìš© ì‹œ 1234567891011121314from keras.layers import Input, Densefrom keras.models import Model# This returns a tensorinputs = Input(shape=(784,))# a layer instance is callable on a tensor, and returns a tensorx = Dense(64, activation='relu')(inputs)x = Dense(64, activation='relu')(x)outputs = Dense(10, activation='softmax')(x)# This creates a model that includes# the Input layer and three Dense layersmodel = Model(inputs=inputs, outputs=outputs) ìœ„ì™€ ê°™ì´ Sequential()ê°ì²´ëŠ” inputë¶€í„° outputê¹Œì§€ ìˆœì°¨ì ìœ¼ë¡œ ì´ë£¨ì–´ì§€ì§€ë§Œ, í•¨ìˆ˜í˜• apiëŠ” ê°ê°ì˜ ë³€ìˆ˜ì— layerë¥¼ ë°›ì•„ ëª¨ë“ˆë³„ë¡œ êµ¬ì„±í•  ìˆ˜ ìˆìœ¼ë©°, ë§ˆì§€ë§‰ì—ëŠ” Model()ê°ì²´ì— inputê³¼ outputí…ì„œë¥¼ ì§€ì •í•˜ì—¬ ëª¨ë¸ì„ ìƒì„±í•œë‹¤. ì ìš©ê·¸ë ‡ë‹¤ë©´ ì§ì ‘ kerasë¥¼ ì´ìš©í•˜ì—¬ ì ìš©í•˜ëŠ” ê³¼ì •ì„ ì†Œê°œí•˜ë ¤ê³  í•œë‹¤.ë°ì´í„° ì…‹ê³¼ ì „ì²˜ë¦¬ ê³¼ì •ì€ ê³µê°œí•  ìˆœ ì—†ìœ¼ë‚˜ í•´ë‹¹ ë°ì´í„°ëŠ” ì¼ë°˜ Sequence ë°ì´í„° ë° Text ë°ì´í„°ë¡œ ì´ë£¨ì–´ì ¸ ìˆê³ , ê³ ì¥ë°œìƒì— ëŒ€í•œ ì—¬ë¶€ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë¬¸ì œì´ë‹¤. functional apië¥¼ ì ìš©í•˜ê¸° ìœ„í•˜ì—¬ ë‘ê°œì˜ ëª¨ë¸ì„ êµ¬ì¶•í•˜ì˜€ë‹¤. Sequence ë°ì´í„°ë¥¼ ìœ„í•´ì„œëŠ” ì‹œê°„ ë° ìˆœì„œê°€ ìˆëŠ” ë°ì´í„°ì— íš¨ìœ¨ì ì¸ LSTM(Long Short Term Memory Network)ë¥¼ ì´ìš©í•˜ì˜€ê³ , text ë°ì´í„°ëŠ” vectorize í›„ Embedding ëª¨ë¸ì„ ì´ìš©í•˜ì˜€ë‹¤. ê°œëµì ì¸ ëª¨ë¸ êµ¬ì„±ë„ëŠ” ëŒ€ëµ ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ë‹¤. 1. LSTM ëª¨ë¸ ì ìš©ì„ ìœ„í•œ Sequence ë°ì´í„° ì²˜ë¦¬ìš°ì„  LSTMê³¼ ê°™ì€ Recurrent ëª¨ë¸ì€ í¬ê¸°ê°€ (timesteps, input_features)ì¸ 2D í…ì„œë¡œ ì¸ì½”ë”©ëœ ë²¡í„°ì˜ ì‹œí€€ìŠ¤ë¥¼ ì…ë ¥ë°›ê¸° ë•Œë¬¸ì— shapeì„ ë§ì¶”ì–´ ì¤€ë‹¤.shapeì„ ë§ì¶°ì£¼ê¸° ì „ì— ìš°ì„  textë³€ìˆ˜ì™€ target ê°’ì„ ì œì™¸í•´ì¤€ í›„, ë°ì´í„°ë¥¼ normalizeí•´ì£¼ì—ˆë‹¤. 12345678910111213## preprocessing for lstm sequence_train = df[:train_size].drop(['text_data','target'], axis=1)sequence_test = df[train_size:].drop(['text_data','target'], axis=1)# normalizescaler = StandardScaler().fit(sequence_train)sequence_train_scale = scaler.transform(sequence_train)sequence_test_scale = scaler.transform(sequence_test)timesteps = 1columns_size = len(sequence_train.columns)sequence_train = sequence_train_scale.reshape((sequence_train_scale.shape[0], timesteps, columns_size))sequence_test = sequence_test_scale.reshape((sequence_test_scale.shape[0], timesteps, columns_size)) 2. Embedding ëª¨ë¸ ì ìš©ì„ ìœ„í•œ text ë°ì´í„° ì²˜ë¦¬Embedding ëª¨ë¸ì„ êµ¬í˜„í•˜ê¸° ìœ„í•˜ì—¬ ë¨¼ì € ë°ì´í„°ë¥¼ 3D í…ì„œë¡œ ë³€í™˜ì‹œì¼œì£¼ì–´ì•¼ í•œë‹¤. ì´ë¥¼ ìœ„í•´ kerasì˜ Tokenizer()ê°ì²´ë¥¼ ì´ìš©í•˜ì˜€ë‹¤. ê³¼ì •ì€ ì•„ë˜ì™€ ê°™ë‹¤. fit_on_texts(): í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ í†µí•´ word indexë¥¼ êµ¬ì¶• texts_to_sequences(): word indexë¥¼ í†µí•´ í•´ë‹¹ í…ìŠ¤íŠ¸ë¥¼ ì‹œí€€ìŠ¤ í˜•íƒœë¡œ ë³€í™˜ pad_sequences(): 3D í…ì„œë¡œ ë³€í™˜í•˜ê¸° ìœ„í•´ paddingì„ ì¶”ê°€ 123456789101112131415## preprocessing for embeddingtext_embed = df.loc[:, ['text_data']]text_embed_train = text_embed[:train_size]text_embed_test = text_embed[train_size:]# tokenizemax_words = 1000 # ì‚¬ìš©í•  ìµœëŒ€ ë‹¨ì–´ ìˆ˜ max_len = 50 # ë‹¨ì–´ì˜ ê¸¸ì´tokenizer = text.Tokenizer(num_words=max_words) # top 1,000 wordstokenizer.fit_on_texts(text_embed_train) # word_index êµ¬ì¶•sequences_text_train = tokenizer.texts_to_sequences(text_embed_train) # return sequencesequences_text_test = tokenizer.texts_to_sequences(text_embed_test) # add padding pad_train = sequence.pad_sequences(sequences_text_train, maxlen=max_len)# return 3D tensorpad_test = sequence.pad_sequences(sequences_text_test, maxlen=max_len) # return 3D tensor 3. multi-input model êµ¬ì¶•ìš°ì„  LSTMëª¨ë¸ê³¼ Embeddingëª¨ë¸ì„ ë§Œë“  í›„ concatenate(model1, model2)í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ ì„œë¡œ ë‹¤ë¥¸ ë‘ ê°œì˜ ëª¨ë¸ì˜ outputì„ í•˜ë‚˜ì˜ ëª¨ë¸ë¡œ í†µí•©í•  ìˆ˜ ìˆë‹¤. 12345678910111213141516171819202122232425262728293031323334def multi_input_lstm_embedding_model(timesteps, columns_size, max_words, max_len): # lstm model lstm_input = layers.Input(shape=(timesteps, columns_size)) lstm_out = layers.LSTM(64, dropout=0.3, recurrent_dropout=0.3)(lstm_input) lstm_model = Model(inputs=lstm_input, outputs=lstm_out) # embedding model embed_input = layers.Input(shape=(None,)) embed_out = layers.Embedding(max_words, 8, input_length=max_len)(embed_input) embed_out = layers.Bidirectional(layers.LSTM(64, dropout=0.3, recurrent_dropout=0.3))(embed_out) embed_model = Model(inputs=embed_input, outputs=embed_out) # concatenate concatenated = layers.concatenate([lstm_model.output, embed_model.output]) concatenated = layers.Dense(32, activation='relu')(concatenated) concatenated = layers.BatchNormalization()(concatenated) concat_out = layers.Dense(2, activation='sigmoid')(concatenated) concat_model = models.Model([lstm_input, embed_input], concat_out) return concat_model## model defineconcat_model = multi_input_lstm_embedding_model(timesteps, columns_size, max_words, max_len)concat_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])# model fitconcat_model.fit([df_label_train, pad_train], target_train epochs=7, batch_size=32, callbacks=callbacks_list, validation_data=([sequence_test, pad_test], target_test), shuffle=False) # because of time-series Outrokeras functional apië¥¼ ì´ìš©í•œë‹¤ë©´ ì¢€ ë” ë°ì´í„° íŠ¹ì„±ì— ìœ ì—°í•˜ê²Œ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ í° ì¥ì ì¸ ê²ƒ ê°™ë‹¤.","categories":[],"tags":[{"name":"deeplearning","slug":"deeplearning","permalink":"https://jaehyeongan.github.io/tags/deeplearning/"},{"name":"keras","slug":"keras","permalink":"https://jaehyeongan.github.io/tags/keras/"},{"name":"rnn","slug":"rnn","permalink":"https://jaehyeongan.github.io/tags/rnn/"},{"name":"lstm","slug":"lstm","permalink":"https://jaehyeongan.github.io/tags/lstm/"},{"name":"machinelearning","slug":"machinelearning","permalink":"https://jaehyeongan.github.io/tags/machinelearning/"},{"name":"tensorflow","slug":"tensorflow","permalink":"https://jaehyeongan.github.io/tags/tensorflow/"},{"name":"functionalapi","slug":"functionalapi","permalink":"https://jaehyeongan.github.io/tags/functionalapi/"},{"name":"embedding","slug":"embedding","permalink":"https://jaehyeongan.github.io/tags/embedding/"}]},{"title":"CNN ëª¨ë¸ì„ í†µí•œ ìë™ì°¨ ì‚¬ê³  ì´ë¯¸ì§€ ë¶„ë¥˜","slug":"CNN-ëª¨ë¸ì„-í†µí•œ-ìë™ì°¨-ì‚¬ê³ -ì´ë¯¸ì§€-ë¶„ë¥˜","date":"2018-07-01T03:50:26.000Z","updated":"2020-10-28T15:14:34.000Z","comments":true,"path":"2018/07/01/CNN-ëª¨ë¸ì„-í†µí•œ-ìë™ì°¨-ì‚¬ê³ -ì´ë¯¸ì§€-ë¶„ë¥˜/","link":"","permalink":"https://jaehyeongan.github.io/2018/07/01/CNN-ëª¨ë¸ì„-í†µí•œ-ìë™ì°¨-ì‚¬ê³ -ì´ë¯¸ì§€-ë¶„ë¥˜/","excerpt":"","text":"IntroíšŒì‚¬ í”„ë¡œì íŠ¸ì—ì„œ ìë™ì°¨ ì‚¬ê³  ì´ë¯¸ì§€ ë¶„ë¥˜ ëª¨ë¸ì„ ë§Œë“¤ ì¼ì´ ìƒê²¨ CNN ëª¨ë¸ì„ ì ìš©í•œ ê³¼ì •ì„ ì •ë¦¬í•´ ë³´ê³ ì í•©ë‹ˆë‹¤.ì „ì²´ì ìœ¼ë¡œ í¬ë¡¤ë§(crawling)ì„ í†µí•´ ì‚¬ê³  ì´ë¯¸ì§€ë¥¼ ìˆ˜ì§‘í•˜ì˜€ìœ¼ë©°, ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ ì•„ë˜ 7ê°œì˜ ì‚¬ê³ ë¥¼ ë¶„ë¥˜í•˜ëŠ” ë‹¤ì¤‘(multi class) ë¶„ë¥˜ ëª¨ë¸ì„ ìƒì„±í•˜ì˜€ìŠµë‹ˆë‹¤. ì „ë°© ì¶”ëŒ(Car front crash) ì¸¡ë©´ ì¶”ëŒ(Car side crash) í›„ë°© ì¶”ëŒ(Rear and crash) ìœ ë¦¬ì°½ ê¹¨ì§(Car broken windshield) ì°¨ ìŠ¤í¬ë˜ì¹˜Car scratch) íƒ€ì´ì–´ í‘í¬(Flat tire) ì „ë³µ (Overturned vehicle) ì‚¬ê³  ì´ë¯¸ì§€ ë°ì´í„° ìˆ˜ì§‘ê·¸ ë™ì•ˆ í¬ë¡¤ë§ì„ í• ë•Œ pythonì˜ lxmlì˜ parse í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ html íƒœê·¸ ê¸°ë°˜ìœ¼ë¡œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì˜€ëŠ”ë°, ì •ë§ ê°„ë”´í•˜ê²Œ! êµ¬ê¸€ì—ì„œ ì´ë¯¸ì§€ë¥¼ ìˆ˜ì§‘í•  ìˆ˜ ìˆëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì¸ icrawlerë¥¼ ì•Œê²Œ ë˜ì–´ ì‰½ê²Œ ì´ë¯¸ì§€ë¥¼ ìˆ˜ì§‘í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ì•„ë˜ì™€ ê°™ì´ icrawlerì˜ GoogleImageCrawler()ë¥¼ ì´ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.1234567from icrawler.builtin import GoogleImageCrawlergoogle_crawler = GoogleImageCrawler(parser_threads=2, downloader_threads=4, storage=&#123;'root_dir': '../data'&#125;)google_crawler.crawl(keyword='car crash', max_num=500, date_min=None, date_max=None, min_size=(200,200), max_size=None) keyward: ìˆ˜ì§‘í•˜ê³ ì í•˜ëŠ” ì´ë¯¸ì§€ max_num: ìˆ˜ì§‘í•  ì´ë¯¸ì§€ ìˆ˜ date_min/date_max: ìˆ˜ì§‘í•  ê¸°ê°„ min_size/max_size: ì´ë¯¸ì§€ í¬ê¸° ì´í›„, ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ ì´ë¯¸ì§€ ì²˜ë¦¬ ë° train/test setìœ¼ë¡œ ë‚˜ëˆ„ì—ˆìŠµë‹ˆë‹¤.123456789101112131415161718192021222324252627282930313233343536373839404142rom PIL import Imageimport os, globimport numpy as npfrom sklearn.model_selection import train_test_split# ë¶„ë¥˜ ëŒ€ìƒ ì¹´í…Œê³ ë¦¬ ì„ íƒí•˜ê¸° accident_dir = \"./image\"categories = [\"Car front crash\",\"Car side crash\",\"Rear and crash\",\"Car broken windshield\",\"Car scratch\",\"Flat tire\",\"Overturned vehicle\"]nb_classes = len(categories)# ì´ë¯¸ì§€ í¬ê¸° ì§€ì • image_w = 64 image_h = 64pixels = image_w * image_h * 3# ì´ë¯¸ì§€ ë°ì´í„° ì½ì–´ ë“¤ì´ê¸° X = []Y = []for idx, cat in enumerate(categories): # ë ˆì´ë¸” ì§€ì • label = [0 for i in range(nb_classes)] label[idx] = 1 # ì´ë¯¸ì§€ image_dir = accident_dir + \"/\" + cat files = glob.glob(image_dir+\"/*.jpg\") for i, f in enumerate(files): img = Image.open(f) img = img.convert(\"RGB\") img = img.resize((image_w, image_h)) data = np.asarray(img) # numpy ë°°ì—´ë¡œ ë³€í™˜ X.append(data) Y.append(label) if i % 10 == 0: print(i, \"\\n\", data)X = np.array(X)Y = np.array(Y)# í•™ìŠµ ì „ìš© ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ì „ìš© ë°ì´í„° êµ¬ë¶„ X_train, X_test, y_train, y_test = \\ train_test_split(X, Y)xy = (X_train, X_test, y_train, y_test)print('&gt;&gt;&gt; data ì €ì¥ì¤‘ ...')np.save(\"./image/7obj.npy\", xy)print(\"ok,\", len(Y)) ì´ë¯¸ì§€ë¥¼ RGBë¡œ ë³€í™˜ í›„, 64x64 í¬ê¸°ë¡œ resizeí•´ì£¼ì—ˆìŠµë‹ˆë‹¤. CNN ëª¨ë¸ ìƒì„±ëª¨ë¸ì€ ì´ë¯¸ì§€ ë¶„ë¥˜ì˜ ì •ì„ìœ¼ë¡œ ë¶ˆë¦¬ëŠ” CNN(Convolution Neural Network) ëª¨ë¸ì„ í™œìš©í•˜ì˜€ìŠµë‹ˆë‹¤.ì´ 3ê°œì˜ ì¸µìœ¼ë¡œ êµ¬ì„±í•˜ì˜€ê³ , í™œì„±í™” í•¨ìˆ˜ë¡œëŠ” relu ë° softmax í•¨ìˆ˜ë¥¼ ì ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. dropoutë„ ì ìš©í•˜ì—¬ ê³¼ì í•©ì„ ë°©ì§€í•˜ì˜€ìŠµë‹ˆë‹¤. ëª¨ë¸ í•™ìŠµ í›„ .hdf5 íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758from keras.models import Sequentialfrom keras.layers import MaxPooling2Dfrom keras.layers import Conv2Dfrom keras.layers import Activation, Dropout, Flatten, Denseimport numpy as npimport os# ì¹´í…Œê³ ë¦¬ ì§€ì •í•˜ê¸°categories = [\"Car front crash\",\"Car side crash\",\"Rear and crash\",\"Car broken windshield\",\"Car scratch\",\"Flat tire\",\"Overturned vehicle\"]nb_classes = len(categories)# ì´ë¯¸ì§€ í¬ê¸° ì§€ì •í•˜ê¸°image_w = 64image_h = 64# ë°ì´í„° ì—´ê¸° X_train, X_test, y_train, y_test = np.load(\"./image/7obj.npy\")# ë°ì´í„° ì •ê·œí™”í•˜ê¸°(0~1ì‚¬ì´ë¡œ)X_train = X_train.astype(\"float\") / 256X_test = X_test.astype(\"float\") / 256print('X_train shape:', X_train.shape)# ëª¨ë¸ êµ¬ì¡° ì •ì˜ model = Sequential()model.add(Conv2D(32, (3, 3), input_shape=X_train.shape[1:], padding='same'))model.add(Activation('relu'))model.add(MaxPooling2D(pool_size=(2, 2)))model.add(Dropout(0.25))model.add(Conv2D(64, (3, 3), padding='same'))model.add(Activation('relu'))model.add(Conv2D(64, (3, 3)))model.add(MaxPooling2D(pool_size=(2, 2)))model.add(Dropout(0.25))# ì „ê²°í•©ì¸µmodel.add(Flatten()) # ë²¡í„°í˜•íƒœë¡œ reshapemodel.add(Dense(512)) # ì¶œë ¥model.add(Activation('relu'))model.add(Dropout(0.5))model.add(Dense(nb_classes))model.add(Activation('softmax'))# ëª¨ë¸ êµ¬ì¶•í•˜ê¸°model.compile(loss='categorical_crossentropy', # ìµœì í™” í•¨ìˆ˜ ì§€ì • optimizer='rmsprop', metrics=['accuracy'])# ëª¨ë¸ í™•ì¸#print(model.summary())# í•™ìŠµ ì™„ë£Œëœ ëª¨ë¸ ì €ì¥hdf5_file = \"./image/7obj-model.hdf5\"if os.path.exists(hdf5_file): # ê¸°ì¡´ì— í•™ìŠµëœ ëª¨ë¸ ë¶ˆëŸ¬ë“¤ì´ê¸° model.load_weights(hdf5_file)else: # í•™ìŠµí•œ ëª¨ë¸ì´ ì—†ìœ¼ë©´ íŒŒì¼ë¡œ ì €ì¥ model.fit(X_train, y_train, batch_size=32, nb_epoch=10) model.save_weights(hdf5_file) ëª¨ë¸ì˜ ì˜¤ì°¨ì™€ ì •í™•ë„ë¥¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.1234# ëª¨ë¸ í‰ê°€í•˜ê¸° score = model.evaluate(X_test, y_test)print('loss=', score[0]) # lossprint('accuracy=', score[1]) # acc ì˜¤ì°¨ëŠ” 0.03, ì •í™•ë„ëŠ” 98% ì •ë„ì˜ ì„±ëŠ¥ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. í™•ì‹¤íˆ ë°ì´í„°ë¥¼ ë§ì´ í•™ìŠµì‹œí‚¤ë‹ˆ ì„±ëŠ¥ì´ ì¢‹ì€ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ì‹ ê·œ ë°ì´í„° ì˜ˆì¸¡í•™ìŠµëœ ëª¨ë¸(7obj-model.hdf5)ì— ì‹ ê·œ ì´ë¯¸ì§€ë¥¼ ì ìš©í•˜ì—¬ ì´ë¯¸ì§€ì˜ í´ë˜ìŠ¤ë¥¼ ì˜ˆì¸¡í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. ì ìš©í•  ì´ë¯¸ì§€ëŠ” ì•„ë˜ì˜ ì°¨ ì „ë³µ(Overturned vehicle) ì´ë¯¸ì§€ ì…ë‹ˆë‹¤. ëª¨ë¸ì— ì ìš©í•´ë´…ë‹ˆë‹¤.1234567891011121314# ì ìš©í•´ë³¼ ì´ë¯¸ì§€ test_image = './image/test_overturned.jpg'# ì´ë¯¸ì§€ resizeimg = Image.open(test_image)img = img.convert(\"RGB\")img = img.resize((64,64))data = np.asarray(img)X = np.array(data)X = X.astype(\"float\") / 256X = X.reshape(-1, 64, 64,3)# ì˜ˆì¸¡pred = model.predict(X) result = [np.argmax(value) for value in pred] # ì˜ˆì¸¡ ê°’ì¤‘ ê°€ì¥ ë†’ì€ í´ë˜ìŠ¤ ë°˜í™˜print('New data category : ',categories[result[0]]) í•™ìŠµí• ë•Œì™€ ë˜‘ê°™ì´ ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬í•´ ì£¼ê³  ì €ì¥ëœ ëª¨ë¸ì„ í†µí•´ ì´ë¯¸ì§€ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤. ì˜ˆì¸¡ê²°ê³¼1New data category : Overturned vehicle Overturned Vehicle(ì°¨ ì „ë³µ) í´ë˜ìŠ¤ë¡œ ì´ë¯¸ì§€ê°€ ëª¨ë¸ì— ì˜í•´ ì˜ˆì¸¡ë˜ì—ˆìŠµë‹ˆë‹¤! ëª¨ë¸ì´ ì˜ í•™ìŠµëœ ê²ƒ ê°™ìŠµë‹ˆë‹¤.ê° í´ë˜ìŠ¤ë³„ë¡œ 500ê°œ ì´ 3500ê°œì˜ ì´ë¯¸ì§€ë¥¼ í†µí•´ í•™ìŠµí•œ CNN ë‹¤ì¤‘ ë¶„ë¥˜ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ìƒê°ë³´ë‹¤ ê´œì°®ì€ ê²ƒ ê°™ìŠµë‹ˆë‹¤.","categories":[],"tags":[{"name":"cnn","slug":"cnn","permalink":"https://jaehyeongan.github.io/tags/cnn/"},{"name":"deeplearning","slug":"deeplearning","permalink":"https://jaehyeongan.github.io/tags/deeplearning/"},{"name":"keras","slug":"keras","permalink":"https://jaehyeongan.github.io/tags/keras/"},{"name":"crawling","slug":"crawling","permalink":"https://jaehyeongan.github.io/tags/crawling/"}]},{"title":"ì´ìƒíƒì§€ ì•Œê³ ë¦¬ì¦˜ì„ í†µí•œ ì´ìƒê±°ë˜íƒì§€(FDS)","slug":"ì´ìƒíƒì§€-ì•Œê³ ë¦¬ì¦˜ì„-í†µí•œ-ì´ìƒê±°ë˜íƒì§€-FDS","date":"2018-06-30T09:47:01.000Z","updated":"2020-12-10T14:56:23.000Z","comments":true,"path":"2018/06/30/ì´ìƒíƒì§€-ì•Œê³ ë¦¬ì¦˜ì„-í†µí•œ-ì´ìƒê±°ë˜íƒì§€-FDS/","link":"","permalink":"https://jaehyeongan.github.io/2018/06/30/ì´ìƒíƒì§€-ì•Œê³ ë¦¬ì¦˜ì„-í†µí•œ-ì´ìƒê±°ë˜íƒì§€-FDS/","excerpt":"","text":"Introê¸ˆìœµê±°ë˜ ì¤‘ ë¶€ì •í•˜ê²Œ ì‚¬ìš©ë˜ëŠ” ê±°ë˜ë¥¼ ë¶€ì • ê±°ë˜ë¼ê³  í•©ë‹ˆë‹¤. ê·¸ ì¤‘ ì‹ ìš©ì¹´ë“œ ìœ„ë³€ì¡°, ë„ìš©, ë¶€ì •ê±°ë˜ì— ëŒ€í•œ ë¹„ìœ¨ì€ í•´ë§ˆë‹¤ ì¦ê°€í•˜ê³  ìˆëŠ” ì¶”ì„¸ì…ë‹ˆë‹¤. ì•„ë˜ í‘œëŠ” ì—°ë„ë³„ ì‹ ìš©ì¹´ë“œ ë¶€ì •ì‚¬ìš© ê¸ˆì•¡. ë”°ë¼ì„œ, ìµœê·¼ì—ëŠ” êµ­ë‚´ ì£¼ìš” ì€í–‰ë“¤ì€ FDS(Fraud Detection System)ì„ ë„ì…í•˜ì—¬ ì´ëŸ¬í•œ ë¶€ì •ê±°ë˜ë¥¼ ë§‰ê¸°ìœ„í•´ ë…¸ë ¥í•˜ê³  ìˆì§€ë§Œ ì£¼ë¡œ ë£°(Rule) ê¸°ë°˜ìœ¼ë¡œ ì‚¬ëŒì— ì˜í•´ ì´ë£¨ì–´ì§€ê¸° ë•Œë¬¸ì— ì‹¤ì‹œê°„ìœ¼ë¡œ ì •í™•í•œ íƒì§€ê°€ ì–´ë ¤ìš´ ìƒí™©ì´ë¼ê³  í•©ë‹ˆë‹¤. ëª©í‘œì—¬ê¸°ì„œëŠ” ë¨¸ì‹ ëŸ¬ë‹ì„ ì´ìš©í•˜ì—¬, ì´ëŸ¬í•œ ë¶€ì •ê±°ë˜ë¥¼ íƒì§€í•´ ë³´ê³ ì í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ, ì§€ë„í•™ìŠµì´ ì•„ë‹Œ ë¹„ì§€ë„ í•™ìŠµì„ ì´ìš©í•©ë‹ˆë‹¤. ê·¸ ì¤‘ ì´ìƒ íƒì§€(Outlier Detection) ì•Œê³ ë¦¬ì¦˜ì„ ì´ìš©í•˜ì—¬ ë¼ë²¨ì„ í†µí•œ í•™ìŠµì´ ì•„ë‹Œ ì´ìƒì¹˜ ë°ì´í„° ì§‘ë‹¨ì„ ì°¾ì•„ ê·¸ ì´ìƒì¹˜ ì§‘ë‹¨ì´ ë¶€ì •ê±°ë˜ ë°ì´í„°ì™€ ì¼ì¹˜ ë° ìœ ì‚¬í•œì§€ ì•Œì•„ë³¼ ê²ƒì…ë‹ˆë‹¤. 1. ì‹ ìš©ì¹´ë“œ ë°ì´í„° ì…‹ë°ì´í„° ì…‹ì˜ ê²½ìš° kaggleì—ì„œ ì œê³µí•˜ëŠ” Credit Card Fraud Detection Datasetì„ ì´ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.ìœ„ ë°ì´í„° ì…‹ì€ 2013ë…„ 9ì›” ìœ ëŸ½ì˜ ì‹¤ì œ ì‹ ìš© ì¹´ë“œ ê±°ë˜ ë°ì´í„°ë¥¼ ë‹´ê³  ìˆìŠµë‹ˆë‹¤. ë°ì´í„°ëŠ” ì´ 284,807ê±´ì´ë©° ê·¸ ì¤‘ 492ê±´ë§Œì´ ë¶€ì • ê±°ë˜ ë°ì´í„° ì…ë‹ˆë‹¤.ì¦‰, ë°ì´í„°ê°€ ë§¤ìš° ë¶ˆê· í˜•(imbalanced) í•©ë‹ˆë‹¤. 1234import pandas as pddf = pd.read_csv('./input/creditcard.csv')df.head(10) ìœ„ ë°ì´í„° ì…‹ì€ ê°œì¸ì •ë³´ ë¹„ì‹ë³„í™”ì²˜ë¦¬ë¡œ ì¸í•´ ì¹¼ëŸ¼ì •ë³´ë¥¼ ì•Œ ìˆ˜ ì—†ìœ¼ë©°, ë°ì´í„° ë˜í•œ ìŠ¤ì¼€ì¼(scale) ë° PCA(principal component analysis) ì²˜ë¦¬ ë˜ì–´ìˆìŠµë‹ˆë‹¤.ì´ 31ê°œì˜ ì¹¼ëŸ¼ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆê³ , Time, Amount, Classë¥¼ ì œì™¸í•œ ëª¨ë“  ì¹¼ëŸ¼ì€ ë¹„ì‹ë³„í™”ì²˜ë¦¬ ë˜ì–´ìˆìŠµë‹ˆë‹¤. 1df.info() ë°ì´í„°ëŠ” ì´ 284,807ê±´ì´ë©° nullê°’ì€ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì •í˜• ë°ì´í„° ì…ë‹ˆë‹¤. 2. ë°ì´í„° íƒìƒ‰(EDA) ì‹œê°„(Time)ëŒ€ë³„ ì •ìƒ/ë¶€ì • ê±°ë˜ ë¹„ìœ¨ 1234567891011import matplotlib.pyplot as plt# ì‹œê°„ëŒ€ë³„ íŠ¸ëœì­ì…˜ ì–‘f, (ax1, ax2) = plt.subplots(2,1, sharex=True, figsize=(12,4))ax1.hist(df.Time[df.Class==1], bins=50)ax2.hist(df.Time[df.Class==0], bins=50)ax1.set_title('Fraud')ax2.set_title('Normal')plt.xlabel('Time(in Seconds)'); plt.ylabel('Number of Transactions')plt.show() ìŒ.. ëŒ€ì²´ì ìœ¼ë¡œ ì •ìƒ ê±°ë˜ì˜ ê²½ìš° ì‹œê°„ì— ë”°ë¼ ì£¼ê¸°ì ì¸ ë°˜ë©´ ë¶€ì • ê±°ë˜ì˜ ê²½ìš° ë¶ˆê·œì¹™í•œ íŠ¹ì„±ì„ ë³´ì…ë‹ˆë‹¤. ê¸ˆì•¡(Amount)ëŒ€ë³„ ì •ìƒ/ë¶€ì • ê±°ë˜ ë¹„ìœ¨ 12345678910111213import matplotlib.pyplot as plt# ê¸ˆì•¡ëŒ€ë³„ íŠ¸ëœì­ì…˜ ì–‘f, (ax1, ax2) = plt.subplots(2,1, sharex=True, figsize=(12,4))ax1.hist(df.Amount[df.Class==1], bins=30)ax2.hist(df.Amount[df.Class==0], bins=30)ax1.set_title('Fraud')ax2.set_title('Normal')plt.xlabel('Amount ($)')plt.ylabel('Number of Transactions')plt.yscale('log')plt.show() ì •ìƒ ê±°ë˜ì˜ ê²½ìš° ë‹¤ì–‘í•œ ê¸ˆì•¡ëŒ€ì—ì„œ ë°œìƒë˜ì§€ë§Œ, ë¶€ì • ê±°ë˜ì˜ ê²½ìš° ì ì€ ê¸ˆì•¡ì—ì„œ ì£¼ë¡œ ë°œìƒí•˜ëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤. ë¹„ì‹ë³„ì¹¼ëŸ¼ ì •ìƒ/ë¶€ì •ê±°ë˜ ë¹„ìœ¨íŠ¹ì„± ì°¨ì´ê°€ ì‹¬í•œ ì¼ë¶€ ë³€ìˆ˜ë§Œ í‘œì‹œí•˜ì˜€ìŠµë‹ˆë‹¤. 1234567891011import matplotlib.pyplot as pltimport seaborn as sns# ì •ìƒ/ë¹„ì •ì‚° ëŸ¼ê°„ ê°’ ë¶„í¬v_features = df.ix[:,1:29].columnsfor cnt, col in enumerate(df[v_features]): sns.distplot(df[col][df.Class==1], bins=50) sns.distplot(df[col][df.Class==0], bins=50) plt.legend(['Y','N'], loc='best') plt.title('histogram of feature '+str(col)) plt.show() 3. Isolation Forestì´ìƒíƒì§€ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œëŠ” Isolation Forest ì•Œê³ ë¦¬ì¦˜ì„ ì´ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. Isolation Forest ëŠ” Tree ê¸°ë°˜ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë‚˜ëˆ„ì–´ ë°ì´í„°ì˜ ê´€ì¸¡ì¹˜ë¥¼ ê³ ë¦½ì‹œí‚¤ëŠ” ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤. ì´ìƒ ë°ì´í„°ì˜ ê²½ìš° root nodeì™€ ê°€ê¹Œìš´ depthë¥¼ ê°€ì§€ê³ , ì •ìƒ ë°ì´í„°ì˜ ê²½ìš° treeì˜ ë§ë‹¨ ë…¸ë“œì— ê°€ê¹Œìš´ depthë¥¼ ê°€ì§‘ë‹ˆë‹¤. 4. ì´ìƒ íƒì§€ ì•Œê³ ë¦¬ì¦˜ ì ìš©Isolation Forest ì•Œê³ ë¦¬ì¦˜ì€ í˜„ì¬ scikit-learnì—ì„œ ì œê³µë˜ê³  ìˆìœ¼ë©°, ë§í¬ë¥¼ í†µí•´ ë‹¤íë¨¼íŠ¸ë¥¼ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.Isolation ForestëŠ” ì´ìƒì¹˜ ì ìˆ˜(outlier score)ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì •ìƒ ê±°ë˜/ ë¶€ì • ê±°ë˜ì— ëŒ€í•œ ì´ìƒì¹˜ ì ìˆ˜ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.ìœ„ì˜ ë¶„í¬ë¥¼ ë³´ì•˜ì„ ë•Œ, ì •ìƒ / ë¶€ì • ê±°ë˜ ê°„ ë¹„ìœ¨ì´ ë‹¤ë¥´ê²Œ ë‚˜íƒ€ë‚˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Pythonìš°ì„  ì ìš©í•˜ê¸°ì— ì•ì„œ, ë°ì´í„° ë¶ˆê· í˜•(Data Imbalance)ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•˜ì—¬, ì •ìƒ ê±°ë˜ê±´ì— ëŒ€í•´ Down samplingì„ 70% ë¹„ìœ¨ë¡œ ì§„í–‰í•˜ì˜€ìŠµë‹ˆë‹¤.123456789101112import pandas as pd from imblearn.under_sampling import RandomUnderSamplercredit_data = pd.read_csv('./data/creditcard.csv')X = credit_data.drop(['Class'], axis=1)y = credit_data['Class']print(Counter(y)) # &#123;0: 284315, 1: 492&#125;# Under Samplingsampler = RandomUnderSampler(ratio=0.70, random_state=0)X, y = sampler.fit_sample(X, y)print('Class : ',Counter(y)) # &#123;0: 702, 1: 492&#125; ì´í›„, Isolation Forestë¥¼ ì•„ë˜ì™€ ê°™ì€ íŒŒë¼ë¯¸í„°ë¥¼ í†µí•´ ì ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.123456from sklearn.ensemble import IsolationForestclf = IsolationForest(n_estimators=300, contamination=0.40, random_state=42)clf.fit(X)pred_outlier = clf.predict(X)pred_outlier = pd.DataFrame(pred_outlier).replace(&#123;1:0, -1:1&#125;) n_estimators : ë…¸ë“œ ìˆ˜ contamination : ì´ìƒì¹˜ ë¹„ìœ¨ ì´ìƒíƒì§€ ì˜ˆì¸¡ê°’ì€ 1ì´ ì •ìƒ, -1ì´ ì´ìƒìœ¼ë¡œ ë¶„ë¥˜ë©ë‹ˆë‹¤. ì´ë¥¼ Class ë¼ë²¨ê³¼ì˜ ì˜¤ì°¨ë¥¼ ê³„ì‚°í•˜ì—¬ì•¼ í•˜ê¸° ë•Œë¬¸ì—, ê°™ì€ ë²”ìœ„ë¡œ ë°”ê¿”ì£¼ì—ˆìŠµë‹ˆë‹¤. ì‹œê°í™”ì´ìƒíƒì§€ ê²°ê³¼ë¥¼ 2d ë° 3dë¡œ ì‹œê°í™”í•œ ê²°ê³¼ ì…ë‹ˆë‹¤. ( ì‹œê°í™”ë¥¼ ìœ„í•´ ì°¨ì›ì„ ì¶•ì†Œí•˜ì˜€ìŠµë‹ˆë‹¤.)12345678910111213141516import matplotlib.pyplot as pltfrom mpl_toolkits.mplot3d import Axes3D# plot 2dplt.scatter(X[:,0], X[:,1], c=pred_outlier, cmap='Paired', s=40, edgecolors='white')plt.title(\"Isolation Forest\")plt.show()# plot 3dfig = plt.figure()ax = fig.add_subplot(111, projection='3d')ax.scatter(X[:,0], X[:,1], X[:,2], c=pred_outlier)ax.set_xlabel('pcomp 1')ax.set_ylabel('pcomp 2')ax.set_zlabel('pcomp 3')plt.show() 2ì°¨ì› ì‹œê°í™” 3ì°¨ì› ì‹œê°í™” ì˜ˆì¸¡ ì„±ëŠ¥ì˜ˆì¸¡ê°’ì„ ì‹¤ì œ ë¶€ì •ê±°ë˜ì—¬ë¶€ ì¹¼ëŸ¼ì¸ Classì™€ ë¹„êµí•˜ì—¬ ì„±ëŠ¥ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.ì¸¡ì • ì§€í‘œë¡œëŠ” Accuracy(ì •í™•ë„), Recall(ì¬í˜„ìœ¨), Precision(ì •ë°€ë„), F1-score ì…ë‹ˆë‹¤. ê¸ˆìœµ ê±°ë˜ì—ì„œëŠ” ì •í™•ë„ë„ ë¬¼ë¡  ì¤‘ìš”í•˜ì§€ë§Œ, ì‹¤ì œ ë¶€ì •ê±°ë˜ë¥¼ ë¶€ì •ê±°ë˜ë¡œ ì˜ˆì¸¡í•˜ëŠ” ë¹„ìœ¨ì¸ Recall(ì¬í˜„ìœ¨) ê°’ì´ ì¤‘ìš”í•˜ê²Œ ì—¬ê²¨ì§‘ë‹ˆë‹¤.1234567891011121314151617181920212223242526272829from sklearn.metrics import confusion_matrix, classification_report, accuracy_scoreimport itertoolsclass_name = [0, 1]def plot_confusion_matrix(classes, pred, y_test, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues): cm = confusion_matrix(y_test, pred) plt.imshow(cm, interpolation='nearest', cmap=cmap) plt.title(title) plt.colorbar() tick_marks = np.arange(len(classes)) plt.xticks(tick_marks, classes, rotation=0) plt.yticks(tick_marks, classes) if normalize: cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] thresh = cm.max() / 2. for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])): plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=\"white\" if cm[i, j] &gt; thresh else \"black\")# í‰ê°€print('confusion matrix\\n', confusion_matrix(pred_outlier, y))print('Accuracy: ',accuracy_score(pred_outlier, y))print('classification_report\\n', classification_report(pred_outlier, y))plot_confusion_matrix(class_name, pred_outlier, y, title='Isolation Forest') Confusion Matrix ê²°ê³¼ Classification report ê²°ê³¼ 12345678Accuracy: 0.8442211055276382classification_report precision recall f1-score support 0 0.88 0.86 0.87 716 1 0.80 0.82 0.81 478avg / total 0.85 0.84 0.84 1194 ìœ„ ê²°ê³¼ë¥¼ ë³´ì•˜ì„ ë•Œ, Accuracy ë¬´ë ¤ 84% ì…ë‹ˆë‹¤.Recall ê°’ ë˜í•œ 82% ë¡œ ë†’ì€ ë¶€ì • ê±°ë˜ íƒì§€ ì •í™•ë„ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. Outroë°ì´í„°ë¥¼ ëª©í‘œ ë³€ìˆ˜ë¥¼ í†µí•´ í•™ìŠµí•˜ëŠ” ì§€ë„í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì— ë¹„í•˜ë©´ ì ì€ ì •í™•ë„ì´ê² ì§€ë§Œ,ë°ì´í„°ë¥¼ ì „í˜€ í•™ìŠµí•˜ì§€ ì•Šê³ , ë°ì´í„°ì˜ íŠ¹ì„±ë§Œì„ ê³ ë ¤í•˜ì—¬ ì´ìƒì¹˜ë¥¼ ì°¾ì•„ë‚´ëŠ” ë¹„ì§€ë„ í•™ìŠµìœ¼ë¡œë„ ì¶©ë¶„íˆ ë¶€ì • ê±°ë˜ë¥¼ íƒì§€í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ í™•ì¸í•˜ì˜€ìŠµë‹ˆë‹¤. ìµœê·¼ì—ëŠ” ë”¥ëŸ¬ë‹ ê¸°ë²•ì„ ì´ìš©í•˜ì—¬ ì˜¤í† ì¸ì½”ë”(Auto-encoder)ë‚˜ GAN ì•Œê³ ë¦¬ì¦˜ì„ ì´ìš©í•˜ì—¬ ì´ìƒíƒì§€ì— í™œìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤. ì €ë„ ë” ê³µë¶€í•´ì„œ í•œë²ˆ ì ìš©í•´ë´ì•¼ê² ìŠµë‹ˆë‹¤ã… ã… ã… ","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://jaehyeongan.github.io/tags/python/"},{"name":"outlier detection","slug":"outlier-detection","permalink":"https://jaehyeongan.github.io/tags/outlier-detection/"},{"name":"isolation forest","slug":"isolation-forest","permalink":"https://jaehyeongan.github.io/tags/isolation-forest/"},{"name":"unsupervised learning","slug":"unsupervised-learning","permalink":"https://jaehyeongan.github.io/tags/unsupervised-learning/"},{"name":"scikit-learn","slug":"scikit-learn","permalink":"https://jaehyeongan.github.io/tags/scikit-learn/"},{"name":"fraud detection system","slug":"fraud-detection-system","permalink":"https://jaehyeongan.github.io/tags/fraud-detection-system/"}]},{"title":"íŒŒì´ì¬ ì†Œì¼“(socket) í”„ë¡œê·¸ë˜ë°","slug":"íŒŒì´ì¬-ì†Œì¼“-socket-í”„ë¡œê·¸ë˜ë°","date":"2018-06-29T08:22:55.000Z","updated":"2020-10-28T15:14:34.000Z","comments":true,"path":"2018/06/29/íŒŒì´ì¬-ì†Œì¼“-socket-í”„ë¡œê·¸ë˜ë°/","link":"","permalink":"https://jaehyeongan.github.io/2018/06/29/íŒŒì´ì¬-ì†Œì¼“-socket-í”„ë¡œê·¸ë˜ë°/","excerpt":"","text":"íŒŒì´ì¬ ì†Œì¼“(socket) í”„ë¡œê·¸ë˜ë° ì†Œì¼“(socket)ì„ í†µí•´ ì„œë²„(server)ì™€ í´ë¼ì´ì–¸íŠ¸(client)ê°„ ì–´ë–»ê²Œ ê¸°ë³¸ì ì¸ ë„¤íŠ¸ì›Œí¬ í†µì‹ ì´ ì´ë£¨ì–´ì§€ëŠ”ì§€ ì•Œì•„ë³´ë ¤ê³  í•©ë‹ˆë‹¤.ë¨¼ì € í†µì‹ ì„ ìœ„í•´ ë‘ê°œì˜ íŒŒì¼ì€ ì¤€ë¹„í•©ë‹ˆë‹¤. íŒŒì¼ì€ ê°ê° ì„œë²„ì™€ í´ë¼ì´ì–¸íŠ¸ì— í•´ë‹¹í•©ë‹ˆë‹¤. server.py client.py ìš°ì„  server.py ì‘ì„± 123456789101112131415161718192021222324252627282930313233from socket import *from select import *HOST = ''PORT = 10000BUFSIZE = 1024ADDR = (HOST, PORT)# ì†Œì¼“ ìƒì„±serverSocket = socket(AF_INET, SOCK_STREAM)# ì†Œì¼“ ì£¼ì†Œ ì •ë³´ í• ë‹¹ serverSocket.bind(ADDR)print('bind')# ì—°ê²° ìˆ˜ì‹  ëŒ€ê¸° ìƒíƒœserverSocket.listen(100)print('listen')# ì—°ê²° ìˆ˜ë½clientSocekt, addr_info = serverSocket.accept()print('accept')print('--client information--')print(clientSocekt)# í´ë¼ì´ì–¸íŠ¸ë¡œë¶€í„° ë©”ì‹œì§€ë¥¼ ê°€ì ¸ì˜´data = clientSocekt.recv(65535)print('recieve data : ',data.decode())# ì†Œì¼“ ì¢…ë£Œ clientSocekt.close()serverSocket.close()print('close') ìš°ì„  ì†Œì¼“ì„ ì„¤ì •í•˜ê³ , bind()í•¨ìˆ˜ë¥¼ í†µí•´ ì£¼ì†Œ ì •ë³´ë¥¼ í• ë‹¹í•œë‹¤. ì´í›„, listen()í•¨ìˆ˜ë¥¼ í†µí•´ ì—°ê²° ìˆ˜ì‹  ëŒ€ê¸° ìƒíƒœë¡œ ì „í™˜ í›„ clientê°€ ì—°ê²°í•  ì‹œ accpet() í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ ì—°ê²°ì„ ìˆ˜ë½í•œë‹¤. ë§Œì•½ clientê°€ ë³´ë‚¸ ë©”ì‹œì§€ê°€ ìˆì„ ê²½ìš°, recv(byteí¬ê¸°)ë¥¼ ì´ìš©í•˜ì—¬ ë©”ì‹œì§€ë¥¼ ê°€ì ¸ì˜¨ë‹¤. ì´ì œ client.pyë¥¼ ì‘ì„± 123456789101112131415161718192021222324#! /usr/bin/python# -*- coding: utf-8 -*-from socket import *from select import *import sysfrom time import ctimeHOST = '127.0.0.1'PORT = 10000BUFSIZE = 1024ADDR = (HOST,PORT)clientSocket = socket(AF_INET, SOCK_STREAM)# ì„œë²„ì— ì ‘ì†í•˜ê¸° ìœ„í•œ ì†Œì¼“ì„ ìƒì„±í•œë‹¤.try: clientSocket.connect(ADDR)# ì„œë²„ì— ì ‘ì†ì„ ì‹œë„í•œë‹¤. clientSocket.send('Hello!'.encode()) # ì„œë²„ì— ë©”ì‹œì§€ ì „ë‹¬except Exception as e: print('%s:%s'%ADDR) sys.exit()print('connect is success') ì£¼ì†Œì™€ í¬íŠ¸ë²ˆí˜¸ë¥¼ ì„¤ì • serverì— ì ‘ì†í•˜ê¸° ìœ„í•œ client socketì„ ìƒì„±í•˜ê³  connect()í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ ì„œë²„ì— ì ‘ì†ì„ ì‹œë„ send()í•¨ìˆ˜ë¥¼ ì´ìš©í•´ ë©”ì‹œì§€ë¥¼ serverì— ì „ë‹¬ ì´ì œ ìœ„ì˜ ì½”ë“œë¥¼ ì‹¤í–‰í•´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. server.pyë¥¼ ì‹¤í–‰ í›„ client.pyë¥¼ í†µí•´ serverì— ì ‘ì†í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. ë¨¼ì € server.pyë¥¼ ì‹¤í–‰í•˜ì—¬, clientì˜ ì ‘ì†ì„ ê¸°ë‹¤ë¦½ë‹ˆë‹¤. ì´í›„, client.pyë¥¼ ì‹¤í–‰í•˜ì—¬ serverì— ì ‘ì†ì„ ì‹œë„í•©ë‹ˆë‹¤. serverì—ì„œ clientì˜ ì ‘ì†ì •ë³´ì™€ ë©”ì‹œì§€ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://jaehyeongan.github.io/tags/python/"},{"name":"network","slug":"network","permalink":"https://jaehyeongan.github.io/tags/network/"},{"name":"server","slug":"server","permalink":"https://jaehyeongan.github.io/tags/server/"},{"name":"client","slug":"client","permalink":"https://jaehyeongan.github.io/tags/client/"},{"name":"socket","slug":"socket","permalink":"https://jaehyeongan.github.io/tags/socket/"}]}]}