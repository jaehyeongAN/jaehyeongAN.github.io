{"meta":{"title":"jaehyeong's ds","subtitle":"for data scientist","description":null,"author":null,"url":"https://jaehyeongan.github.io","root":"/"},"pages":[{"title":"Hello,","date":"2018-06-29T01:49:26.000Z","updated":"2020-10-28T15:14:34.487Z","comments":true,"path":"about/index.html","permalink":"https://jaehyeongan.github.io/about/index.html","excerpt":"","text":"JAEHYEONG AN Bachelor degree at Ajou Univ.Master degree at Gachon Univ. Github : jaehyeongANInstagram : jh_an.dsInterests : Machine learning, Deep learning, Python, Tensorflow, Flask &amp; Django e-mail : nonamed000000@gmail.comphone : 010.3566.3150"},{"title":"그 동안 고군분투하며 도움이 되었던 글들.","date":"2018-06-29T04:11:18.000Z","updated":"2020-10-28T15:14:34.750Z","comments":true,"path":"links/index.html","permalink":"https://jaehyeongan.github.io/links/index.html","excerpt":"","text":"# For Data ScienceGAN 쉽게 씌여진 GAN 생성적 적대 신경망(GANs)에 대한 초보자용 가이드 (GANs) AnoGAN in tensorflow AnoGAN을 이용한 철강 소재 결함 검출 AI RNN/LSTM Understanding LSTM Networks Long Short-Term Memory (LSTM) 이해하기 RNN/LSTM 논문리뷰 Keras를 통한 LSTM 구현 Deep Learning for Time Series by Jason Brownlee Stateful LSTM in Keras How to connect LSTM layers in Keras, RepeatVector or return_sequence=True? 기상 날씨 예측 Autoencoder 오토인코더(Autoencoder) 개념 AutoEncoder - LSTM AutoEncoder LSTM Autoencoder for Extreme Rare Event Classification in Keras ConvNet CNN, Convolutional Neural Network 요약 CNN을 활용한 주요 Model - Modern CNN CNN을 활용한 주요 Model - Image Detection Convolutional Feature Maps Class Activation Map(Learning Deep Features for Discriminative Localization) CAM - Class Activation Map Quality inspection in manufacturing using deep learning based computer vision Object Detection R-CNN, Fast R-CNN, Faster R-CNN, YOLO — Object Detection Algorithms R-CNNs Tutorial You Only Look Once - Paper Review Darkflow를 활용하여 YOLO 모델로 이미지 디텍션 구현 윈도우 환경에서 YOLO로 실시간 객체탐지 자동차 번호판 인식 (OCR) with YOLO v2 imgaug를 이용하여 바운딩박스 정보를 포함한 이미지 증폭시키기 OpenCV OpenCV-Python Tutorials NAMP님의 OpenCV-Python Tutorials OpenCV 라이브러리로, 윤곽에 기반한 자동차 번호판 영역 추출(License Plates Recognition) Python+OpenCV로 자동차 번호판 영역 추출 및 인식하기 Recommender System 쿠팡 추천 시스템 2년간의 변천사 (상품추천에서 실시간 개인화로) 당근마켓 딥러닝 개인화 추천 Text 딥러닝을 이용한 자연어 처리 입문 Word2Vec으로 문장 분류하기 keras word embedding Boosting Model Bagging과 Boosting 그리고 Stacking What is LightGBM, How to implement it? How to fine tune the parameters? LIGHTGBM 주요 파라미터 정리 # For DevelopmentLinux Ubuntu 우분투 리눅스 듀얼부팅 설치방법 정리 Install Docker on Ubuntu 18.04 How to install TensorFlow GPU on UBUNTU 18.04 ubuntu 18.04.1 theme 설정 Flask Flask doc 한글 번역 간단한 Flask 어플리케이션 만들기 파이썬 Flask로 간단 웹서버 구동하기 AWS EC2에서 플라스크(Flask) 웹 서버 구동시키기 Django Django/장고 MVT 패턴이란 Deploy Machine Learning Models with Django Real-Time System 루빅스(RUBICS) – kakao의 실시간 추천 시스템 데이타 스트리밍 처리에 대한 이해 Apache Kafka 소개 및 정리 Real Time Credit Card Fraud Detection with Apache Spark and Event Streaming Etc 누구나 쉽게 이해할 수 있는 Git 입문 벡엔드가 이정도는 해줘야 함 시리즈"}],"posts":[{"title":"PEGASUS:Pre-training with Extracted Gap-sentences for Abstractive Summarization 논문 리뷰","slug":"PEGASUS","date":"2020-08-01T11:05:03.000Z","updated":"2020-12-05T13:43:13.905Z","comments":true,"path":"2020/08/01/PEGASUS/","link":"","permalink":"https://jaehyeongan.github.io/2020/08/01/PEGASUS/","excerpt":"","text":"Intro최근 NLP의 downstream tasks 중 하나인 Summarization분야에 “PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization”이라는 새로운 논문(멋진 이름이다..)이 등장하여 간략하게 소개해보려고 한다. What is Text Summarization?Text Summarization은 자연어 처리 분야의 여러 개의 Downstram tasks중 하나이다.이름에서부터 쉽게 알 수 있듯이 Text Summarization은 문서를 요약하는 기술을 의미한다. Text Summarization은 크게 아래와 같이 두 가지로 분류가 된다. 1. Extractive Summarization2. Abstractive Summarization 위 두 방식은 요약(summarization)을 한다는 측면에서는 동일하나, 그 방법에 차이가 있다.위 예시와 같이 Extractive는 원문 텍스트로부터 주요 Sentence를 원문 그대로 추출해내는 방식이라면, Abstractive는 우리가 원문 텍스트를 보고 생각과 느낌을 한 줄 요약하듯이 표현하는 방식이라고 할 수 있다. Extractive Summarization에서 가장 많이 알려진 알고리즘은 아무래도 Text-Rank일 것이다. 초기 구글의 검색엔진랭킹 알고리즘인 Page-Rank를 Text에 적용한 알고리즘으로, 적은 연산량으로도 좋은 성능을 내고 있다. Text-Rank알고리즘은 Document 내에서 Term-Frequency가 높고, Co-occurence가 높은 단어를 keyword로 판단하며, 그러한 keyword를 많이 갖는 Sentence를 Key-Sentence일 것이라 가정하는 알고리즘이라고 할 수 있다.Text-Rank의 자세한 설명은 해당 링크(TextRank 를 이용한 키워드 추출과 핵심 문장 추출) 참조하면 좋을 것 같다. Extractive Summarization vs Abstractive Summarization둘 중에 최근 가장 활발히 연구되는 분야는 아무래도 Abstractive Summarization이다.Abstractive방식이 Extractive방식보다 훨씬 어려운 난이도의 task일 뿐만 아니라 원문을 그대로 추출해내는 것이 아닌 다양한 표현방식으로 Generate하기 때문에 훨씬 더 다양한 분야에 사용될 수 있기 때문이다.최근 몇 년 사이 Seqence-to-Sequence, Attention mechanism, Transformer 등과 같은 아키텍처가 등장하고 Bert와 같은 대량의 corpus로 학습된 pre-training 모델이 등장하며 이러한 generator모델의 성능도 나날이 향상되는 추세이다.이제 아래에서 가장 최근 Abstractive Summarizaion 논문으로 등장한 PEGASUS에 대해 알아보자. PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive SummarizationAbstract최근 대량의 text corpora로 self-supervised된 pre-training Transfomers 모델들이 text summarization을 포함한 fine-tuning downstream NLP task에서 좋은 성능을 보이고 있다. 하지만, Abstractive Text Summarization의 목적에 맞게 pre-training된 모델은 찾아보기 힘들고, 더욱이 다양한 domain을 커버할만 한 체계적인 평가 방법도 부족한 상황이다. 따라서, 해당 논문에서는 대량의 text copora로 self-supervised된 encoder-decoder 기반의 pre-training Transformer 모델인 PEGASUS를 소개한다. PEGASUS의 주요 특징은 GSG(Gap sentence generation)을 사용한다는 것인데, 간단히 설명하면 MLM방식에서는 token 단위로 masking하여 masked token을 예측하는 방식으로 학습했던 것과 유사하게, GSG는 token 단위가 아닌 Importance Sentence 단위로 masking을 하여 학습을 수행한다. 여기서 말하는 Importance Sentence란 document 내에서 다른 문장에 비해 전체적인 context를 잘 설명할 수 있는 문장을 말한다. PEGASUS 모델은 12개의 downstream summarization tasks로부터 ROUGE score를 기반으로 SOTA를 달성하였고, 그 중 6개의 데이터 셋에서 오직 1,000개의 examples만으로도 SOTA를 달성할 만큼 적은 리소스 비용으로 놀라운 성능을 나타내었다. The Basic architecture of PEGASUS PEGASUS는 기본적으로 encoder-decoder기반의 Transformer구조를 하고 있으며, 기존 MLM(Masked Language Model)과 유사하게 Input text의 일부를 masking하여 Encoder의 input으로 보내게 된다. 하지만 기존 MLM과 다른 점은 바로 Sentence 자체를 masking한다는 점이다.기존 MLM 모델들은 token 단위로 masking하여 masked token을 예측하는 방식으로 학습을 진행하였지만, PEGASUS는 Input Document로부터 Sentence 단위로 Masking을 한 후 남은 Sentence를 기반으로 masked sentence를 예측하는 방식으로 학습된다. 논문에서는 이러한 방식을 Gap-Sentences-Generation(GSG)라고 말하고 있다. Gap Sentences Generation (GSG)해당 section에서는 새로운 pre-training 방식인 GSG를 소개하고, 기존 BERT masked-language model과 비교를 수행한다. 해당 논문에서 강조하는 것 중 하나는, 좋은 성능을 얻기 위해서는 적용하고자 하는 downstream task의 목적에 맞는 pre-training 모델을 사용하고 이를 fine-tuning 하라는 것이다. 즉, 뉴스를 요약하기 위한 데이터로 학습된 모델은 뉴스 요약에 좋은 성능을 내겠지만, 영화 시나리오를 요약하는데는 전혀 맞지 않을 수 있기 때문이다. 논문의 실험부분에서 더 소개가 되는데 News관련 데이터셋으로 학습한 모델은 Non-news task에서는 좋은 성능을 내지 못했다. Summarization을 수행하기 위해서는 input document와 그에 맞는 summary text가 쌍으로 활용되어야 한다. 하지만 단순히 extractive 방식으로 summary를 추출하게 되면 모델은 단순히 sentence를 copy하는 방식으로 학습이 되기 때문에, 저자는 최근 masking words와 contiguous spans의 성공에 영감을 받아 GSG를 수행한다고 설명한다. GSG는 전체적으로 아래와 같은 방식으로 수행된다. Select and mask whole sentences form documents. Concatenate the gap-sentences into a pseudo-summary. The corresponding position of each selected gap sentence is replaced by a mask token [MASK1] to inform the model 여기서 gap sentence 비율은 GSR(Gap Sentences Ratio)에 의해 결정되는데 이는 문서의 전체 sentence에서 선택된 gap sentence의 비율을 의미하고, 다른 Masked Language Model에서의 mask rate와 유사한 개념이라고 생각하면 된다. 해당 논문에서는 GSR의 비율에 따른 성능을 실험하였는데 데이터셋에 따라 성능 편차가 있었지만, 최종적으로 GSR을 30%로 선택하였다고 한다. Three primary strategies for gap-sentence그렇다면 어떤 문장이 gap sentence로 선택이 되는걸까?해당 논문은 적절한 Summarization을 위해서 gap sentence는 document내에서 다른 문장들(remaining sentence)에 비해 전체 문맥을 설명할 수 있는 중요한(important/principal) 문장이 선택되어야 한다고 한다. 이를 위해 Random, Lead, Principal이라는 3가지 전략을 사용한다. Random은 말그대로 랜덤하게 m개의 sentence를 추출하는 것이고, Lead는 문서의 가장 첫 m개의 문장, Principal은 selected sentence와 remaining sentence간의 ROUGE1-F1 score를 기반으로 top-m개의 sentence를 선정하는 것을 말한다. (Principal 방법의 경우는 Ind/seq그리고 Orig/Uniq 옵션으로 세분화 되어 실험된다.) 아래는 document내에서 Random, Lead, Principal(Ing-Orig) 각각의 전략에 의해 선택된 sentence들을 보여준다. Masked Language Model(MLM)BERT에서는 input text의 15%의 token을 선택하여, 그 중 80%는 mask token으로 변환하고, 10%는 random token, 나머지 10%는 그대로 사용하게 된다.위 첫번째 그림인 PEGASUS 모델의 아키텍처를 보면 GSG와 MLM이 동시에 적용되고 있는 것을 볼 수 있지만, 실제로는 MLM이 downstream task의 성능 향상에 영향을 주지 않아 최종 모델에서는 MLM을 포함하지 않았다고 한다. Pre-training Corpuspre-training을 위해 사용된 corpus는 C4와 HugeNews이다. C4(Colossal and Cleaned version of Common Crawl) : consist of text from 350M web-pages(750GB) HugeNews : a dataset of 1.5B articles (3.8TB) collected from news and news-like websites from 2013-2019 Downstream Tasks/Datasetsdownstream summarization 및 재현 가능한 코드 제공을 위해 public datasets인 Tensorflow Summarization Datasets 데이터 셋을 활용하였다. 사용된 데이터 셋은 총 12개로 아래와 같다.-Xsum-CNN/DailyMail-NEWSROOM-Multi-News-Gigaword-arXiv-PubMed-BIGPATENT-WikiHow-Reddit TIFU-AESLC-BillSum Experiments효율적인 실험을 위하여 모델의 사이즈를 줄인 PEAGASUS-base모델(223M parameters)과 PEGASUS-large모델(568M parmeters)을 각각 비교한다. PEAGASUS-base number of layers of encoder and decoder(L) : 12 hidden size(H) : 768 feed-forward layer size(F) : 3,072 number of self-attention heads(A) : 12 PEGASUS-large number of layers of encoder and decoder(L) : 16 hidden size(H) : 1024 feed-forward layer size(F) : 4,096 number of self-attention heads(A) : 16 Pre-Training Corpus위 그림에서 볼 수 있듯이 학습시 사용된 Corpus가 무엇이냐에 따라 downstream task의 성능에 영향을 주게 된다.HugeNews를 토대로 학습된 모델은 news 데이터 셋(XSum, CNN/DailyMail)에서는 높은 성능을 보여주고 있는 반면, non-news 데이터셋(WikiHow, Reddit TIFU)에서는 낮은 성능을 보여주고 있다. EFFECT OF PRE-TRAINING OBJECTIVESGSG의 성능비교를 위해 Lead, Random, Ing-Oig, Ing-Uniq, Seq-Orig, Seq-Uniq를 비교하였으며, GSR의 경우 데이터셋마다 성능 차이를 보이지만, 최종적으로 30%를 선택하였다. EFFECT OF VOCABULARY실험을 위해 BPE(Byte-pair encoding)와 SentencePiece Unigram을 비교하였다.비교결과 news 데이터셋에서는 BPE와 Unigram의 성능이 유사하였지만, non-news 데이터셋(especially WikiHow)에서는 SentencePiece Unigram모델이 훨씬 좋은 성능을 나타냈다. 위 그래프에서 볼 수 있듯이, WikiHow의 경우 Unigram이 128k일 때, Reddit TIFU는 64k일 때 best score를 나타내었기 때문에 이를 고려하여 최종적으로 SentencePiece Unigram을 사용하고 vocabulary size는 96k로 선정하였다. Larger Model위 table에서 볼 수 있듯이, PEGASUS모델은 이전 SOTA모델 대비 모든 12개의 downstream tasks에서 모두 SOTA를 달성한 것을 확인할 수 있다. Zero and Low-Resource SummarizationPAGASUS-large 모델을 2000 steps, 256 batch-size, 0.0005 learning-rate로 fine-tuning하였을 때, 단지 100개의 examples만으로도 기존 20k~200k개로 학습된 Transformer-base모델과 유사한 성능을 달성하였고, 1000개의 examples를 사용하였을때 12개 데이터 셋중 6개의 데이터 셋에서 SOTA를 달성할 만큼 기존 모델 대비 적은 비용으로 높은 성능을 달성하였다는 것이 큰 특징이다. 또한, 실제 사람이 만든 요약본과 PEGASUS-large모델이 만든 요약본은 비교한 결과를 보면, Reddit TIFU 데이터셋을 제외한 XSum, CNN/DailyMail 데이터셋에서는 PEGASUS-large모델이 만든 요약본이 사람이 만든 요약본보다 더 높은 성능을 나타냈다는 것이 특징이다. Conclusion정리해보자면 해당 논문의 큰 특징이라고 할 수 있는 점은, 첫째, Abstractive summmarization이라는 특정 task를 위해 GSG(Gap-Sentence Generation)라는 새로운 pre-training기법을 통해 적용한 점둘째, GSG에서 principal sentence selection을 위해 다양한 방법을 적용한 점셋째, 적은 리소스 비용(ex, 1000 examples)만으로도 대부분의 결과에서 SOTA를 달성한 점 인 것 같다. 그런데 여기서 의문이 들었던 점은 사람의 요약본과 성능 비교를 하는데 있어서 PEGASUS-large모델이 대부분 더 좋은 성능을 보였는데, 과연 human evaluation이 객관적으로 이루어졌는지 의문이 들었다. 각 task마다 3명의 평가자에 의해 1-5점으로 평가를 하였다고 하는데 과연 일반화 할 수 있을까? 여하튼, 최근 text summarization분야를 관심있게 보고 있었는데, summarization task에 최적화된 모델이 나왔다는 점에서 흥미가 갔던 논문이었다. Reference https://arxiv.org/pdf/1912.08777.pdf https://www.youtube.com/watch?v=JhGmeQBbDdA","categories":[],"tags":[{"name":"nlp","slug":"nlp","permalink":"https://jaehyeongan.github.io/tags/nlp/"},{"name":"summarization","slug":"summarization","permalink":"https://jaehyeongan.github.io/tags/summarization/"},{"name":"transformer","slug":"transformer","permalink":"https://jaehyeongan.github.io/tags/transformer/"},{"name":"gap-sentence-generator","slug":"gap-sentence-generator","permalink":"https://jaehyeongan.github.io/tags/gap-sentence-generator/"},{"name":"mlm","slug":"mlm","permalink":"https://jaehyeongan.github.io/tags/mlm/"}]},{"title":"Basic Object-Detection","slug":"Basic-Object-Detection","date":"2020-04-15T09:34:17.000Z","updated":"2020-10-28T15:14:34.471Z","comments":true,"path":"2020/04/15/Basic-Object-Detection/","link":"","permalink":"https://jaehyeongan.github.io/2020/04/15/Basic-Object-Detection/","excerpt":"","text":"IntroInflearn의 딥러닝 컴퓨터 비전 완벽 가이드를 수강하며 공부 목적으로 정리한 글입니다. Computer Vision Techniques Classification(분류) : 이미지에 있는 object가 무엇인지만 판별, 위치 고려 x Localization(발견) : object 판별 및 단 하나의 object 위치를 bounding box로 지정하여 찾음 Detection(발견) : object 판별 및 여러 개의 object들에 대한 위치를 bounding box로 지정하여 찾음 Segmentation(분할) : object 판별 및 Pixel 레벨의 detection을 통해 모든 픽셀의 레이블을 예측 Object DetectionHistory 현재 YOLO 모델이 real-time 예측 측면에서 성능이 나쁘지 않아 실무에서 가장 많이 활용되고 있음 real-time에는 한계가 있으나 가장 성능이 좋은 모델은 RetinaNet Sliding Window 방식을 활용한 초기 object detection object detection의 초기 기법 window를 왼쪽 상단에서부터 오른쪽 하단으로 이동시키면서 object를 detection하는 방식 오브젝트가 없는 역역도 무조건 슬라이딩하며 여러 형태의 window와 scale을 스캔해야 하므로 수행시간 및 성능이 효율적이지 않음 Region Proposal 기법의 등장 이후 활용도가 떨어졌지만 object detection의 기술적 토대 제공 Obejct Detection의 주요 구성 요소 및 문제주요 구성요소 Region Proposal Detection을 위한 Network 구성(feature extraction, network prediction) detection을 위한 요소들(IoU, NMS, mAP, Anchor Box 등) 주요 문제 물체 판별(Classification) + 물체 위치 찾기(Regression)을 동시에 수행해야 함 한 이미지 내에 크기, 색, 생김새가 다양한 object가 섞여 있음 실시간 detection을 위해 시간 성능이 중요 명확하지 않은 이미지가 많음(노이즈 혹은 배경이 전부인 사진 등) 이미지 데이터 셋의 부족 Region Proposal(영역 추정) 목표 : Object가 있을 만한 후보 영역을 찾자! 대표적인 기법이 Selective Search Selective Search Region Proposal의 대표적인 기법 컬러(color), 무늬(texture), 크기(size), 형태(shape) 등에 따라 유사한 region들을 계층적으로 그룹핑 하는 방법 Selective Search 수행 프로세스 초기 수 천개의 개별 Over segmentation된 모든 부분들을 bounding box로 만들어 region proposal 리스트에 추가 컬러(color), 무늬(texture), 크기(size), 형태(shape) 등에 따라 유사한 segment들을 그룹핑 위 과정을 반복하며 최종 그룹핑 된 segment들을 제안 Python을 통한 Selective Search 구현 pip install selectivesearch 를 통해 라이브러리 설치 12345678import selectivesearchimport cv2img = cv2.imread('./image/test.jpg') # 이미지 로드 img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)plt.figure(figsize=(8, 8))plt.imshow(img_rgb)plt.show() 123456#selectivesearch.selective_search()는 이미지의 Region Proposal정보를 반환 _, regions = selectivesearch.selective_search(img_rgb, scale=100, # bounding box scale min_size=2000) # rect의 최소 사이즈regions[:5] 12345[&#123;&apos;rect&apos;: (0, 0, 58, 257), &apos;size&apos;: 7918, &apos;labels&apos;: [0.0]&#125;, &#123;&apos;rect&apos;: (16, 0, 270, 50), &apos;size&apos;: 5110, &apos;labels&apos;: [1.0]&#125;, &#123;&apos;rect&apos;: (284, 0, 90, 420), &apos;size&apos;: 6986, &apos;labels&apos;: [2.0]&#125;, &#123;&apos;rect&apos;: (59, 14, 262, 407), &apos;size&apos;: 3986, &apos;labels&apos;: [3.0]&#125;, &#123;&apos;rect&apos;: (62, 17, 256, 401), &apos;size&apos;: 5282, &apos;labels&apos;: [4.0]&#125;] 반환된 regions 변수는 리스트 타입으로 세부 원소로 딕셔너리를 가지고 있음. rect 키값은 x,y 시작 좌표와 너비, 높이 값을 가지며 이 값이 Detected Object 후보를 나타내는 Bounding box임. size는 Bounding box의 크기 labels는 해당 rect로 지정된 Bounding Box내에 있는 오브젝트들의 고유 ID 아래로 내려갈 수록 특성이 비슷한 것들이 합쳐지고, 너비와 높이 값이 큰 Bounding box이며 하나의 Bounding box에 여러개의 오브젝트가 있을 확률이 커짐. 12345678910111213141516# Bounding Box 시각화 green_rgb = (125, 255, 51)img_rgb_copy = img_rgb.copy()for rect in cand_rects: left = rect[0] top = rect[1] # rect[2], rect[3]은 너비와 높이이므로 우하단 좌표를 구하기 위해 좌상단 좌표에 각각을 더함. right = left + rect[2] bottom = top + rect[3] img_rgb_copy = cv2.rectangle(img_rgb_copy, (left, top), (right, bottom), color=green_rgb, thickness=2) plt.figure(figsize=(8, 8))plt.imshow(img_rgb_copy)plt.show() IoU(Intersection over Union)모델이 예측한 bounding box와 실제 ground truth box가 얼마나 정확하게 겹치는지를 측정하는 지표 아래와 같은 지표로 계산 되며 100%로 정확하게 겹쳐질 때의 값은 1이 됨 IoU 값에 따라 detection 예측 성공 결정 object detection에서 개별 object에 대한 검출 예측이 성공하였는지에 대한 여부를 IoU를 통해 결정 일반적으로 PASCAL VOC Challenge에서 는 IoU가 0.5이상이면 예측 성공했다고 판단 Python을 통한 IoU 계산123456789101112131415def compute_iou(cand_box, gt_box): # Calculate intersection areas x1 = np.maximum(cand_box[0], gt_box[0]) y1 = np.maximum(cand_box[1], gt_box[1]) x2 = np.minimum(cand_box[2], gt_box[2]) y2 = np.minimum(cand_box[3], gt_box[3]) intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0) # width * height (x2에서 x1을 뺀 값이 width, y2에서 y1을 뺀 값이 height 이므로) cand_box_area = (cand_box[2] - cand_box[0]) * (cand_box[3] - cand_box[1]) # width * height gt_box_area = (gt_box[2] - gt_box[0]) * (gt_box[3] - gt_box[1]) # width * height union = cand_box_area + gt_box_area - intersection # 실제box와 예측box의 합에서 intersection을 뺌 iou = intersection / union return iou 실제 bounding box와 후보 bounding box가 있을 때, 둘 중에서 x1과 y1좌표는 max값, x2와 y2좌표는 min값을 선택하게 되면 그 좌표가 Intersection area가 되며 두 개의 box를 더한 후 intersection을 빼준 값이 Union area 마지막으로 intersection을 union으로 나누어 주면 IoU값을 얻을 수 있음 NMS(Non Max Suppression)object detection 시 최대한 object를 놓치지 않기 위해 많은 bounding box를 찾게 되는데, 이렇게 detected 된 수많은 bounding box 중 비슷한 위치에 있는 box를 제거하고 가장 적합한 box를 선택하는 기법 NMS 수행 프로세스 Detected 된 Bounding box별로 특정 Confidence score threshold 이하 bounding box는 먼저 제거 (ex. confidence score threshold &lt; 0.5) 가장 높은 confidence score를 가진 box 순으로 내림차순 정렬하고 아래 로직을 모든 box에 순차적으로 적용 높은 confidence score를 가진 box와 겹치는 다른 box를 모두 조사하여 IoU가 특정 threshold 이상인 box를 모두 제거 (ex. IoU Threshold &gt; 0.4) 남아있는 box만 선택 Confidence score threshold가 높을 수록, IoU Threshold가 낮을 수록 많은 box가 제거 됨 Object Detection 성능 평가mAP(mean Average Precision) 실제 Object가 detected된 재현율(recall)의 변화에 따른 정밀도(precision)의 값을 평균한 성능 수치 정밀도와 재현율 정밀도는 모델이 positive라고 예측한 대상 중 예측 값이 실제 positive 값과 얼마나 일치하는지에 대한 비율(즉, 예측한 object가 실제 object들과 얼마나 일치하는지) 재현율은 실제 positive 값 중 모델이 얼마나 실제 값을 positive라고 예측했는지에 대한 비율(즉, 실제 object를 얼마나 빠드리지 않고 잘 예측했는지) Precision Recall Trade-off : 정밀도와 재현율은 상호 보완적인 관계이므로 어느 한쪽이 높아지면 다른 쪽이 낮아지게 됨 Precision-Recall Curve : confidence threshold의 변화에 따른 정밀도와 재현율의 변화 곡선, 이 곡선의 아랫부분 면적을 AP(Averge Precision, 평균 정밀도)라고 함 AP는 하나의 object에 대한 성능 수치이며, mAP는 여러 object들의 AP를 평균한 값을 의미 Image Resolution / FPS / Detection 성능 상관 관계일반적으로 이미지 해상도(Image Resolution)가 높을 수록 Detection성능이 좋아지지만 이미지를 처리하는 시간(FPS)이 오래걸림 High Resolution -&gt; High Detection Score -&gt; Low FPS Low Resolution -&gt; Low Detection Score -&gt; High FPS Object Detection을 위한 주요 데이터 셋 Pascal-VOC - XML format, 20개의 오브젝트 카테고리 MS-COCO - json format, 80개의 오브젝트 카테고리 Google Open Images - csv format, 600개의 오브젝트 카테고리","categories":[],"tags":[{"name":"objectdetection","slug":"objectdetection","permalink":"https://jaehyeongan.github.io/tags/objectdetection/"},{"name":"region-proposal","slug":"region-proposal","permalink":"https://jaehyeongan.github.io/tags/region-proposal/"},{"name":"selectivesearch","slug":"selectivesearch","permalink":"https://jaehyeongan.github.io/tags/selectivesearch/"},{"name":"IoU","slug":"IoU","permalink":"https://jaehyeongan.github.io/tags/IoU/"},{"name":"NMS","slug":"NMS","permalink":"https://jaehyeongan.github.io/tags/NMS/"},{"name":"mAP","slug":"mAP","permalink":"https://jaehyeongan.github.io/tags/mAP/"}]},{"title":"LSTM Autoencoder for Anomaly Detection","slug":"LSTM-Autoencoder-for-Anomaly-Detection","date":"2020-02-29T09:19:16.000Z","updated":"2020-10-28T15:14:34.475Z","comments":true,"path":"2020/02/29/LSTM-Autoencoder-for-Anomaly-Detection/","link":"","permalink":"https://jaehyeongan.github.io/2020/02/29/LSTM-Autoencoder-for-Anomaly-Detection/","excerpt":"","text":"Intro지난 포스팅(Autoencoder와 LSTM Autoencoder)에 이어 LSTM Autoencoder를 통해 Anomaly Detection하는 방안에 대해 소개하고자 한다. Autoencoder의 경우 보통 이미지의 생성이나 복원에 많이 사용되며 이러한 구조를 이어받아 대표적인 딥러닝 생성 모델인 GAN(Generative Adversarial Network)으로 까지 이어졌는데 이러한 자기 학습 모델은 Anomaly Detection 분야에서도 널리 사용되고 있다.대표적으로 이미지 분야에서도 정상적인 이미지로 모델 학습 후 비정상적인 이미지를 넣어 이를 디코딩 하게 되면 정상 이미지 특성과 디코딩 된 이미지 간의 차이인 재구성 손실(Reconstruction Error)를 계산하게 되는데 이 재구성 손실이 낮은 부분은 정상(normal), 재구성 손실이 높은 부분은 이상(Abnormal)로 판단할 수 있다. 이러한 Anomaly Detection은 이미지 뿐만 아니라 이제부터 살펴보고자 하는 시계열 데이터에도 적용이 가능하다. 예를 들어 특정 설비의 센서를 통해 비정상 신호를 탐지하고자 한다면 Autoencoder를 LSTM 레이어로 구성한다면 이러한 시퀀스 학습이 가능하게 된다. 이를 통해 정상 신호만을 이용하여 모델을 학습시켜 추후 비정상 신호가 모델에 입력되면 높은 reconstruction error를 나타낼 것이므로 이를 비정상 신호로 판단할 수 있게 된다. LSTM Autoencoder LSTM Autoencoder는 시퀀스(sequence) 데이터에 Encoder-Decoder LSTM 아키텍처를 적용하여 구현한 오토인코더이다. 모델에 입력 시퀀스가 순차적으로 들어오게 되고, 마지막 입력 시퀀스가 들어온 후 디코더는 입력 시퀀스를 재생성하거나 혹은 목표 시퀀스에 대한 예측을 출력한다.위에서 설명한 것과 마찬가지로 LSTM Autoencoder 학습 시에는 정상(normal) 신호의 데이터로만 모델을 학습시키게 된다. encoder와 decoder는 학습이 진행될 수 록 정상 신호를 더 정상 신호 답게 표현하는 방법을 학습하게 될 것이며 최종적으로 재구성 한 결과도 정상 신호와 매우 유사한 분포를 가지는 데이터일 것이다. 그렇기 때문에 이 모델에 비정상 신호를 입력으로 넣게 되면 정상 분포와 다른 특성의 분포를 나타낼 것이기 때문에 높은 reconstruction error를 보이게 될 것이다. Curve Shifting을 적용한 LSTM Autoencoder 전체 프로세스는 위 아키텍처와 같다. 먼저 Curve Shifting을 통해 데이터의 시점을 변환해주고 normal 데이터만을 통해 LSTM Autoencoder 모델을 학습시키게 된다. 그 후 재구성 손실을 계산 후 Precision Recall Curve를 통해 normal/abnormal을 구분하기 위한 threshold를 지정하게 되고 이 threshold를 기준으로 마지막으로 테스트 셋의 재구성 손실을 분류하여 t+n 시점을 예측하게 된다.각 부분에 대해 아래에서 좀 더 상세히 살펴보자. 1. Curve Shifting비정상 신호를 탐지하기 위해서는 비정상 신호가 들어오기 전에 즉, 뭔가 고장 혹은 결함이 발생하기 전에 미리 예측을 해야만 한다. 그렇기 때문에 단순히 현재 시점의 error를 계산하여 비정상 신호를 탐지하는 것은 이미 고장이 발생한 후 예측하는 것과 다름이 없기 때문에 데이터에 대한 시점 변환이 꼭 필요하다. 이러한 future value 예측을 위해 다양한 방법이 있는데 여기서는 Curve Shifting이라는 기법을 적용할 것이다. Curve Shifting은 사전 예측 개념을 적용하기 위한 Shifting 방법이다. 예를 들어 위 그림과 같이 비정상 신호(1)를 2일 전에 조기 예측 하고자 한다면 단순히 Y값을 두 칸씩 내리는 것이 아니라 비정상 신호(1)가 있는 날짜로부터 2일 전까지의 데이터를 비정상 신호(1)로 바꾸어주는 것이다. 이는 비정상 신호가 발생하기 전 어떠한 조짐이 있을 것이며 이러한 조짐이 데이터 특성에 나타날 것이라는 가정을 가지고 학습하는 방법이다.그리고 나서 본래 비정상 신호(1) 데이터를 제거해주는데 이렇게 하는 이유는 라벨을 바꿔주는 순간 이는 비정상 신호 예측 문제가 아닌 비정상 신호 조짐 예측 문제가 되는 것이 때문에 데이터의 학습 혼동을 없애주기 위해 제거하는 것이라 보면 될 것이다. 2. Threshold by Precision-Recall-CurveAutoencoder는 재구성 된 결과를 intput과 비교하여 재구성 손실(Reconstruction Error)를 계산한다고 말했다. 그리고 이 재구성 손실값을 통해 손실값이 낮으면 정상으로, 손실값이 높으면 이상으로 판단한다고 하였는데, 이 정상과 이상을 나누는 기준은 과연 무엇일까?일반적으로 모델이 정상 데이터만으로 학습을 하여 정상 데이터를 재구성하였을 때 학습이 잘 되었다고 가정하면 손실값은 0에 가까울 것이고, 학습이 잘 안되었다고 하면 손실값은 1에 가까울 것이다. 보통 분류(Classification)문제에서는 예측 확률값(0% ~ 100%)을 통해 50%를 기준으로 분류를 하게 되는데, 이 recontruction error의 경우 그렇게 극단적으로 값이 튀기는 힘들기 때문에 정상과 이상을 분리하는 타당한 threshold값을 정하는 것이 필요하다. Precision Recall Curve 위와 같은 문제의 적절한 threshold값을 적용하기 위한 방법 중 하나로 precision recall curve가 있다. 이는 Recall(재현율)과 Precision(정밀도)가 서로 Trade off 관계를 가지기 때문에 어느 한쪽에 치우지지 않는 최적의 threshold를 구하기 위한 방법이다.추후 이 검증 기법을 적용하여 LSTM Autoencoder를 통해 재구성 된 정상 신호와 비정상 신호를 구분하기 위한 적절한 threshold를 찾아낼 것이다. Implementation적용해 볼 데이터는 펄프 제지 공장의 Sheet breaks(종이 씹힘)에 관한 이진 라벨 데이터이다. 데이터 설명에 따르면 해당 공장에서 한번 sheet break가 발생하면 수천 달러의 손해가 발생한다고 하며, 이러한 사고가 적어도 매일 한 번 이상 발생한다고 한다.해당 데이터는 15일치에 해당하는 18,268 rows를 가지고 있으며 이 중 sheet break에 해당하는 positive label의 비율은 124개로 전체 데이터의 0.6%를 차지하고 있다.데이터는 여기에서 신청 후 받을 수 있다. 1. Import Libraries12345678910111213import pandas as pd import numpy as npimport matplotlib.pyplot as pltimport seaborn as snsfrom pylab import rcParamsfrom collections import Counterimport tensorflow as tffrom tensorflow.keras import Model ,models, layers, optimizers, regularizersfrom tensorflow.keras.callbacks import ModelCheckpointfrom sklearn.preprocessing import StandardScalerfrom sklearn.model_selection import train_test_splitfrom sklearn import metrics 2. Load Datatime과 라벨 y값을 빼면 총 61개의 칼럼을 가지고 있다.1234LABELS = ['Normal', 'Break']df = pd.read_csv('./data/processminer-rare-event-mts-csv.csv')df.head() normal(0)이 18,274건, break(1)가 124건으로 구성 되어있다.1Counter(df['y']) # Counter(&#123;0: 18274, 1: 124&#125;) 3. Curve Shiftingtime 칼럼을 보면 2분 단위로 데이터가 나누어져 있는 것을 알 수 있다. 여기서의 목표는 break가 발생하기 4분 전에 조기 예측하는 것이다. 그러므로 4분 전까지의 데이터를 break 데이터로 만들기 위해서는 curve shifting을 2개의 row만큼만 적용하면 된다. 이후, 본래 break 데이터는 제거한다.12345678910111213141516171819202122sign = lambda x: (1, -1)[x &lt; 0]def curve_shift(df, shift_by): vector = df['y'].copy() for s in range(abs(shift_by)): tmp = vector.shift(sign(shift_by)) tmp = tmp.fillna(0) vector += tmp labelcol = 'y' # Add vector to the df df.insert(loc=0, column=labelcol+'tmp', value=vector) # Remove the rows with labelcol == 1. df = df.drop(df[df[labelcol] == 1].index) # Drop labelcol and rename the tmp col as labelcol df = df.drop(labelcol, axis=1) df = df.rename(columns=&#123;labelcol+'tmp': labelcol&#125;) # Make the labelcol binary df.loc[df[labelcol] &gt; 0, labelcol] = 1 return df 123# shift the response column y by 2 rows to do a 4-min ahead predictionshifted_df = curve_shift(df, shift_by=-5)shifted_df.head() 몇 가지 불필요한 데이터는 제거한 후, 데이터와 라벨을 분리해준다.12345678# drop remove columnsshifted_df = shifted_df.drop(['time','x28','x61'], axis=1)# x, yinput_x = shifted_df.drop('y', axis=1).valuesinput_y = shifted_df['y'].valuesn_features = input_x.shape[1] 4. Transform to Series DataLSTM 모델은 (samples, timesteps, feature)에 해당하는 3d 차원의 shape을 가지므로, 데이터를 시퀀스 형태로 변환한다. timesteps은 5(즉, 10분)만큼 잡았다.1234567891011def temporalize(X, y, timesteps): output_X = [] output_y = [] for i in range(len(X) - timesteps - 1): t = [] for j in range(1, timesteps + 1): # Gather the past records upto the lookback period t.append(X[[(i + j + 1)], :]) output_X.append(t) output_y.append(y[i + timesteps + 1]) return np.squeeze(np.array(output_X)), np.array(output_y) 12345timesteps = 5# Temporalizex, y = temporalize(input_x, input_y, timesteps)print(x.shape) # (18268, 5, 59) 5. Split Train / Valid / Test이후, 훈련, 검증, 테스트 용 데이터로 분리한다. 각각 11,691, 2,923, 3,654개로 나누어주었다.1234567# Split into train, valid, and test x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2)print(len(x_train)) # 11691print(len(x_valid)) # 2923print(len(x_test)) # 3654 LSTM Autoencoder 학습 시에는 Normal(0) 데이터만으로 학습할 것이기 때문에 데이터로 부터 Normal(0)과 Break(1) 데이터를 분리한다.123456# For training the autoencoder, split 0 / 1x_train_y0 = x_train[y_train == 0]x_train_y1 = x_train[y_train == 1]x_valid_y0 = x_valid[y_valid == 0]x_valid_y1 = x_valid[y_valid == 1] 6. Standardize각기 다른 데이터 특성의 표준화를 위해 z-score 정규화인 scikit-learn의 StandardScaler()를 적용하였다. 해당 함수를 적용하기 위해서는 2d 형태여야 하므로 Flatten 과정을 거친 후 스케일을 적용하였으며 이후 다시 3d 형태로 변환하였다.1234567891011def flatten(X): flattened_X = np.empty((X.shape[0], X.shape[2])) # sample x features array. for i in range(X.shape[0]): flattened_X[i] = X[i, (X.shape[1]-1), :] return(flattened_X)def scale(X, scaler): for i in range(X.shape[0]): X[i, :, :] = scaler.transform(X[i, :, :]) return X 123456scaler = StandardScaler().fit(flatten(x_train_y0))x_train_y0_scaled = scale(x_train_y0, scaler)x_valid_scaled = scale(x_valid, scaler)x_valid_y0_scaled = scale(x_valid_y0, scaler)x_test_scaled = scale(x_test, scaler) 7. Training LSTM Autoencoder대칭 구조의 Staked Autoencoder 형태로 LSTM Autoencoder를 구성하여 정상 데이터로만 구성 된 데이터를 통해 총 200 epoch 학습시켰다.123epochs = 200batch = 128lr = 0.001 1234567891011lstm_ae = models.Sequential()# Encoderlstm_ae.add(layers.LSTM(32, activation='relu', input_shape=(timesteps, n_features), return_sequences=True))lstm_ae.add(layers.LSTM(16, activation='relu', return_sequences=False))lstm_ae.add(layers.RepeatVector(timesteps))# Decoderlstm_ae.add(layers.LSTM(16, activation='relu', return_sequences=True))lstm_ae.add(layers.LSTM(32, activation='relu', return_sequences=True))lstm_ae.add(layers.TimeDistributed(layers.Dense(n_features)))lstm_ae.summary() 1234567# compilelstm_ae.compile(loss='mse', optimizer=optimizers.Adam(lr))# fithistory = lstm_ae.fit(x_train_y0_scaled, x_train_y0_scaled, epochs=epochs, batch_size=batch, validation_data=(x_valid_y0_scaled, x_valid_y0_scaled)) 12345678910111213141516Train on 11314 samples, validate on 2830 samplesEpoch 1/20011314/11314 [==============================] - 4s 393us/sample - loss: 0.8505 - val_loss: 0.6345Epoch 2/20011314/11314 [==============================] - 1s 86us/sample - loss: 0.5249 - val_loss: 0.4738Epoch 3/20011314/11314 [==============================] - 1s 83us/sample - loss: 0.4049 - val_loss: 0.3784 : : : : Epoch 198/20011314/11314 [==============================] - 1s 94us/sample - loss: 0.1256 - val_loss: 0.1308Epoch 199/20011314/11314 [==============================] - 1s 97us/sample - loss: 0.1209 - val_loss: 0.1282Epoch 200/20011314/11314 [==============================] - 1s 96us/sample - loss: 0.1212 - val_loss: 0.1308 train loss와 valid loss 모두 0.1근처로 수렴하고 있다.12345plt.plot(history.history['loss'], label='train loss')plt.plot(history.history['val_loss'], label='valid loss')plt.legend()plt.xlabel('Epoch'); plt.ylabel('loss')plt.show() 8. Threshold by Precision Recall Curvenormal과 break를 구분하기 위한 threshold를 지정하기 위해 precision recall curve를 적용한다. 주의해야할 것은 디코딩 된 재구성 결과가 아닌 재구성 손실(reconstruction error)와 실제 라벨 값을 비교한다는 것이다. 12345678910111213valid_x_predictions = lstm_ae.predict(x_valid_scaled)mse = np.mean(np.power(flatten(x_valid_scaled) - flatten(valid_x_predictions), 2), axis=1)error_df = pd.DataFrame(&#123;'Reconstruction_error':mse, 'True_class':list(y_valid)&#125;)precision_rt, recall_rt, threshold_rt = metrics.precision_recall_curve(error_df['True_class'], error_df['Reconstruction_error'])plt.figure(figsize=(8,5))plt.plot(threshold_rt, precision_rt[1:], label='Precision')plt.plot(threshold_rt, recall_rt[1:], label='Recall')plt.xlabel('Threshold'); plt.ylabel('Precision/Recall')plt.legend()plt.show() 여기서 threshold의 경우 Recall과 Precision의 값이 교차되는 지점을 최적의 threshold 지점으로 잡았다.여기서 최적의 threshold는 0.407이다.1234567# best position of thresholdindex_cnt = [cnt for cnt, (p, r) in enumerate(zip(precision_rt, recall_rt)) if p==r][0]print('precision: ',precision_rt[index_cnt],', recall: ',recall_rt[index_cnt])# fixed Thresholdthreshold_fixed = threshold_rt[index_cnt]print('threshold: ',threshold_fixed) 12precision: 0.10752688172043011 , recall: 0.10752688172043011threshold: 0.40777142413843237 9. Predict Test이제 테스트 셋에 적용해볼 차례이다. 학습하였던 LSTM Autoencoder 모델을 통해 테스트 셋을 예측 후 재구성 손실을 계산한다. 그 후 위에서 찾은 threshold를 적용하여 Normal과 Break를 구분한다.123456789101112131415161718test_x_predictions = lstm_ae.predict(x_test_scaled)mse = np.mean(np.power(flatten(x_test_scaled) - flatten(test_x_predictions), 2), axis=1)error_df = pd.DataFrame(&#123;'Reconstruction_error': mse, 'True_class': y_test.tolist()&#125;)groups = error_df.groupby('True_class')fig, ax = plt.subplots()for name, group in groups: ax.plot(group.index, group.Reconstruction_error, marker='o', ms=3.5, linestyle='', label= \"Break\" if name == 1 else \"Normal\")ax.hlines(threshold_fixed, ax.get_xlim()[0], ax.get_xlim()[1], colors=\"r\", zorder=100, label='Threshold')ax.legend()plt.title(\"Reconstruction error for different classes\")plt.ylabel(\"Reconstruction error\")plt.xlabel(\"Data point index\")plt.show(); 10. Evaluationconfusion matrix테스트 셋에 대한 재구성 손실을 threshold를 기준으로 0/1로 나누고 이를 confusion matrix로 표현하였다.Break에 대한 예측 결과가 실망스러울 수 있지만 이렇게 Sheet Break의 10%만 줄여도 엄청난 손실을 줄일 수 있다고 한다.123456789# classification by thresholdpred_y = [1 if e &gt; threshold_fixed else 0 for e in error_df['Reconstruction_error'].values]conf_matrix = metrics.confusion_matrix(error_df['True_class'], pred_y)plt.figure(figsize=(7, 7))sns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt='d')plt.title('Confusion Matrix')plt.xlabel('Predicted Class'); plt.ylabel('True Class')plt.show() ROC Curve and AUC123456789101112false_pos_rate, true_pos_rate, thresholds = metrics.roc_curve(error_df['True_class'], error_df['Reconstruction_error'])roc_auc = metrics.auc(false_pos_rate, true_pos_rate,)plt.plot(false_pos_rate, true_pos_rate, linewidth=5, label='AUC = %0.3f'% roc_auc)plt.plot([0,1],[0,1], linewidth=5)plt.xlim([-0.01, 1])plt.ylim([0, 1.01])plt.legend(loc='lower right')plt.title('Receiver operating characteristic curve (ROC)')plt.ylabel('True Positive Rate'); plt.xlabel('False Positive Rate')plt.show() 11. Result최종적으로 테스트 셋에 대한 재구성 손실을 threshold를 통해 구분한 pred_y의 마지막 5번째(timestep만큼)을 출력하여 예측 결과를 확인할 수 있다.1pred_y[-5:] # [0, 0, 1, 0, 0] 위 결과를 해석하기가 애매모호한 부분이 있지만, 대략 넓게 잡았을 때 최소 10분 이내에는 Break가 발생할 것 같다고 해석할 수 있을 것이다. References Extreme Rare Event Classification using Autoencoders in Keras LSTM Autoencoder for Extreme Rare Event Classification in Keras Understanding LSTM Networks LSTM TimeDistributed layer How to connect LSTM layers in Keras, RepeatVector or return_sequence=True?","categories":[],"tags":[{"name":"autoencoder","slug":"autoencoder","permalink":"https://jaehyeongan.github.io/tags/autoencoder/"},{"name":"lstm","slug":"lstm","permalink":"https://jaehyeongan.github.io/tags/lstm/"},{"name":"deeplearning","slug":"deeplearning","permalink":"https://jaehyeongan.github.io/tags/deeplearning/"},{"name":"machinelearning","slug":"machinelearning","permalink":"https://jaehyeongan.github.io/tags/machinelearning/"},{"name":"lstmautoencoder","slug":"lstmautoencoder","permalink":"https://jaehyeongan.github.io/tags/lstmautoencoder/"},{"name":"prediction","slug":"prediction","permalink":"https://jaehyeongan.github.io/tags/prediction/"},{"name":"shifting","slug":"shifting","permalink":"https://jaehyeongan.github.io/tags/shifting/"},{"name":"windowing","slug":"windowing","permalink":"https://jaehyeongan.github.io/tags/windowing/"}]},{"title":"Autoencoder와 LSTM Autoencoder","slug":"Autoencoder-LSTMautoencoder","date":"2020-02-28T14:26:57.000Z","updated":"2020-10-28T15:14:34.471Z","comments":true,"path":"2020/02/28/Autoencoder-LSTMautoencoder/","link":"","permalink":"https://jaehyeongan.github.io/2020/02/28/Autoencoder-LSTMautoencoder/","excerpt":"","text":"Intro대표적인 자기 지도 학습인 Autoencoder와 Autoencoder에 LSTM cell을 적용해 시퀀스 학습이 가능한 LSTM Autoencoder에 대해 소개한다. 이후 다음 포스팅에는 LSTM Autoencoder를 통해 미래에 발생 할 고장이나 이상신호를 조기 예측하기 위한 Anomaly Detection 방안에 대해 소개할 것이다. 1. Autoencoder?오토인코더는(autoencoder)는 라벨이 없는 훈련 데이터를 사용한 학습(즉, 지도 학습) 없이도 입력 데이터의 표현을 효율적으로 학습할 수 있는 인공신경망이다. 오토인코더는 아래 그림과 같이 input 레이어, hidden 레이어, output 레이어로 구성되어 있으며 일반적으로 Input 유닛보다 훨씬 낮은 차원의 hidden 유닛을 가지므로 주로 차원 축소(Dimensionality Reduction) 목적으로 사용된다. 또한 오토인코더는 강력한 feature extractor로 작동하기 때문에 비지도 사전훈련에 사용될 수 있고, 훈련 데이터와 매우 비슷한 새로운 데이터를 생성하는 생성 모델(generative model)로서 사용될 수 있다. 오토인코더가 학습하는 것은 단순히 입력을 출력으로 복사하는 것이다. 하지만 그 과정에서 여러 방법의 제약(내부 표현 크기 제한, 입력 잡음 추가 등)을 통해 오토인코더가 단순히 입력을 바로 출력으로 복사하지 못하도록 막고, 데이터를 효율적으로 재표현(representation)하는 방법을 학습하도록 제어한다. 오토인코더는 인코더(encoder)와 디코더(decoder)로 구분된다. 인코더(encoder) : 인지 네트워크(recognition network)라고도 하며, 입력을 내부 표현으로 변환 디코더(decoder) : 생성 네트워크(generative network)라고도 하며, 내부 표현을 출력으로 변환 오토인코더가 입력을 재구성하기 때문에 출력을 재구성(reconstruction)이라고 부르며, 입력과 재구성된 출력과의 차이를 계산하여 재구성 손실(reconstruction loss)이라고 한다. 여기서 파라미터 (θ,φ)는 encoder에 입력되는 original input (x)과 디코더를 통해 출력 된 reconstruced input (f(g(x))이 같아지도록 학습하며 업데이트 된다. 1.1. Stacked Autoencoder여러 개의 hidden 레이어를 가진 경우를 적층 오토인코더(stacked autoencoder)라고 한다. 아래 그림와 같이 레이어를 더 추가할 경우 오토인코더는 더 복작한 표현을 학습할 수 있게 되며 일반적으로 적층 오토인코더는 추가된 hideen 레이어를 기준으로 인코더와 디코더는 대칭 구조를 이룬다. 위와 같이 오토인코더가 완벽하게 대칭 구조를 이룰 때는 일반적으로 인코더와 디코더의 가중치를 묶게 되는데 이렇게 할 경우 모델의 가중치 수를 절반으로 줄여 훈련속도를 높이고 overfitting 위험을 줄일 수 있다고 한다. 1.2. Denoising Autoencoder위에서 살펴보았던 Stacked Autoencoder의 경우 다수의 hidden 레이어와 노드가 추가 될 경우 overfitting 자신에 대한 표현을 세밀하게 학습하게 되는 overfitting 문제에 직면할 수 있다. 이를 해결하기 위한 한 가지 방법으로 제안된 것이 Denoising Autoencoder(Vincent et al. 2008)이다. 이 모델은 말 그대로 모델에 학습되기 전 Input 데이터에 잡음(noise)을 주어 모델이 데이터 표현을 학습하기 힘들게 만든다. 이렇게 하는 이유는 모델을 일반화하기 위한 목적이며, 노이즈 즉, 제약이 있는 상황에서도 데이터를 효울적으로 복원하기 위함이다. 이때 잡음을 주기 위한 방법은 여러가지가 있지만 해당 논문에서는 아래와 같이 데이터의 일부가 삭제된 input(x~) 를 넣어 이 x~가 출력 된 reconstruced input(x’)과 유사해지도록 학습하는 것이다. 2. LSTM AutoencoerLSTM Autoencoder는 시퀀스(sequence) 데이터에 Encoder-Decoder LSTM 아키텍처를 적용하여 구현한 오토인코더이다. 아래 그림은 LSTM 오토인코더의 구조이며 입력 시퀀스가 순차적으로 들어오게 되고, 마지막 입력 시퀀스가 들어온 후 디코더는 입력 시퀀스를 재생성하거나 혹은 목표 시퀀스에 대한 예측을 출력한다. 2.1 Reconstruction LSTM Autoencoder재구성(reconstruction)을 위한 LSTM Autoencoder 구조이다. 즉, input과 최대한 유사하게 output을 디코딩하며, LSTM 학습을 위해 데이터를 우선 (samples, timesteps, feature)와 같은 3d형태로 변환한다. input 레이어의 feature는 1차원으므로 output 레이어도 동일한 차원으로 구성하여 출력되도록 한다. 1234import pandas as pd import numpy as npimport tensorflow as tffrom tensorflow.keras import Model ,models, layers, optimizers, utils 123456789101112131415161718192021# define input sequencesequence = np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])# reshape input into [samples, timesteps, features]n_in = len(sequence)sequence = sequence.reshape((1, n_in, 1))# define modelmodel = models.Sequential()model.add(layers.LSTM(100, activation='relu', input_shape=(n_in, 1)))model.add(layers.RepeatVector(n_in))model.add(layers.LSTM(100, activation='relu', return_sequences=True))model.add(layers.TimeDistributed(layers.Dense(1)))model.compile(optimizer='adam', loss='mse')# fit modelmodel.fit(sequence, sequence, epochs=300, verbose=0)# predictyhat = model.predict(sequence)yhat 123456789array([[[0.10559099], [0.20217314], [0.30041453], [0.39952287], [0.49908453], [0.5987617 ], [0.69832975], [0.7991052 ], [0.9024458 ]]], dtype=float32) 2.2 Prediction LSTM Autoencoder시계열적 예측을 위한 LSTM 구조이며 input 시퀀스는 현재 시점(t) output 시점은 (t+1)로 두어 한 시점 앞을 학습하도록 데이터를 구성한다. 여기서 autoencoder는 학습 시 encoder에는 t 시점이 입력되지만 decoding 후에는 (t+1)시점과 reconstruction error를 계산하며 결국 t 시점이 t+1 시점을 학습하게 된다.결과적으로 예측 결과는 1이 입력되면 2와 가까운 수를, 2가 입력되면 3과 가까운 수를 예측하게 된다.12345678910111213141516171819202122232425# define input sequenceseq_in = np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])# reshape input into [samples, timesteps, features]n_in = len(seq_in)seq_in = seq_in.reshape((1, n_in, 1))# prepare output sequenceseq_out = seq_in[:, 1:, :]n_out = n_in - 1# define model model = models.Sequential()model.add(layers.LSTM(100, activation='relu', input_shape=(n_in, 1)))model.add(layers.RepeatVector(n_out))model.add(layers.LSTM(100, activation='relu', return_sequences=True))model.add(layers.TimeDistributed(layers.Dense(1)))model.compile(optimizer='adam', loss='mse')# fit modelmodel.fit(seq_in, seq_out, epochs=300, verbose=0)# predictyhat = model.predict(seq_in)yhat 12345678array([[[0.16683361], [0.2898971 ], [0.403169 ], [0.5089176 ], [0.6094323 ], [0.7060289 ], [0.7997408 ], [0.89148134]]], dtype=float32) 2.3 Composite LSTM AutoencoderReconstruction과 Prediction 모델을 통합한 모델이다. 모델의 통합을 위해 예제에서는 keras functional api를 활용하였으며, 결과적으로 출력 시 reconstruction결과와 prediction결과가 함께 출력된다.123456789101112131415161718192021222324252627282930313233# define input sequenceseq_in = np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])# reshape input into [samples, timesteps, features]n_in = len(seq_in)seq_in = seq_in.reshape((1, n_in, 1))# prepare output sequenceseq_out = seq_in[:, 1:, :]n_out = n_in - 1# define encodervisible = layers.Input(shape=(n_in, 1))encoder = layers.LSTM(100, activation='relu')(visible)# define reconstruct decoderdecoder1 = layers.RepeatVector(n_in)(encoder)decoder1 = layers.LSTM(100, activation='relu', return_sequences=True)(decoder1)decoder1 = layers.TimeDistributed(layers.Dense(1))(decoder1)# define predict decoderdecoder2 = layers.RepeatVector(n_out)(encoder)decoder2 = layers.LSTM(100, activation='relu', return_sequences=True)(decoder2)decoder2 = layers.TimeDistributed(layers.Dense(1))(decoder2)# concat modelmodel = Model(inputs=visible, outputs=[decoder1, decoder2])model.compile(optimizer='adam', loss='mse')# utils.plot_model(model, show_shapes=True, to_file='composite_lstm_autoencoder.png')# fit model model.fit(seq_in, [seq_in, seq_out], epochs=300, verbose=0)# predictyhat = model.predict(seq_in)yhat 1234567891011121314151617[array([[[0.10127164], [0.19949059], [0.29943317], [0.39987874], [0.50023794], [0.60028654], [0.7000689 ], [0.79983366], [0.89999163]]], dtype=float32), array([[[0.19868489], [0.30206183], [0.3981459 ], [0.4989811 ], [0.600592 ], [0.7013527 ], [0.80077535], [0.8988221 ]]], dtype=float32)] References Hands-On Machine Learning with Scikit-Learn and TensorFlow Unsupervised Learning of Video Representations using LSTMs https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html https://machinelearningmastery.com/lstm-autoencoders/","categories":[],"tags":[{"name":"autoencoder","slug":"autoencoder","permalink":"https://jaehyeongan.github.io/tags/autoencoder/"},{"name":"rnn","slug":"rnn","permalink":"https://jaehyeongan.github.io/tags/rnn/"},{"name":"lstm","slug":"lstm","permalink":"https://jaehyeongan.github.io/tags/lstm/"},{"name":"reconstruction","slug":"reconstruction","permalink":"https://jaehyeongan.github.io/tags/reconstruction/"},{"name":"encoder","slug":"encoder","permalink":"https://jaehyeongan.github.io/tags/encoder/"},{"name":"decoder","slug":"decoder","permalink":"https://jaehyeongan.github.io/tags/decoder/"}]},{"title":"OpenCV를 활용한 기초 이미지 처리 with Python","slug":"OpenCV를-활용한-기초-이미지-처리","date":"2020-02-15T09:01:24.000Z","updated":"2020-10-28T15:14:34.476Z","comments":true,"path":"2020/02/15/OpenCV를-활용한-기초-이미지-처리/","link":"","permalink":"https://jaehyeongan.github.io/2020/02/15/OpenCV를-활용한-기초-이미지-처리/","excerpt":"","text":"Intro머신러닝 분야에서 가장 활발하게 연구 되고 있는 분야는 아무래도 컴퓨터 비전(computer vision)분야 인 것 같다.최근 컨볼루션 네트워크 모델들은 feature extraction 능력이 매우 뛰어나서 이미지에 추가적인 전처리 작업을 하지 않더라도 뛰어난 성능을 내고 있다. 하지만 그렇더라도 더 효과적인 모델을 위해서는 적용하고자 하는 목적에 맞게 이미지 전처리 작업을 거쳐야 하는 경우가 있다. 이번 글은 ‘파이썬을 활용한 머신러닝 쿡북 - CHAPTER 8 이미지 다루기’를 읽고 정리한 글이며, OpenCV를 활용한 다양한 이미지 처리 기술에 대해 소개한다. 1. OpenCV 설치OpenCV(Open Source Computer Vision Libary)는 이미지를 다루는 분야에서 가장 널리 이용되고 인기 있는 라이브러리이며, 이미지를 처리하기 위한 편리한 기능을 대부분 담고 있다. 아래의 명령어를 통해 설치가 가능하다.1pip install opencv-python 설치가 제대로 되었는지 OpenCV를 import하여 버전을 확인한다.12import cv2cv2.__version__ # 4.1.2 2. 이미지 로드여기서 활용하는 샘플 이미지는 해당 책의 github 에서 다운받을 수 있다. 먼저 앞으로 공통적으로 계속 사용 될 라이브러리를 임포트한다.123import cv2import numpy as npimport matplotlib.pyplot as plt imread() 메소드를 통해 이미지를 로드 후 matplotlib을 통해 출력해본다.123image = cv2.imread('images/plane.jpg', cv2.IMREAD_GRAYSCALE)plt.imshow(); plt.show() 위 이미지의 type 및 shape을 출력해보면 아래와 같다.12image.type # numpy.ndarrayimage.shape # (2270, 3600) 이미지 데이터는 본래 개별 원소로 이루어진 행렬의 집합이다. 여기서 개별 원소는 픽셀(pixel)이라고 할 수 있으며 개별 원소의 값은 픽셀의 강도라고 할 수 있다. 그리고 픽셀의 강도는 0(검정)부터 255(흰색) 사이의 범위를 가지고 있다. 이미지를 행렬 그대로 출력하게 되면 아래와 같이 표현된다.1image 1234567array([[140, 136, 146, ..., 132, 139, 134], [144, 136, 149, ..., 142, 124, 126], [152, 139, 144, ..., 121, 127, 134], ..., [156, 146, 144, ..., 157, 154, 151], [146, 150, 147, ..., 156, 158, 157], [143, 138, 147, ..., 156, 157, 157]], dtype=uint8) 컬러를 이미지를 읽기 위해서는 imread() 메소드에 cv2.IMREAD_COLOR 매개변수를 넣어주면 된다. 그런데 주의할점은 OpenCV는 기본적으로 이미지를 BGR타입으로 읽는다는 것이다. 하지만 Matplotlib등 대부분의 이미지 라이브러리는 RGB타입을 사용하기 때문에 BGR RGB타입으로 변경해주는 것이 좋다. 12345678# 컬러 이미지 로드image_bgr = cv2.imread('images/plane.jpg', cv2.IMREAD_COLOR)# RGB타입으로 변환image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)# plotplt.imshow(); plt.show() 3. 이미지 저장OpenCV의 imwrite() 메소드를 사용하여 이미지를 저장할 수 있다. 12# 이미지 로드 image = cv2.imread('images/plane.jpg', cv.IMREAD_GRAYSCALE) 12# 이미지 저장 cv2.imwrite('images/new_plane.jpg', image) 4. 이미지 크기 변경OpenCV의 resize() 메소드를 이용하여 이미지 크기 변경이 가능하다.256x256 크기의 이미지를 로드한 후 이를 50x50 크기의 이미지로 변경한 후 출력해본다.1image = cv2.imread('images/plane_256x256.jpg', cv2.IMREAD_GRAYSCALE) 123456789# 이미지 크기를 50x50으로 변경image_50x50 = cv2.resize(image, (50, 50))# 출력 fig, ax = plt.subplots(1,2, figsize=(10,5))ax[0].imshow(image, cmap='gray')ax[0].set_title('Original Image')ax[1].imshow(image_50x50, cmap='gray')ax[1].set_title('Resized Image') 5. 이미지 자르기(crop)이미지를 자르고 싶을 경우 배열 슬라이싱을 이용하여 원하는 부분만 crop할 수 있다.1234567image = cv2.imread('images/plane_256x256.jpg', cv2.IMREAD_GRAYSCALE)# 이미지의 모든 행과 열의 절반만 선택image_cropped = image[:,:128]plt.imshow(image_cropped, cmap='gray')plt.show() 6. 이미지 blur 처리이미지를 흐리게 하기 위해서는 각 픽셀을 주변 픽셀의 평균값으로 변환하면 되며, 이렇게 주변 픽셀에 수행되는 연산을 커널(kernel)이라고 한다. 커널이 클수록 이미지가 더 부드러워지게 된다. 12# 이미지 로드 image = cv2.imread('images/plane_256x256.jpg', cv2.IMREAD_GRAYSCALE) 12345# blur() : 각 픽셀에 커널 개수의 역수를 곱하여 모두 더함image_blurry = cv2.blur(image, (5,5)) # 5 x 5 커널 평균값으로 이미지를 흐리게 함 plt.imshow(image_blurry, cmap='gray')plt.show() 100x100 커널과 같이 큰 커널을 적용할 경우 이미지가 훨씬 뭉개지게 된다.1234image_very_blurry = cv2.blur(image, (100,100))plt.imshow(image_very_blurry, cmap='gray')plt.show() 아래와 같이 커널을 직접 정의한 후 filter2D() 메소드를 통해 이미지에 적용하는 것도 가능하다.생성된 커널을 이미지에 적용 시 중앙 원소가 변환되는 픽셀이며, 나머지는 그 픽셀의 이웃이 된다.123# 커널 생성 kernel = np.ones((10,10)) / 25.0 # 모두 더하면 1이 되도록 정규화kernel 12345678910array([[0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04], [0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04], [0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04], [0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04], [0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04], [0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04], [0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04], [0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04], [0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04], [0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04]]) 12345# filter2D함수로 커널을 이미지에 직접 적용 image_kernel = cv2.filter2D(image, -1, kernel)plt.imshow(image_kernel, cmap='gray')plt.show() 자주 사용되는 블러 함수로 가우시안 분포를 사용하는 가우시안 블러(GaussianBlur)가 있다. GaussianBlur() 함수의 세 번째 매개변수는 X축(너비) 방향의 표준편차이며, 0으로 지정하면 ((너비-1)0.5-1)0.3+0.8과 같이 계산된다. 1234image_very_blurry = cv2.GaussianBlur(image, (5,5), 0) plt.imshow(image_very_blurry, cmap='gray')plt.show() 7. 이미지 선명하게 표현대상 픽셀을 강조하는 커널을 정의한 후 filter2D() 메소드를 사용하여 이미지에 적용한다.123456789101112131415image = cv2.imread('images/plane_256x256.jpg', cv2.IMREAD_GRAYSCALE)# 커널 생성(대상이 있는 픽셀을 강조)kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])# 커널 적용 image_sharp = cv2.filter2D(image, -1, kernel)fig, ax = plt.subplots(1,2, figsize=(10,5))ax[0].imshow(image, cmap='gray')ax[0].set_title('Original Image')ax[1].imshow(image_sharp, cmap='gray')ax[1].set_title('Sharp Image') 8. 이미지 대비 높이기히스토그램 평활화(Histogram Equalization)은 객체의 형태가 두드러지도록 만들어주는 이미지 처리 도구이며, OpenCV에서는 equalizeHist() 메소드를 통해 적용할 수 있다.1234567891011image = cv2.imread('images/plane_256x256.jpg', cv2.IMREAD_GRAYSCALE)# 이미지 대비를 향상image_enhanced = cv2.equalizeHist(image)# plotfig, ax = plt.subplots(1,2, figsize=(10, 5))ax[0].imshow(image, cmap='gray')ax[0].set_title('Original Image')ax[1].imshow(image_enhanced, cmap='gray')ax[1].set_title('Enhanced Image') 컬러 이미지의 경우 먼저 YUV 컬러 포맷으로 변환해야 한다. Y는 루마 또는 밝기이고 U와 V는 컬러를 나타낸다. 변환한 뒤에 위와 동일하게 equlizeHist() 메소드를 적용하고 다시 RGB 포맷으로 변환 후 출력한다.123456789101112131415image_bgr = cv2.imread('images/plane.jpg')# YUV 컬로 포맷으로 변환image_yuv = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2YUV)# 히스토그램 평활화 적용image_yuv[:, :, 0] = cv2.equalizeHist(image_yuv[:, :, 0])# #RGB로 변환image_rgb = cv2.cvtColor(image_yuv, cv2.COLOR_YUV2RGB)# plotfig, ax = plt.subplots(1,2, figsize=(12, 8))ax[0].imshow(image_bgr, cmap='gray')ax[0].set_title('Original Color Image')ax[1].imshow(image_rgb, cmap='gray')ax[1].set_title('Enhanced Color Image') 9. 이미지 이진화이미지 이진화(임계처리)는 어떤 값보다 큰 값을 가진 픽셀을 흰색으로 만들고 작은 값을 가진 픽셀은 검은색으로 만드는 과정이다. 더 고급 기술은 적응적 이진화(Adaptive Thresholding)로, 픽셀의 임곗값이 주변 픽셀의 강도에 의해 결정된다. 이는 이미지 안의 영역마다 빛 조건이 달라질 때 도움이 된다.1234567891011121314# 이미지 로드 image_grey = cv2.imread('images/plane_256x256.jpg', cv2.IMREAD_GRAYSCALE)# Adaptive Thresholding 적용 max_output_value = 255 # 출력 픽셀 강도의 최대값neighborhood_size = 99subtract_from_mean = 10image_binarized = cv2.adaptiveThreshold(image_grey, max_output_value, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, neighborhood_size, subtract_from_mean) adaptiveThreshold() 함수에는 네 개의 중요한 매개변수가 있다. max_output_value : 출력 픽셀 강도의 최댓값 저장 cv2.ADAPTIVE_THRESH_GAUSSIAN_C : 픽셀의 임곗값을 주변 픽셀 강도의 가중치 합으로 설정. 가중치는 가우시안 윈도우에 의해 결정 cv2.ADAPTIVE_THRESH_MEAN_C : 주변 픽셀의 평균을 임곗값으로 설정 123# plotplt.imshow(image_binarized, cmap='gray')plt.show() 10. 배경 제거배경을 제거하고자 하는 전경 주위에 사각형 박스를 그리고 그랩컷(grabCut) 알고리즘을 적용하여 배경을 제거한다.grabCut의 경우 잘 작동하더라도 여전히 이미지에 제거하지 못한 배경이 발생할 수 있다. 이렇게 제거 되지 못한 부분은 다시 적용하여 제거할 수 있지만 실전에서 수 천장의 이미지를 수동으로 고치는 것은 불가능한 일이므로 머신러닝을 적용한다거나 할 때도 일부러 noise를 적용하는 것처럼 일부 배경이 남아있는 것을 수용하는 것이 좋다.123# 이미지 로드 후 RGB로 변환image_bgr = cv2.imread('images/plane_256x256.jpg')image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB) 123456789101112131415161718# 사각형 좌표: 시작점의 x,y ,넢이, 너비rectangle = (0, 56, 256, 150)# 초기 마스크 생성mask = np.zeros(image_rgb.shape[:2], np.uint8)# grabCut에 사용할 임시 배열 생성bgdModel = np.zeros((1, 65), np.float64)fgdModel = np.zeros((1, 65), np.float64)# grabCut 실행cv2.grabCut(image_rgb, # 원본 이미지 mask, # 마스크 rectangle, # 사각형 bgdModel, # 배경을 위한 임시 배열 fgdModel, # 전경을 위한 임시 배열 5, # 반복 횟수 cv2.GC_INIT_WITH_RECT) # 사각형을 위한 초기화 123456789# 배경인 곳은 0, 그 외에는 1로 설정한 마스크 생성mask_2 = np.where((mask==2) | (mask==0), 0, 1).astype('uint8')# 이미지에 새로운 마스크를 곱행 배경을 제외image_rgb_nobg = image_rgb * mask_2[:, :, np.newaxis]# plotplt.imshow(image_rgb_nobg)plt.show() 위에서 먼저 전경이 들어있는 영역 주위를 사각형으로 표시하였는데, grabCut은 이 사각형 밖에 있는 모든 것이 배경이라고 가정하고 이 정보를 사용하여 사각형 안에 있는 배경을 찾는다.왼쪽 그림의 검은 영역은 배경이라고 확실하게 가정한 사각형의 바깥쪽 영역이며, 회색 영역은 그랩컷이 배경이라고 생각하는 영역, 그리고 흰색 영역은 전경이다. 오른쪽 그림은 두 번째 마스크를 이미지에 적용하여 전경만 남긴 이미지이다. 11. 경계선 감지Canny()메소드를 활용하여 경계선을 감지 할 수 있다. Canny()메소드는 그래디언트 임곗값 사이의 저점과 고점을 나타내는 두 매개변수를 필요로 하며, 낮은 임곗값과 높은 임곗값 사이의 가능성 있는 경계선 픽셀은 약한 경계선 픽셀로 간주하고, 높은 임곗값보다 큰 픽셀은 강한 경계선 픽셀로 간주한다.123456789# 이미지 로드image_gray = cv2.imread('images/plane_256x256.jpg', cv2.IMREAD_GRAYSCALE)# 픽셀 강도의 중간값을 계산median_intensity = np.median(image_gray)# 중간 픽셀 강도에서 위아래 1 표준편차 떨어진 값을 임곗값으로 지정lower_threshold = int(max(0, (1.0 - 0.33) * median_intensity))upper_threshold = int(min(255, (1.0 + 0.33) * median_intensity)) 12345# Canny edge detection 적용image_canny = cv2.Canny(image_gray, lower_threshold, upper_threshold)plt.imshow(image_canny, cmap='gray')plt.show()","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://jaehyeongan.github.io/tags/python/"},{"name":"image","slug":"image","permalink":"https://jaehyeongan.github.io/tags/image/"},{"name":"opencv","slug":"opencv","permalink":"https://jaehyeongan.github.io/tags/opencv/"}]},{"title":"AWS EC2에 플라스크(Flask) 클라우드 웹 서버 구축하기","slug":"aws-flask","date":"2020-01-13T12:26:58.000Z","updated":"2020-10-28T15:14:34.477Z","comments":true,"path":"2020/01/13/aws-flask/","link":"","permalink":"https://jaehyeongan.github.io/2020/01/13/aws-flask/","excerpt":"","text":"Intro 지난 번 글에서 Flask 웹 프레임워크를 통해 간단한 딥러닝 웹 애플리케이션을 개발해보았다. 하지만 로컬(local) 환경에서 개발하였기 때문에 개발 서버를 종일 켜놓거나 고정 도메인을 따로 받지 않은 이상 외부 IP로 접근은 불가능하다.그렇기 때문에 나처럼 물리적인 서버를 구축 및 운영할 환경이 되지 않을 경우는 클라우드(Cloud) 서비스를 이용하게 되는데, 이번 글에서는 AWS(Amazon Web Services)라고 하는 클라우드 서비스를 활용하여 웹 서버를 구축 후 Flask를 배포하는 과정을 설명하려고 한다. 1. AWS EC2 가입 및 인스턴스 생성우선 AWS Management Consol로 이동 후 가입이 되어있지 않다면 가입 후 로그인을 한다. (가입 시 region을 Seoul로 설정할 것)서비스 검색을 통해 EC2를 선택한다. EC2 대시보드에서 인스턴스 생성 아래의 인스턴스 시작 버튼을 클릭 AMI로는 기업용이 아니니 개인 개발용으로 편한 Ubuntu Linux 18.04 버전을 사용하며,무료 서버 이용이 가능한 프리 티어(Free Tier)로 서버를 생성한다. 위 이미지에서 시작 버튼을 누를 경우 키 페어를 설정하는 메시지가 나타나는데 이 키 페어는 말 그대로 생성한 웹 서버에 추후 접속할 때 꼭 필요한 키 역할을 한다. ‘새 키 페어 생성’을 선택하고 ‘키 페어 이름’을 본인 취향에 맞게 설정 후 ‘키 페어 다운로드’를 선택한다. (이 키 페어는 추후 서버 접속 시 꼭 필요하므로 본인 개발 폴더에 잘 보관해둔다.)키 페어를 다운로드하여 인스턴스 시작 버튼이 활성화되면 버튼을 클릭하여 진행한다. 인스턴스 보기를 선택 여기까지가 진행하게 되면 인스턴스가 아래와 같이 생성된다. 2. Key Pair 권한 설정 변경전 과정에서 인스턴스를 생성하면서 Key Pair를 같이 다운로드 하였을 것이다. 하지만 접속하기 위해서는 이 권한 설정을 변경해줘야만 접속이 가능하다. 우선 다운받은 키페어를 우클릭하여 [속성]-[보안] 탭으로 이동 후 [고급]을 클릭한다. 아래와 같은 화면에서,[상속 사용 안 함]을 클릭 후, 팝업 메시지에서 ‘상속된 사용 권한을 이 개체에 대한 명시적 사용 권한으로 변환합니다’를 선택 이후 아래와 같이 Administrators를 제외한 모든 사용 권한 항목을 제거한다 3. 보안 그룹 설정인스턴스 화면으로 돌아와 Flask 웹 서버 포트 번호인 5000번 포트를 열어 주기 위해 보안 그룹을 설정한다. [인바운드] 탭에서 [편집]을 클릭 후 [규칙 추가]를 하여 아래와 같이 5000번 포트를 설정한다. 4. 인스턴스 접속하기다시 인스턴스 화면으로 돌아와 아래 화면에서 생성한 인스턴스를 선택 후 연결 버튼을 클릭한다.연결 방법으로는 ‘독립 실행형 SSH 클라이언트’로 선택하고, 아래 ssh 명령어를 복사한다. 명령프롬프트(CMD)를 관리자 권한으로 실행 후 다운 받은 Key Pair가 있는 위치로 이동한다. 그 후 위에서 복사한 SSH 명령어를 복사하여 우분투 리눅스 인스턴스에 접속한다. 우선 파이썬 라이브러리 도구인 pip 및 java jdk 등을 설치해주고, 본인의 파이썬 코드가 수행되기 위한 라이브러리를 설치해준다.12345678910111213$ sudo apt update# java 설치 $ sudo apt install openjdk-8-jre$ sudo apt install openjdk-8-jdk# pip 설치 및 라이브러리 설치$ sudo apt install python3-pip$ sudo apt install tensorflow$ sudo apt install keras$ sudo apt install opencv-python$ sudo apt install scipy : 이후 개발한 flask를 웹 서버로 clone하여 해당 경로로 이동 후 웹 서버를 실행해준다. 이제 웹 서버가 실행 중이니 퍼블릭 IP로 접속이 가능하다. 아래 인스턴스 화면에서 ‘IPv4 퍼블릭 IP’ 주소를 복사 후 5000번 포트번호( http://54.180.150.154:5000/ )로 접속한다. 고정 IP에서 서버가 잘 실행되고 있다. 5. 파이썬 서버 계속 실행 시키기위와 같이 정상적으로 고정 IP를 통해 접속이 가능함을 확인하였다. 하지만 SSH 프롬프트를 종료하게 되면 파이썬 서버도 함께 종료되게 된다. 마지막으로 파이썬 서버가 항상 실행될 수 있도록 설정한다. Ctrl+Z 를 통해 파이썬 프로세스 중지 $ bg : 백그라운드에서 프로세스 재 구동 $ disown -h : 소유권 포기 References https://ndb796.tistory.com/244","categories":[],"tags":[{"name":"flask","slug":"flask","permalink":"https://jaehyeongan.github.io/tags/flask/"},{"name":"python","slug":"python","permalink":"https://jaehyeongan.github.io/tags/python/"},{"name":"web","slug":"web","permalink":"https://jaehyeongan.github.io/tags/web/"},{"name":"amazon","slug":"amazon","permalink":"https://jaehyeongan.github.io/tags/amazon/"},{"name":"aws","slug":"aws","permalink":"https://jaehyeongan.github.io/tags/aws/"},{"name":"webservice","slug":"webservice","permalink":"https://jaehyeongan.github.io/tags/webservice/"},{"name":"cloud","slug":"cloud","permalink":"https://jaehyeongan.github.io/tags/cloud/"},{"name":"webframework","slug":"webframework","permalink":"https://jaehyeongan.github.io/tags/webframework/"}]},{"title":"파이썬 웹 프레임워크 Flask를 활용한 딥러닝 웹 애플리케이션 개발","slug":"flask-deeplearning-webapplication","date":"2019-12-27T14:20:58.000Z","updated":"2020-10-28T15:14:34.479Z","comments":true,"path":"2019/12/27/flask-deeplearning-webapplication/","link":"","permalink":"https://jaehyeongan.github.io/2019/12/27/flask-deeplearning-webapplication/","excerpt":"","text":"IntroJava의 Spring처럼 Python에서도 웹 프레임워크를 제공한다. 그 중 가장 인기 있는 것이 Django와 Flask인데, Django의 경우 Instagram, LinkedIn 사이트로 사용될 정도로 인기 있고 안정적인 웹 프레임워크라고 할 수 있다. 그 만큼 체계적이고 정교한 구조를 가지고 있다고 할 수 있는데 그와 반대로 Flask는 좀 더 간편하고 경량화 된 웹 프레임워크라고 생각하면 될 것 같다. 그래서 실제 서비스 하기 보다는 간단한 프로토타입 개발 용도로 많이 사용되는 것 같다. 이번에는 Flask 웹 프레임워크에 대해 알아보고 딥러닝 모델 중 Neural Style Transfer를 Flask에서 실행하여 결과를 웹으로 표출해보도록 할 것이다. ※ 해당 전체 코드는 github에서 확인 할 수 있습니다. 1. Flask 설치1) flask 프로젝트 폴더 생성우선 flask 프로젝트를 수행할 폴더를 본인의 임의 경로에 설치해준다. 나는 아래와 같은 경로에 폴더를 만들었다.12cd workspaceworkspace &gt; mkdir pyflask 2) 가상환경flask 환경을 위한 virtualenv 가상환경 라이브러리를 설치한다.1pip install virtualenv 설치 완료 후 본인의 flask 경로 내에서 가상환경을 생성해준다.1workspace\\pyflask &gt; virtualenv venv 정상적으로 실행 시 flask 폴더 내에 venv 폴더가 생성되는데 venv/Scripts 폴더로 이동하여 가상환경을 활성화(active) 한다.12workspace\\pyflask &gt; cd venv/Scriptsworkspace\\pyflask\\venv\\Scripts &gt; activate 3) Flask 설치위에서 active한 가상환경 내에서 flask를 설치해준다.1(venv) pip install flask 설치완료 된 flask의 버전은 아래처럼 확인할 수 있다.1flask --version 2. 웹 구성이제 Flask를 개발할 환경은 구축하였으니, 웹 애플리케이션의 구조를 설계해 볼 것이다.여기서 프로토타입으로 구성해 볼 웬 애플리케이션은 간단하게 메인 페이지로 구성되고, 각 기능을 수행하는 서브 페이지로 이동하게 된다. 이후 사용자로 부터 입력값을 받아 딥러닝 기능 수행 후 그 결과값을 다시 웹으로 출력해주는 구조를 가진다.123index (메인 페이지)├── nst_get (user input 받는 페이지)└── nst_post (결과 출력 페이지) 1) 폴더 구성우선 효율적인 웹 개발을 위하여 flask 폴더 내에 몇 개의 폴더를 설치하여 기능별로 관리한다.123456pyflask/├── static/ └── images/├── templates/├── venv/└── neural_style_transfer.py static/images : 사용자로부터 받을 이미지를 저장할 경로 templates : html 파일 neural_style_transfer.py : neural style transfer를 수행할 딥러닝 코드 2) HTML 템플릿우선 화면 구성을 위하여 HTML 템플릿을 아래와 같이 최대한 간단하게 작성하였다.(지면상 CSS와 JS코드는 제거하였는데 전체 코드는 github에서 확인할 수 있다.) index.html (메인 페이지) 1234567891011121314151617181920&lt;!DOCTYPE html&gt;&lt;html lang=\"ko\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt; &lt;meta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\"&gt; &lt;title&gt;Flask Index&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;br&gt; &lt;h1 align=\"center\"&gt;Flask for Deep ConvNet&lt;/h1&gt; &lt;br&gt; &lt;ul&gt; &lt;li&gt;&lt;h2&gt;&lt;a href=\"/nst_get\"&gt;Neural Style Transfer&lt;/a&gt;&lt;/h2&gt;&lt;/li&gt; &lt;li&gt;&lt;h2&gt;&lt;a href=\"#\"&gt;Obejct Detection&lt;/a&gt;&lt;/h2&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/body&gt;&lt;footer align='center'&gt;Powerd by &lt;strong&gt;© 2019 JaeHyeong&lt;/strong&gt;&lt;/footer&gt;&lt;/html&gt; — a태그 링크에 /nst_get을 명시하여 클릭 시 nst_get.html로 이동 nst_get.html (user input 받는 페이지) 12345678910111213141516171819202122232425262728293031323334353637383940&lt;!DOCTYPE html&gt;&lt;html lang=\"ko\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt; &lt;meta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\"&gt; &lt;title&gt;Flask image get&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;br&gt; &lt;h1 align=\"center\"&gt;Neural Sytle Transfer&lt;/h1&gt; &lt;br&gt; &lt;form align=\"center\" action=\"/nst_post\" method=\"POST\" enctype=\"multipart/form-data\"&gt; &lt;h2 align=\"center\"&gt;Reference Images&lt;/h2&gt; &lt;table align=\"center\"&gt; &lt;tr&gt; &lt;td&gt;&lt;img class=\"refer_img\" id=\"refer_img1\" src=\"./static/images/rain_princess.jpg\"&gt;&lt;/td&gt; &lt;td&gt;&lt;img class=\"refer_img\" id=\"refer_img2\" src=\"./static/images/the_stary_night.JPG\"&gt;&lt;/td&gt; &lt;td&gt;&lt;img class=\"refer_img\" id=\"refer_img3\" src=\"./static/images/scream.jpg\"&gt;&lt;/td&gt; &lt;td&gt;&lt;img class=\"refer_img\" id=\"refer_img3\" src=\"./static/images/zentangle_art.jpg\"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;input type=\"radio\" name=\"refer_img\" value=\"rain_princess.jpg\"&gt;&lt;/td&gt; &lt;td&gt;&lt;input type=\"radio\" name=\"refer_img\" value=\"the_stary_night.JPG\"&gt;&lt;/td&gt; &lt;td&gt;&lt;input type=\"radio\" name=\"refer_img\" value=\"scream.jpg\"&gt;&lt;/td&gt; &lt;td&gt;&lt;input type=\"radio\" name=\"refer_img\" value=\"zentangle_art.jpg\"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;br&gt;&lt;br&gt; &lt;h2 align=\"center\"&gt;Target Image&lt;/h2&gt; &lt;div align=\"center\" id='view_area'&gt;&lt;/div&gt; &lt;br&gt; &lt;input type=\"file\" name=\"user_img\" id=\"user_img\" value=\"userIMgage\" onchange=\"previewImage(this,'view_area')\"/&gt; &lt;input type=\"submit\" value=\"확인\"/&gt; &lt;/form&gt; &lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;/body&gt;&lt;/html&gt; — 이미지를 Flask로 넘겨주는 페이지이므로 form 태그의 POST 방식을 수행— neural style transfer 학습을 위한 reference 이미지 경로를 지정하여 표시— 사용자로부터 이미지 파일을 입력 받음— 확인 버튼을 통해 선택한 reference 이미지와 사용자 이미지를 전송 nst_post.html (결과 출력 페이지) 1234567891011121314151617181920212223242526272829&lt;!DOCTYPE html&gt;&lt;html lang=\"ko\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt; &lt;meta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\"&gt; &lt;title&gt;Flask image post&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;br&gt; &lt;table align=\"center\"&gt; &lt;tr&gt; &lt;td&gt;&lt;h2 align=\"center\"&gt;Reference Image&lt;/h2&gt;&lt;/td&gt; &lt;td&gt;&lt;h2 align=\"center\"&gt;Target Image&lt;/h2&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;img class='nst_img' src=\"&#123;&#123;url_for('static', filename=refer_img)&#125;&#125;\"&gt;&lt;/td&gt; &lt;td&gt;&lt;img class=nst_img src=\"&#123;&#123;url_for('static', filename=user_img)&#125;&#125;\"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td colspan=\"2\"&gt;&lt;h2 align=\"center\"&gt;Transfer Image&lt;/h2&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td colspan=\"2\" align=\"center\"&gt;&lt;img class=\"nst_result_img\" src=\"&#123;&#123;url_for('static', filename=transfer_img)&#125;&#125;\"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;br&gt;&lt;/body&gt;&lt;/html&gt; — Flask로 부터 넘겨 받은 결과 이미지를 받아서 출력 3) Neural Style Transfer 수행 코드 작성전체 코드는 github 참조 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162def preprocess_image(image_path): img = load_img(image_path, target_size=(img_height, img_width)) # (400, 381) img = img_to_array(img) # (400, 381, 3) img = np.expand_dims(img, axis=0) # (1, 400, 381, 3) img = vgg19.preprocess_input(img) return img : : ( 중략 ) :def main(refer_img_path, target_img_path): style_reference_image_path = 'flask_deep/static/'+ refer_img_path target_image_path = 'flask_deep/static/'+ target_img_path # 모든 이미지를 fixed-size(400pixel)로 변경 width, height = load_img(target_image_path).size global img_height; global img_width; img_height = 400 img_width = int(width * img_height / height) target_image = K.constant(preprocess_image(target_image_path)) # creates img to a constant tensor style_reference_image = K.constant(preprocess_image(style_reference_image_path)) combination_image = K.placeholder((1, img_height, img_width, 3)) # 생성된 이미지를 담을 placeholder # 3개의 이미지를 하나의 배치로 합침 input_tensor = K.concatenate([target_image, style_reference_image, combination_image], axis=0) # 3개 이미지의 배치를 입력으로 받는 VGGNet 생성 model = vgg19.VGG19(input_tensor=input_tensor, weights='imagenet', # pre-trained ImageNet 가중치 로드 include_top=False) # FC layer 제외 : : ( 중략 ) : evaluator = Evaluator() refer_img_name = refer_img_path.split('.')[0].split('/')[-1] result_prefix = 'flask_deep/static/images/nst_result_'+refer_img_name iterations = 30 # 뉴럴 스타일 트랜스퍼의 손실을 최소화하기 위해 생성된 이미지에 대해 L-BFGS 최적화를 수행 x = preprocess_image(target_image_path) x = x.flatten() for i in range(iterations): start_time = time.time() x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x, fprime=evaluator.grads, maxfun=20) # 생성된 현재 이미지를 저장 img = x.copy().reshape((img_height, img_width, 3)) img = deprocess_image(img) fname = result_prefix + '.png' end_time = time.time() save_img(fname, img) return fnameif __name__ == \"__main__\": main() 4) Flask app 파일 생성Flask 애플리케이션 실행을 위한 _init_.py 파일을 생성한다. 여기서 Flask 파라미터로 전달되는 __name\\ 파라미터는 Flask 애플리케이션을 구분하기 위한 구분자로 사용된다.app.debug 를 True로 지정할 경우 코드 수정 시 바로바로 디버깅이 가능하게 해준다. @app.route는 페이지 URL과 함수를 연결해주는 역할을 하며 아래와 같이 @app.route 데코레이터 지정 후 render_template(‘URL’)을 통해 연결할 페이지 경로를 입력하면 해당 경로를 웹 브라우저로 전달해주게 된다. _init_.py 123456789101112131415161718192021222324252627282930313233343536import os, sysfrom flask import Flask, escape, request, Response, g, make_responsefrom flask.templating import render_templatefrom werkzeug import secure_filenamefrom . import neural_style_transferapp = Flask(__name__)app.debug = True# Main page@app.route('/')def index(): return render_template('index.html')@app.route('/nst_get')def nst_get(): return render_template('nst_get.html')@app.route('/nst_post', methods=['GET','POST'])def nst_post(): if request.method == 'POST': # Reference Image refer_img = request.form['refer_img'] refer_img_path = 'static/images/'+str(refer_img) # User Image (target image) user_img = request.files['user_img'] user_img.save('./flask_deep/static/images/'+str(user_img.filename)) user_img_path = './static/images/'+str(user_img.filename) # Neural Style Transfer transfer_img = neural_style_transfer.main(refer_img_path, user_img_path) transfer_img_path = './static/images/'+str(transfer_img.split('/')[-1]) return render_template('nst_post.html', refer_img=refer_img_path, user_img=user_img_path, transfer_img=transfer_img_path) — index에서 nst_get 링크 클릭 시 nst_get.html로 경로 이동— nst_get.html에서 POST방식을 통해 전달받은 이미지를 nst_post 함수에서 request 메소드를 통해 넘겨받음— reference 이미지와 user 이미지 경로를 neural style transfer를 수행하는 딥러닝 메소드로 전달— neural style transfer 딥러닝 코드에서 모델 수행 후 받은 결과값을 nst_post.html로 전달 3. 웹 실행1) Flask 서버 실행Flask 서버 실행을 위해 프로젝트 폴더 상위에 아래와 같은 파이썬 코드를 생성하였다.123from pyflask import appapp.run(host='127.0.0.1') 위 코드는 _init_.py에서 app 애플리케이션을 실행하게 해주며, 위 파이썬 파일 실행 시 아래와 같이 flask 서버가 실행되게 된다. 이후 브라우저에서 http://127.0.0.1:5000 입력 시 위에서 만든 화면을 확인할 수 있다. 2) 웹 화면 메인 페이지 사용자 입력 페이지 결과 출력 페이지 OutroFlask는 이번에 간단한 애플리케이션을 적용해보기 위해서 처음 사용해보았다. 이전에 Spring 프레임워크를 통해 프로젝트를 해본 적은 있는데 사용안하지 너무 오래되다보니 까먹기도 했고 환경 구성하는 것도 일이어서 좀 가벼운 Flask를 사용해보게 되었다. 일단 기본적으로 html/css 그리고 python만 기초적으로 알아도 누구나 쉽고 간단하게 웹 화면을 구성해볼 수 있다는 장점이 있는 것 같다. 근데 확실히 큰 프로젝트 성으로 여러사람이 복잡한 화면을 구성할 때는 작업 관리가 쉽지 않을 것 같다는 생각이 든다. 시간나면 Django도 공부해봐야할 것 같다.","categories":[],"tags":[{"name":"deeplearning","slug":"deeplearning","permalink":"https://jaehyeongan.github.io/tags/deeplearning/"},{"name":"flask","slug":"flask","permalink":"https://jaehyeongan.github.io/tags/flask/"},{"name":"python","slug":"python","permalink":"https://jaehyeongan.github.io/tags/python/"},{"name":"webframework","slug":"webframework","permalink":"https://jaehyeongan.github.io/tags/webframework/"},{"name":"webdevelopment","slug":"webdevelopment","permalink":"https://jaehyeongan.github.io/tags/webdevelopment/"},{"name":"django","slug":"django","permalink":"https://jaehyeongan.github.io/tags/django/"},{"name":"neuralstyletransfer","slug":"neuralstyletransfer","permalink":"https://jaehyeongan.github.io/tags/neuralstyletransfer/"}]},{"title":"R-CNN(Regions with CNN features) 논문 리뷰","slug":"R-CNN","date":"2019-10-10T14:56:28.000Z","updated":"2020-10-28T15:14:34.477Z","comments":true,"path":"2019/10/10/R-CNN/","link":"","permalink":"https://jaehyeongan.github.io/2019/10/10/R-CNN/","excerpt":"","text":"Intro 오늘은 초기 Object Detection 발전에 가장 많은 영향을 미친 논문인 Ross Girshick의 Rich feature hierarchies for accurate object detection and semantic segmentation 즉, R-CNN에 대한 논문 리뷰를 간단히 하고자 한다. 우선 Obejct Detection이란 이미지가 무엇인지 판단하는 Classification과 이미지 내의 물체의 위치 정보를 찾는 Localization을 수행하는 것을 말한다. 이를 통해 영상 내의 객체가 사람인지 동물인지 물건인지 등을 구별하여 각 객체가 어디에 위치하는지 표시하는 것이 가능하다. Abstract지난 몇 년 동안 PASCAL VOC 데이터셋에서 Object Detection의 가장 좋은 성능을 내는 것은 high-level context의 복잡한 앙상블 모델이었다. 하지만 이 논문에서는 VOC 2012 데이터를 기준으로 이전 모델에 비해 mAP(mean average precision)가 30%이상 향상된 더 간단하고 확장 가능한 detection 알고리즘을 소개하였다.이 알고리즘은 크게 두 가지 핵심 인사이트를 가지고 있는데 다음과 같다. 객체를 localize 및 segment하기 위해 bottom-up방식의 region proposal(지역 제안)에 Convolutional Neural Network를 적용 domain-specific fine-tuning을 통한 supervised pre-training을 적용 저자는 해당 모델을 R-CNN(Regions with CNN features)이라고 명시하였으며, 그 이유는 CNN과 Region proposal이 결합되었기 때문이라고 한다. 1. Introduction지난 10년간 다양한 visual recognition 작업에서는 주로 SIFT와 HOG(gradient 기반의 특징점 추출 알고리즘)가 가장 많이 사용되었는데, 이는 2010 ~ 2012년의 PASCAL VOC obeject detection에서 일반적으로 인정되는 방법이었다. 하지만 이후 back-propagation이 가능한 SGD(Stochastic Gradient Descent)기반의 CNN(Convolutional Neural Networks)이 등장하기 시작하였고 SIFT와 HOG와 같은 알고리즘과 비교하여 PASCAL VOC object detection에서 굉장한 성능을 보이게 되었다.Image Classification과 다르게 detection은 이미지내에서 객체를 localizing하는 것이 요구되는데 이를 위해, 논문의 모델은 sliding-window 방식을 적용하였고, 높은 공간 해상도(high spartial resolution)을 유지하기 위해 5개의 Convolutional 레이어를 적용하였다.우선 간단하게 R-CNN은 아래와 같은 프로세스로 작동한다. R-CNN 프로세스 Input 이미지로부터 2,000개의 독립적인 region proposal을 생성 CNN을 통해 각 proposal 마다 고정된 길이의 feature vector를 추출(CNN 적용 시 서로 다른 region shape에 영향을 받지 않기 위해 fixed-size로 이미지를 변경) 이후, 각 region 마다 category-specific linear SVM을 적용하여 classification을 수행 2. Object detection with R-CNN이 논문의 object detection은 크게 3가지 모듈로 구성되어 있다. 1. category-independent한 region proposals를 생성2. 각 region으로부터 feature vector를 추출하기 위한 large CNN3. classification을 위한 linear SVMs이제 아래에서 본격적으로 각 모듈에 대해 설명하고 PASCAL VOC2010-12에 대한 결과를 소개한다. Region proposals카테고리 독립적인 region proposal을 생성하기 위한 방법은 여러가지가 있는데 해당 논문에서는 이전 detection 작업들과 비교하기 위하여 Selective Search라는 최적의 region proposal를 제안하는 기법을 사용하여 독립적인 region proposal을 추출하였다. selective search는 아래와 같은 프로세스로 이루어진다. Selective Search 이미지의 초기 세그먼트를 정하여, 수많은 region 영역을 생성 greedy 알고리즘을 이용하여 각 region을 기준으로 주변의 유사한 영역을 결합 결합되어 커진 region을 최종 region proposal로 제안 Feature extraction우선 위에서 언급한 Selective Search를 통해 도출 된 각 region proposal로부터 CNN을 사용하여 4096차원의 feature vector를 추출한다. 이후, feature들은 5개의 convolutional layer와 2개의 fully connected layer로 전파되는데, 이때 CNN의 입력으로 사용되기 위해 각 region은 227x227 RGB의 고정된 사이즈로 변환되게 된다. Training학습에 사용되는 CNN 모델의 경우 ILSVRC 2012 데이터 셋으로 미리 학습된 pre-trained CNN(AlexNet)모델을 사용한다. Domain-specific fine-tuningClassification에 최적화된 CNN 모델을 새로운 Detection 작업 그리고 VOC 데이터셋에 적용하기 위해 오직 VOC의 region proposals를 통해 SGD(stochastic gradient descent)방식으로 CNN 파라미터를 업데이트 한다. 이후 CNN을 통해 나온 feature map은 SVM을 통해 classification 및 bounding regreesion이 진행되게 되는데, 여기서 SVM 학습을 위해 NMS(non-maximum suppresion)과 IoU(inter-section-over-union)이라는 개념이 활용된다. IoU는 Area of Overlap(교집합) / Area of Union(합집합)으로 계산되며, 간단히 말해 전체 bounding box 영역 중 겹치는 부분의 비율을 나타내는데 NMS 알고리즘이 이 IoU 점수를 활용하여 겹치는 박스를 모두 제거하고 가장 적합한 박스만 남기게 된다. NMS의 과정을 간단히 살펴보면 아래와 같은 프로세로 진행된다. NMS(Non-maximum suppresion) 예측한 bounding box들의 예측 점수를 내림차순으로 정렬 높은 점수의 박스부터 시작하여 나머지 박스들 간의 IoU를 계산 IoU값이 지정한 threhold보다 높은 박스를 제거 최적의 박스만 남을 떄까지 위 과정을 반복 해당 논문에서는 SVM 학습을 위한 라벨로서 IoU를 활용하였고 IoU 가 0.5이상인 것들을 positive 객체로 보고 나머지는 negative로 분류하여 학습하게 된다. 각 SGD iteration마다 32개의 positive window와 96개의 backgroud window 총 128개의 배치로 학습이 진행된다. 3. Results on PASCAL VOC 2010-12 위 테이블은 VOC 2010 테스트 데이터에 대한 각 모델별 결과이다. 맨 오른쪽에서 mAP를 확인할 수 있는데, 논문에서는 결과를 비교하는데 같은 region proposal 알고리즘을 적용한 UVA모델과 mAP를 비교한다.위 표를 보면 UVA 모델의 mAP는 35.1%이고, R-CNN의 mAP는 53.7%인 것을 확인할 수 있으며 이것은 높은 증가율이라고 저자는 말한다. 또한 VOC 2011/12 데이터 셋 또한 53.3% mAP 높은 성능을 나타냈다. 4. ProblemsR-CNN의 가장 큰 문제는 복잡한 프로세스로 인한 과도한 연상량에 있다. 최근에는 고성능 GPU가 많이 보급 되었기 때문에 deep한 neural net이라도 GPU연산을 통해 빠른 처리가 가능하다. 하지만 R-CNN은 selective search 알고리즘를 통한 region proposal 작업 그리고 NMS 알고리즘 작업 등은 CPU 연산에 의해 이루어 지기 때문에 굉장히 많은 연산량 및 시간이 소모된다.또한 SVM 예측 시 region에 대한 classification 및 bounding box에 대한 regression 작업이 함께 작동하다 보니 모델 예측 부분에서도 연산 및 시간이 많이 소모되어 real-time 분석이 어렵다는 단점이 있다. R-CNN의 이러한 한계들로 인해, 추후 프로세스 및 연산 측면에서 보완된 모델이 나오게 되는데 그것이 바로 Fast R-CNN과 Faster R-CNN이다. 위 그림은 R-CNN, SPP-Net, Fast R-CNN, Faster R-CNN의 실행 속도 차이를 나타내는데 Faster R-CNN이 이전 모델보다 비교가 안될 정도로 훠얼씬 빠르다는 것을 알 수 있다. (성능도 더 좋아졌다.)아래에서 Fast R-CNN과 Faster R-CNN에 대해 간단하게 집고 넘어가 보도록 한다. 5. Fast R-CNN &amp; Faster R-CNNFast R-CNNFast R-CNN의 R-CNN의 문제를 해결하기 위해 나온 모델이다.동작 방식은 R-CNN과 유사하게 region proposal이 작동하지만, RCNN과 다르게 Fast R-CNN은 먼저 전체 이미지가 ConvNet의 input으로 입력이 된다. 이미지는 ConvNet을 통과하며 feature map을 추출하게 되고, 이 feature map은 selectice search 기반의 region proposal을 통해 RoI(Regions of Interest)를 뽑아낸다. 이후 선택 된 Region들은 RoI Pooling layer를 거치게 되는데, 이 과정은 추후 예측을 위해 region들을 다운 사이즈하여 모두 같은 고정된 크기로 변환해주는 역할을 한다. 마지막 과정으로 fully connected layer를 거치며 Softmax Classification과 Bounding Box Regression이 수행된다. 위의 과정은 하나의 ConvNet모델에 의해 동시에 수행이 되기 때문에 RCNN에 비하여 훨씬 빠르게 작동하는 장점이 있다. 하지만 결국 Fast RCNN 또한 많은 연산을 필요로 하는 Selective Search 기법이 작동을 하므로 큰 데이터 셋에 적용하는데는 한계가 있다. Faster R-CNNFaster R-CNN은 R-CNN과 Fast R-CNN이 region proposal로 인한 과도한 연산 문제를 해결하기 위해 나온 모델이다. 기존 region proposal에 사용되었던 selective search는 연산량을 늘리고 시간을 많이 소모하는 주요 원인이었다. 그래서 Faster R-CNN에서는 selective search 알고리즘을 없애고 Region Proposal Networks(RPN)라는 뉴럴 네트워크를 추가하여 region proposal을 예측하도록 했다. 그 후, 예측된 region proposal은 Fast R-CNN과 유사하게 RoI Pooling layer를 거치며 모든 region을 같은 크기로 고정 후, Classification 및 Bounding Box Regreesion이 수행된다. Referencespaper R-Rich feature hierarchies for accurate object detection and semantic segmentation(https://arxiv.org/abs/1311.2524) Fast R-CNN(https://arxiv.org/abs/1504.08083) Faster R-CNN(https://arxiv.org/abs/1506.01497) blog https://reniew.github.io/10/ https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365ehttps://blog.lunit.io/2017/06/01/r-cnns-tutorial/","categories":[],"tags":[{"name":"selectivesearch","slug":"selectivesearch","permalink":"https://jaehyeongan.github.io/tags/selectivesearch/"},{"name":"rcnn","slug":"rcnn","permalink":"https://jaehyeongan.github.io/tags/rcnn/"},{"name":"fastrcnn","slug":"fastrcnn","permalink":"https://jaehyeongan.github.io/tags/fastrcnn/"},{"name":"fasterrcnn","slug":"fasterrcnn","permalink":"https://jaehyeongan.github.io/tags/fasterrcnn/"},{"name":"nms","slug":"nms","permalink":"https://jaehyeongan.github.io/tags/nms/"},{"name":"regionproposals","slug":"regionproposals","permalink":"https://jaehyeongan.github.io/tags/regionproposals/"},{"name":"iou","slug":"iou","permalink":"https://jaehyeongan.github.io/tags/iou/"},{"name":"voc","slug":"voc","permalink":"https://jaehyeongan.github.io/tags/voc/"}]},{"title":"합성곱 신경망(ConvNet, Convolutional Neural Network)","slug":"basic-convnet","date":"2019-09-23T14:18:01.000Z","updated":"2020-10-28T15:14:34.478Z","comments":true,"path":"2019/09/23/basic-convnet/","link":"","permalink":"https://jaehyeongan.github.io/2019/09/23/basic-convnet/","excerpt":"","text":"Intro 현재 ConvNet 기반의 모델은 단순 이미지 인식을 넘어 Object Detection, Semantic Segmentation 까지 딥러닝 알고리즘 중 가장 활발히 연구되고 성과를 내고 있는 분야이다. 우선 각 분야별 적용되고 있는 주요 모델을 간단히 살펴보면 아래와 같다. Classification Image Detection Semantic Segentation VGG Net RCNN FCN GoogLeNet Fast RCNN DeepLab ResNet Faster RCNN U-Net MobileNet YOLO ReSeg ShuffleNet SDD 이 글에서는 위의 훌륭한 모델들이 가장 기본으로 하는 ConvNet의 구조 및 학습 방법에 대해 간단히 알려보려고 한다. 합성곱 신경망(Convolutional Neural Network, ConvNet)합성곱 신경망은 합성곱 연산을 사용하는 신경망 중 하나로서, 주로 음성 인식이나 시각적 이미지를 분석하는데 사용된다. 합성곱 신경망이 일반 신경망과 다른 점은 일반적인 신경망들이 이미지 데이터를 원본 그대로 1차원 신경망인 Fully-Connected layer(FC layer 혹은 Dense layer)에 입력되어 전체 특성을 학습하고 처리하는 데 반해, 합성곱 신경망은 FC layer를 거치기 전에 Convolution 및 Poolling과 같은 데이터의 주요 특징 벡터를 추출하는 과정을 거친 후 FC layer로 입력되게 된다.그렇기 때문에 대부분의 이미지 인식 분야는 딥러닝 기반의 합성곱 신경망이 주를 이루고 있다. ConvNet의 구조 합성곱 신경망은 기본적으로 위와 같은 구조로 이루어져 있다. 위 ConvNet을 크게 3덩어리로 짤라서 보면 아래와 같다. 1. Input layer2. Convolutional layer ~ Max pooling layer3. Fully-connected layer ~ Output layers로 위 구조를 간단히 설명하면 위에서도 말했듯이 ConvNet은 단순 FC Layer로만 구성되어 있지 않다. Convolutional Layer와 Pooling Layer라는 과정을 거치게 되는데 이는 Input Image의 주요 특징 벡터를 추출하는 과정이라고 할 수 있다. 그 후 이렇게 추출된 주요 특징 벡터들은 그제야 FC Layer를 거치며 1차원 벡터로 변환되고 마지막 Output layer에서 활성화 함수인 Softmax함수를 통해 각 해당 클래스의 확률로 출력되게 된다. 아래에서 각 과정을 좀 더 상세히 살펴보겠다. 1. Input LayerInput Layer는 입력된 이미지 데이터가 최초로 거치게 되는 Layer이다. 모두가 알고 있듯이 이미지는 단순 1차원의 데이터가 아니다. 이미지는 기본적으로 (높이, 넓이, 채널)의 크기를 갖는 3차원의 크기를 가지며, 여기서 채널(channels)의 경우 Gray Scale(1)이냐 RGB(3)이냐 에 따라 크기가 달라지게 된다. (채널의 컬러 공간은 Gray, RGB, HSV, CMYK 등 다양하다) 위와 같은 형태는 높이 4, 넓이 4, 채널 RGB를 갖고 있으므로 위 이미지의 shape은 (4, 4, 3)으로 표현할 수 있으며, 이미지 인식의 교과서라 할 수 있는 위 MNIST 손글씨 데이터 셋의 경우 높이 28, 넓이 28, 채널 Gray를 가지고 있으므로 (28, 28, 1)의 shape을 가졌다고 말할 수 있다. 또한 다른 말로 특성 맵(Feature Map)이라고도 한다. 2. Convolutional LayerConvolutional Layer와 FC Layer의 경우 근본적은 차이가 존재하는데, Dense 층의 경우 특성 공간에 있는 전역 패턴(입력된 이미지의 모든 픽셀에 걸친 패턴)을 학습하는 반면 합성곱 층의 경우 지역 패턴을 학습하게 된다. 2.1 kernelConvolutional Layer에서는 Input Image의 크기인 특성 맵(Feature Map)을 입력으로 받게 되는데 지역 패턴 학습을 위하여 이러한 특성 맵에 커널(kernel) 혹은 필터(Filter)라 불리는 정사각 행렬을 적용하며 합성곱 연산을 수행하게 된다.커널의 경우 3 x 3, 5 x 5크기로 적용되는 것이 일반적이며 스트라이드(Stride)라고 불리는 지정된 간격에 따라 순차적으로 이동하게 된다. 위 그림의 경우 Image의 크기는 (5, 5, 1)의 크기를 가지고 있으며, 현재 3 x 3크기의 kernel이 1 Stride의 간격으로 이동하며 합성곱 연산을 수행하는 것을 보여준다. 만약 커널이 2개의 크기만큼 이동하고 있다면 2 Stride 간격으로 이동한다고 할 수 있다. 이렇게 커널은 스트라이드 간격만큼 순회하며 모든 채널의 합성곱의 합을 새로운 특성 맵으로 만들게 되며, 결국 위 그림의 경우 커널과 스트라이드의 상호작용으로 인해 원본 (5, 5, 1) 크기의 Feature Map아 (3, 3, 1)크기의 Feature Map의 크기로 줄어들게 되었다. 커널과 스트라이드의 경우 크기가 클 수 록 좀더 빨리 이미지를 처리할 수 있지만, 넓은 특성을 큰 보폭으로 이동하는 만큼 주요 특성을 놓칠 수 있다는 단점이 존재한다. 2.2 Padding합성곱 연산을 수행할 경우 단점이 존재하는데, 바로 위에서 살펴보았듯이 kernel과 stride의 작용으로 인해 원본 크기가 줄어든다는 것이다. 따라서 이렇게 Feature Map의 크기가 작아지는 것을 방지하기 위해서 Padding이란 기법을 이용하게 되는데, 쉽게 말해 단순히 원본 이미지에 0이라는 padding값을 채워 넣어 이미지를 확장한 후 합성곱 연산을 적용하는 것을 말한다. 위 그림을 보면 위에서 살펴본 바와 같이 똑같은 (5, 5, 1)크기의 이미지 데이터가 놓여있다. 다른 점은 사방으로 빈 공간(0)이 1칸씩 더 채워져 있다는 것인데 이것이 바로 padding이다. 이후 위와 똑같은 3 x 3 크기의 커널을 적용하게 되는데 출력되는 feature map의 크기는 (3, 3, 1)이 아닌 원본 이미지와 똑같은 (5, 5, 1)크기의 feature map이다. 이렇듯, 원본 이미지의 크기를 줄이지 않으면서 합성곱 연산을 수행가능하게 해주는 것이 바로 padding의 역할이라고 할 수 있다. 2.3 ReLU Activation Function합성곱 연산을 거친 Feature Map은 활성화 함수를 거치게 되는데, 많은 활성화 함수 중 가장 널리 사용되고 있는 것은 ReLU 함수 이다. 일반적으로 Sigmoid 함수의 경우 값을 0 ~ 1사이로 정규화시키는데 레이어가 깊어질 수 록 0.xxx의 값이 계속 미분되게 되면 값이 점차 0으로 수렴하게 되어 결국 weight값이 희미해지는 gradient vanishing문제가 발생하게 된다.하지만 ReLU의 경우 0미만의 값은 0으로 출력하고 0이상의 값은 그대로 출력하기 때문에 이러한 문제에 덜 민감하고 그렇기 때문에 깊은 레이어에서도 효율적인 역전파(Back Propagation)가 가능하다. 이러한 이유로 합성곱 연산을 통해 출력된 feature map의 경우 일반적으로 ReLU 활성화 함수를 거치게 되며, ReLU 함수가 양의 값만을 활성화하며 특징을 좀 더 두드러지게 표현해주게 된다. 3. Pooling LayerConvolutional Layer와 유사하게 feature map의 차원을 다운 샘플링하여 연산량을 감소시키고 주요한 특징 벡터를 추출하여 학습을 효과적으로 하는 것이 pooling layer의 역할이라고 할 수 있다. 풀링 연산에는 대표적으로 두 가지가 사용된다. Max Pooling : 각 커널에서 다루는 이미지 패치에서 최대값을 추출 Average Pooling: 각 커널에서 다루는 이미지 패치에서 모든 값의 평균을 반환 하지만 대부분의 ConvNet에서는 Avg Pooling이 아닌 Max Pooling이 사용된다. Avg Pooling의 경우 각 커널의 값을 평균화시키기 때문에 주요한 가중치를 갖는 value의 특성이 희미해질 수 있는 문제가 있기 때문이다. 또한 Pooling 사이즈의 경우 Stride와 같은 크기로 설정하여 모든 원소가 한번씩 처리되도록 하는것이 일반적이며, 보통 Max Pooling의 경우 2 x 2커널과 2 stride를 사용하여 feature map을 절반 크기로 다운샘플링하게 된다. 4. Fully Connected Layer위에서 설명한 Convolutional Layer - ReLU Activation Function - Pooling Layer의 과정을 거치며 차원이 축소 된 feature map은 최종적으로 Fully Connected Layer라는 완전 연결 층으로 전달되게 된다. 이 부분에서는 이미지의 3차원 벡터는 1차원으로 Flatten되게 되고 신경망에서 흔히 사용되는 활성화 함수(relu)와 함께 Output Layer로 학습이 진행된다.Output Layer는 Softmax 활성화 함수가 사용되는데, Softmax 함수는 입력받은 값을 모두 0 ~ 1사이의 값으로 정규화하하고 이렇게 정규화된 값들의 총합은 항상 1이되는 특성을 가지는 함수이다. 따라서 마지막 Output layer의 softmax함수를 통해 이미지가 각 레이블에 속할 확률값이 레이블마다 각각 출력되게 되고 이중 가장 높은 확률값을 가지는 레이블이 최종 예측치로 선정되게 된다. References https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53 https://sonofgodcom.wordpress.com/2018/12/31/cnn%EC%9D%84-%EC%9D%B4%ED%95%B4%ED%95%B4%EB%B3%B4%EC%9E%90-fully-connected-layer%EB%8A%94-%EB%AD%94%EA%B0%80/ https://medium.com/dataseries/basic-overview-of-convolutional-neural-network-cnn-4fcc7dbb4f17 https://de-novo.org/2018/05/27/convnet-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0/ https://reniew.github.io/10/","categories":[],"tags":[{"name":"cnn","slug":"cnn","permalink":"https://jaehyeongan.github.io/tags/cnn/"},{"name":"deeplearing","slug":"deeplearing","permalink":"https://jaehyeongan.github.io/tags/deeplearing/"},{"name":"convnet","slug":"convnet","permalink":"https://jaehyeongan.github.io/tags/convnet/"},{"name":"image","slug":"image","permalink":"https://jaehyeongan.github.io/tags/image/"},{"name":"convolutional","slug":"convolutional","permalink":"https://jaehyeongan.github.io/tags/convolutional/"},{"name":"kernel","slug":"kernel","permalink":"https://jaehyeongan.github.io/tags/kernel/"},{"name":"padding","slug":"padding","permalink":"https://jaehyeongan.github.io/tags/padding/"},{"name":"maxpooling","slug":"maxpooling","permalink":"https://jaehyeongan.github.io/tags/maxpooling/"},{"name":"relu","slug":"relu","permalink":"https://jaehyeongan.github.io/tags/relu/"}]},{"title":"[kaggle challenge] 분자 특성 예측(Predicting Molecular Properties)","slug":"molecular-prediction","date":"2019-09-06T14:20:52.000Z","updated":"2020-10-28T15:14:34.482Z","comments":true,"path":"2019/09/06/molecular-prediction/","link":"","permalink":"https://jaehyeongan.github.io/2019/09/06/molecular-prediction/","excerpt":"","text":"Intro최근 kaggle에서 굉장히 눈에 띄는 competition이 있었으니 바로, Predicting Molecular Properties라는 이름의 대회였다. 해당 competition은 브리스톨 대학교, 카디프 대학교, 임페리얼 칼리지 및 리즈 대학교로 이루어진 CHAMPS(CHemistry And Mathematics in Phase Space) 에 의해 주최되었으며, 수상하는 팀에게는 대학 연구 프로그램과 협력할 수 있는 기회가 주어진다고 한다. 예측 대상우선 해당 대회의 도전과제는 소제목 및 Description을 통해 파악할 수 있다. Can you measure the magnetic interactions between a pair of atoms?In this competition, you will develop an algorithm that can predict the magnetic interaction between two atoms in a molecule 이번 대회를 통해 우리가 예측 해야 하는 것은 바로 분자 설계 시 한 쌍의 원자 간 결합으로 인해 발생하는 결합상수(Coupling Constant)를 예측하는 것이다.결합 상수라는 것은 물리적 상호작용(여기서는 원자 간)의 세기를 나타내는 상수로서, 결합상수가 1일 때 완전결합이라고 한다. 아래에서 좀 더 자세히 살펴보겠지만, 제공되는 데이터에는 분자 및 원자에 대한 정보가 있으며 두 원자 간의 결합상수가 target value로 존재하고 있다. 학습 전략처음 제공된 데이터를 보았을 때 train, test 외에 추가로 제공되는 데이터를 어떻게 활용해야 할지 난감했다. 그 이유는 structures 데이터를 제외하고는 모두 train 데이터에 대한 정보 밖에 없었기 떄문이다. 모델을 학습하고 예측할 때 당연히 train set과 test set의 차원의 크기가 같아야 했기 때문에 train에 대한 정보만 있는 데이터를 활용하는 것이 의미가 없다고 판단되었다. 그래서 최대한 활용할 수 있는 데이터만 사용하였으며 몇 가지 파생변수를 만들어 부족한 차원을 채워주었다. 모델 학습을 위해서는 LightGBM이라는 최근 캐글에서 가장 인기 있는 Gradient Boosting 기반의 모델을 사용하였다. 해당 데이터에는 type 이라는 분자의 타입을 구분하는 칼럼이 존재하는데, 처음 모델을 만들 때는 type 구분 없이 전체를 학습시켰으나 성능이 기대만큼 잘 나오지 않았다. 그래서 feature를 늘려야 하나 고민하던 중 우연히 Nanashi라는 사람의 kernel에서 전체 분자를 학습시키지 않고 분자의 type별로 따로 학습 및 예측을 진행하는 것을 보게 되었다. score를 보니 상당히 높은 score를 가지고 있었고 시도해볼 만 한 가치가 있다고 판단되어 이번 모델에 벤치마킹하여 적용하였다. 평가 방법이번 대회에서는 Evaluation을 위해 평균절대오차(MAE, Mean Absolute Error)에 log값을 씌운 점수로 평가를 진행하게 된다. 공식 metric은 아래와 같으며, 완벽하게 예측했을 때 최종 점수는 -20.7232이다. !코드 작성은 Jupyter lab을 이용하였으며, 아래 작성된 코드는 ipynb파일을 markdown으로 변환 후 업로드한 것이다. 1234567891011121314import pandas as pdimport numpy as npimport osimport seaborn as snsimport matplotlib.pyplot as pltfrom mpl_toolkits.mplot3d import Axes3Dfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import LabelEncoderimport lightgbm as lgb# pathpath_dir = 'C:/Users/USER/.kaggle/competitions/champs-scalar-coupling/'file_list = os.listdir(path_dir)file_list [‘dipole_moments.csv’, ‘magnetic_shielding_tensors.csv’, ‘mulliken_charges.csv’, ‘potential_energy.csv’, ‘sample_submission.csv’, ‘scalar_coupling_contributions.csv’, ‘structures.csv’, ‘structures.zip’, ‘test.csv’, ‘train.csv’] 1. Load Train/Test DataColumns molecule_name : 분자 이름 atom_index_0 / atom_index_1 : 원자 인덱스 type Coupling Constant(결합상수) : 물리적 상호작용(여기서는 원자 간)의 세기를 나타내는 상수, 결합상수가 1일때 완전결합이라고 함 12345train_df = pd.read_csv(path_dir+'train.csv')test_df = pd.read_csv(path_dir+'test.csv') # target = 'scalar_coupling_constant'print('Length of train set: &#123;&#125;'.format(len(train_df)))print('Length of test set: &#123;&#125;'.format(len(test_df))) Length of train set: 4658147Length of test set: 2505542 12print('Unique molecule of train set: &#123;&#125;'.format(len(train_df['molecule_name'].unique())))train_df.head() Unique molecule of train set: 85003 12print('Unique molecule of test set: &#123;&#125;'.format(len(test_df['molecule_name'].unique())))test_df.head() Unique molecule of test set: 45772 2. EDA2.1 Distribution of Target (‘scalar_coupling_constant’) Min Value : -36.2186 Max Value : 204.88 대부분이 -20 ~ +20 사이에 존재 작은 분포로 80 ~ 100 사이에 존재 12345678# Distribution of targetprint('Min Value of Target : &#123;&#125;'.format(train_df['scalar_coupling_constant'].min()))print('Max Value of Target : &#123;&#125;'.format(train_df['scalar_coupling_constant'].max()))plt.figure(figsize=(11, 5))sns.distplot(train_df['scalar_coupling_constant'])plt.title('Distribution of scalar_coupling_constant')plt.show() Min Value of Target : -36.2186Max Value of Target : 204.88 2.2 Distribution of ‘scalar_coupling_constant’ by type ‘1JHC’ type이 상대적으로 높은 scalar coupling 범위에 분포(+66.6 ~ +204.8) ‘2JHH’ type이 상대적으로 낮은 scalar coupling 범위에 분포(-35.1 ~ +11.8 1234567# Distribution of 'scalar_coupling_constant' by typeplt.figure(figsize=(14, 13))for i, t in enumerate(train_df['type'].unique()): plt.subplot(4,2, i+1) sns.distplot(train_df[train_df['type'] == t]['scalar_coupling_constant']) plt.title('Distribution of coupling constant by type '+ t) plt.tight_layout() 2.3 Count by ‘type’ 3JHC, 2JHC, 1JHC, 3JHH, 2JHH, 3JHN, 2JHN, 1JHN 순서로 높음 123456789# Count by 'type'type_index = train_df['type'].value_counts().indextype_cnt = train_df['type'].value_counts()plt.figure(figsize=(11, 4))sns.barplot(x=type_index, y=type_cnt)plt.xlabel('type'); plt.ylabel('Count')plt.title('Count by type')plt.tight_layout() 2.4 Count by atom index 0, 1 atom index 0의 경우 9 ~ 18번이 가장 많이 분포 atom index 1의 경우 1 ~ 8번이 가장 많이 분포 12345678910# Count by atom index 0, 1for i in [0, 1]: atom_index = train_df['atom_index_'+str(i)].value_counts().index atom_cnt = train_df['atom_index_'+str(i)].value_counts() plt.figure(figsize=(11, 4)) sns.barplot(x=atom_index, y=atom_cnt) plt.xlabel('atom index '+str(i)); plt.ylabel('Count') plt.title('Count by atom index '+str(i)) plt.tight_layout() 3. Load Structures DataColumns molecule_name atom_index atom x, y, z axis of atom 1234structures_df = pd.read_csv(path_dir+'structures.csv')print('Length of test set: &#123;&#125;'.format(len(structures_df)))structures_df.head() Length of test set: 2358657 3.1. 3Dimension plot by Molecule1234567891011for name in structures_df['molecule_name'].unique()[:4]: structures_molecule =structures_df[structures_df['molecule_name'] == name] fig = plt.figure(figsize=(8, 5)) ax = fig.add_subplot(111, projection='3d') ax.scatter(structures_molecule['x'], structures_molecule['y'], structures_molecule['z'], s=200, edgecolors='white') ax.set_title(str(name)+ ' 3D plot') ax.set_xlabel('x') ax.set_ylabel('y') ax.set_zlabel('z') plt.show() 4. Preprocessing4.1. Merge Train&amp;Test - Structures Data12345678910111213def mapping_atom_index(df, atom_idx): atom_idx = str(atom_idx) df = pd.merge(df, structures_df, left_on = ['molecule_name', 'atom_index_'+atom_idx], right_on = ['molecule_name', 'atom_index'], how = 'left') df = df.drop('atom_index', axis=1) df = df.rename(columns=&#123;'atom': 'atom_'+atom_idx, 'x': 'x_'+atom_idx, 'y': 'y_'+atom_idx, 'z': 'z_'+atom_idx&#125;) return df 12345train_merge = mapping_atom_index(train_df, 0)train_merge = mapping_atom_index(train_merge, 1)test_merge = mapping_atom_index(test_df, 0)test_merge = mapping_atom_index(test_merge, 1) 1234train_tmp = train_merge[['id','molecule_name','type']]test_tmp = test_merge[['id','molecule_name','type']]train_merge.head() 4.2. Derived variables - ‘Distance’ distance between x axis of atom index distance between y axis of atom index distance between z axis of atom index distance between atom 12345678910111213def dist_between_atom(df): # distance between axis of atom df['x_dist'] = (df['x_0'] - df['x_1'])**2 df['y_dist'] = (df['y_0'] - df['y_1'])**2 df['z_dist'] = (df['z_0'] - df['z_1'])**2 # distance between atom df['atom_dist'] = (df['x_dist']+df['y_dist']+df['z_dist'])**0.5 return df train_dist = dist_between_atom(train_merge)test_dist = dist_between_atom(test_merge) 1train_dist.head() 4.3. Label encoding type, atom_0, atom_1 12345678910# Label encodingcategorical_features = ['type', 'atom_0', 'atom_1']for col in categorical_features: le = LabelEncoder() le.fit(list(train_dist[col].values) + list(test_dist[col].values)) train_dist[col] = le.transform(list(train_dist[col].values)) test_dist[col] = le.transform(list(test_dist[col].values))train_le = train_dist.copy()test_le = test_dist.copy() 1train_le.head() 4.4. Standardization z = (x - u) / s 12345# traintrain_data = train_le.drop(['id','molecule_name','scalar_coupling_constant'], axis=1)train_target = train_le['scalar_coupling_constant']# testtest_data = test_le.drop(['id','molecule_name',], axis=1) 1234# z-score standardizationtrain_scale = (train_data - train_data.mean()) / train_data.std()train_scale = train_scale.fillna(0)test_scale = (test_data - train_data.mean()) / train_data.std() 4.5. Variable Correlations123456789train_corr = train_scale.copy()train_corr['scalar_coupling_constant'] = train_targetcorrmat = train_corr.corr()top_corr_features = corrmat.index[abs(corrmat['scalar_coupling_constant']) &gt;= 0.1]plt.figure(figsize=(10,7))sns.heatmap(train_corr[top_corr_features].corr(), annot=True, cmap=\"RdYlGn\")plt.title('Variable Correlations')plt.show() 5. Training Model5.1. Training by ‘type’ through LightGBM123456train_scale = train_scale.drop('type', axis=1)train_scale['type'] = train_tmp['type']train_scale['scalar_coupling_constant'] = train_targettest_scale = test_scale.drop('type', axis=1)test_scale[['id', 'type']] = test_tmp[['id', 'type']] 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859score_by_type = [] # List of Validation score by type feature_importance_df = []test_pred_df = pd.DataFrame(columns=['id', 'scalar_coupling_constant']) # Dataframe for submission# Extract data by typetypes = train_tmp['type'].unique()for typ in types: print('---Type of '+str(typ)+'---') train = train_scale[train_scale['type'] == typ] target = train['scalar_coupling_constant'] train = train.drop(['type','scalar_coupling_constant'], axis=1) # Split train set / valid set x_train, x_val, y_train, y_val = train_test_split(train, target, random_state=42) # LightGBM categorical_features = ['atom_0','atom_1'] lgb_train = lgb.Dataset(x_train, y_train, categorical_feature=categorical_features) lgb_val = lgb.Dataset(x_val, y_val, categorical_feature=categorical_features) # Parameters of LightGBM params = &#123;'num_leaves': 128, 'min_child_samples': 79, 'objective': 'regression', 'max_depth': 9, 'learning_rate': 0.1, \"boosting_type\": \"gbdt\", \"subsample_freq\": 1, \"subsample\": 0.9, \"bagging_seed\": 11, \"metric\": 'mae', \"verbosity\": -1, 'reg_alpha': 0.13, 'reg_lambda': 0.36, 'colsample_bytree': 1.0 &#125; # Training lgb_model = lgb.train(params, lgb_train, valid_sets=[lgb_train, lgb_val], num_boost_round=20000, # Number of boosting iterations. early_stopping_rounds=500, # early stopping for valid set verbose_eval=2500) # eval metric on the valid set is printed at 1000 each boosting # Feature Importances feature_importance = lgb_model.feature_importance() df_fi = pd.DataFrame(&#123;'columns':x_train.columns, 'importances':feature_importance&#125;) df_fi = df_fi[df_fi['importances'] &gt; 0].sort_values(by=['importances'], ascending=False) feature_importance_df.append(df_fi) # Predict Validation set score_by_type.append(list(lgb_model.best_score['valid_1'].values())) # Predict Test set test = test_scale[test_scale['type'] == typ] test_id = test['id'] test = test.drop(['id','type'], axis=1) test_preds = lgb_model.predict(test) test_pred_df = pd.concat([test_pred_df, pd.DataFrame(&#123;'id':test_id, 'scalar_coupling_constant':test_preds&#125;)], axis=0) —-Type of 1JHC—-Training until validation scores don’t improve for 500 rounds.[2500] training’s l1: 2.77031 valid_1’s l1: 3.67976[5000] training’s l1: 2.19648 valid_1’s l1: 3.59536[7500] training’s l1: 1.81083 valid_1’s l1: 3.56509[10000] training’s l1: 1.52513 valid_1’s l1: 3.55207[12500] training’s l1: 1.30189 valid_1’s l1: 3.54733Early stopping, best iteration is:[12398] training’s l1: 1.31009 valid_1’s l1: 3.54716—-Type of 2JHH—-Training until validation scores don’t improve for 500 rounds.[2500] training’s l1: 0.599486 valid_1’s l1: 0.930904[5000] training’s l1: 0.429952 valid_1’s l1: 0.920848Early stopping, best iteration is:[6444] training’s l1: 0.363731 valid_1’s l1: 0.919744—-Type of 1JHN—-Training until validation scores don’t improve for 500 rounds.[2500] training’s l1: 0.574269 valid_1’s l1: 1.869Early stopping, best iteration is:[2790] training’s l1: 0.51238 valid_1’s l1: 1.86748—-Type of 2JHN—-Training until validation scores don’t improve for 500 rounds.[2500] training’s l1: 0.493351 valid_1’s l1: 1.29156[5000] training’s l1: 0.245368 valid_1’s l1: 1.26562[7500] training’s l1: 0.136667 valid_1’s l1: 1.25931[10000] training’s l1: 0.0806357 valid_1’s l1: 1.25729[12500] training’s l1: 0.0501038 valid_1’s l1: 1.25649[15000] training’s l1: 0.0325061 valid_1’s l1: 1.25602Early stopping, best iteration is:[16603] training’s l1: 0.025108 valid_1’s l1: 1.25588—-Type of 2JHC—-Training until validation scores don’t improve for 500 rounds.[2500] training’s l1: 1.45707 valid_1’s l1: 1.78894[5000] training’s l1: 1.22333 valid_1’s l1: 1.74951[7500] training’s l1: 1.0595 valid_1’s l1: 1.734[10000] training’s l1: 0.931867 valid_1’s l1: 1.72621[12500] training’s l1: 0.827693 valid_1’s l1: 1.7222[15000] training’s l1: 0.740337 valid_1’s l1: 1.72091[17500] training’s l1: 0.665914 valid_1’s l1: 1.7203Early stopping, best iteration is:[17624] training’s l1: 0.662523 valid_1’s l1: 1.72024—-Type of 3JHH—-Training until validation scores don’t improve for 500 rounds.[2500] training’s l1: 0.750457 valid_1’s l1: 1.084[5000] training’s l1: 0.549578 valid_1’s l1: 1.04283[7500] training’s l1: 0.427516 valid_1’s l1: 1.02508[10000] training’s l1: 0.343351 valid_1’s l1: 1.01595[12500] training’s l1: 0.281535 valid_1’s l1: 1.01117[15000] training’s l1: 0.234395 valid_1’s l1: 1.00805[17500] training’s l1: 0.197292 valid_1’s l1: 1.00612[20000] training’s l1: 0.167506 valid_1’s l1: 1.00516Did not meet early stopping. Best iteration is:[20000] training’s l1: 0.167506 valid_1’s l1: 1.00516—-Type of 3JHC—-Training until validation scores don’t improve for 500 rounds.[2500] training’s l1: 1.14564 valid_1’s l1: 1.35361[5000] training’s l1: 0.957949 valid_1’s l1: 1.28948[7500] training’s l1: 0.830959 valid_1’s l1: 1.2569[10000] training’s l1: 0.735763 valid_1’s l1: 1.23707[12500] training’s l1: 0.65866 valid_1’s l1: 1.22372[15000] training’s l1: 0.594712 valid_1’s l1: 1.2142[17500] training’s l1: 0.540621 valid_1’s l1: 1.2074[20000] training’s l1: 0.493743 valid_1’s l1: 1.20232Did not meet early stopping. Best iteration is:[20000] training’s l1: 0.493743 valid_1’s l1: 1.20232—-Type of 3JHN—-Training until validation scores don’t improve for 500 rounds.[2500] training’s l1: 0.231975 valid_1’s l1: 0.513043[5000] training’s l1: 0.127354 valid_1’s l1: 0.502675[7500] training’s l1: 0.0764305 valid_1’s l1: 0.49964[10000] training’s l1: 0.0486108 valid_1’s l1: 0.498466[12500] training’s l1: 0.0325766 valid_1’s l1: 0.498049[15000] training’s l1: 0.0228697 valid_1’s l1: 0.497765[17500] training’s l1: 0.0167369 valid_1’s l1: 0.497581[20000] training’s l1: 0.0128106 valid_1’s l1: 0.497526Did not meet early stopping. Best iteration is:[20000] training’s l1: 0.0128106 valid_1’s l1: 0.497526 5.2. Validation MAE by type1234for typ, score in zip(types, score_by_type): print('Type &#123;&#125; valid MAE : &#123;&#125;'.format(str(typ), score))print('\\nAverage of valid MAE : &#123;&#125;'.format(np.mean(score_by_type))) Type 1JHC valid MAE : [3.5471584407190475]Type 2JHH valid MAE : [0.9197439377103146]Type 1JHN valid MAE : [1.8674786631630775]Type 2JHN valid MAE : [1.255876548899015]Type 2JHC valid MAE : [1.7202390170123096]Type 3JHH valid MAE : [1.0051635344922942]Type 3JHC valid MAE : [1.2023186835296467]Type 3JHN valid MAE : [0.4975260038664571] Average of valid MAE : 1.5019381036740203 5.3. Feature Importances Plot by Type1234567for typ, df_fi in zip(types, feature_importance_df): fig = plt.figure(figsize=(12, 6)) ax = sns.barplot(df_fi['columns'], df_fi['importances']) ax.set_xticklabels(df_fi['columns'], rotation=80, fontsize=13) plt.title('Type '+str(typ)+' feature importance') plt.tight_layout() plt.show() 5.4. Save prediction of test set to *.csv1test_pred_df.head(10) 1test_pred_df.to_csv('lgb_submission.csv', index=False) Referenceskaggle kernels https://www.kaggle.com/jesucristo/single-lgbm-2-242-top54 https://www.kaggle.com/super13579/simple-eda-and-lightgbm https://www.kaggle.com/artgor/molecular-properties-eda-and-models blog/docs https://gorakgarak.tistory.com/1285 https://towardsdatascience.com/understanding-gradient-boosting-machines-using-xgboost-and-lightgbm-parameters-3af1f9db9700 https://lightgbm.readthedocs.io/en/latest/ Outro처음 예측 모델을 위해서 Neural Net을 이용하였었는데 default parameter의 Random Forest 알고리즘보다도 훨씬 낮은 성능을 보였다. 어느 글에서 말하길, 딥러닝이 항상 좋은 성능을 내지 않는다고 한다.이런 Structured tabular 형태의 데이터에 neural net은 over-fitting 하는 경우가 많으며, 거의 대부분의 경우 xgboost나 lightgbm과 같은 gradient boosting 계열의 알고리즘이 잘 작동한다고 한다.(파라미터값을 잘 optimize 했을 때..) 앞으로 gbm 계열의 알고리즘에 대해 좀더 공부해봐야할 것 같다.","categories":[],"tags":[{"name":"kaggle","slug":"kaggle","permalink":"https://jaehyeongan.github.io/tags/kaggle/"},{"name":"molecular","slug":"molecular","permalink":"https://jaehyeongan.github.io/tags/molecular/"},{"name":"atom","slug":"atom","permalink":"https://jaehyeongan.github.io/tags/atom/"},{"name":"couplingconstant","slug":"couplingconstant","permalink":"https://jaehyeongan.github.io/tags/couplingconstant/"},{"name":"competitions","slug":"competitions","permalink":"https://jaehyeongan.github.io/tags/competitions/"},{"name":"lightgbm","slug":"lightgbm","permalink":"https://jaehyeongan.github.io/tags/lightgbm/"},{"name":"eda","slug":"eda","permalink":"https://jaehyeongan.github.io/tags/eda/"}]},{"title":"데이터 분석을 위한 기초 시각화 with Python","slug":"Basic-Visualization-Analytics","date":"2019-08-13T14:00:16.000Z","updated":"2020-10-28T15:14:34.472Z","comments":true,"path":"2019/08/13/Basic-Visualization-Analytics/","link":"","permalink":"https://jaehyeongan.github.io/2019/08/13/Basic-Visualization-Analytics/","excerpt":"","text":"Intro데이터를 분석하려는데 데이터의 row와 columns 수가 많은 수백 차원 데이터의 경우 데이터를 파악하기가 쉽지 않다. 그렇기에 인간이 이해할 수 있는 정도의 차원으로 줄여 데이터를 개략적으로 파악하는 것이 필요하고, 역시 인간은 읽고, 듣는 것 보다는 눈으로 보는게 확실히 기억에 오래남고 이해하기 쉽기 때문에 데이터를 시각화하여 분석하는 것이 필요하다. 이번에는 데이터 분석에 앞서 기초적이지만 필수적으로 살펴보아 할 시각화 방법에 대해 살펴볼 것이며, 목록은 아래와 같다. 변수 별 데이터 분포(Data Distribution) 타겟 별 2차원 및 3차원 시각화(2D and 3D plot) 변수 간 상관관계(Corrleation) 변수 중요도(Featrue Importances) 예제로 사용 할 데이터로 Breast Cancer Wisconsin Dataset이다. 많이들 알다시피 유방암에 대해 양성/음성을 예측하기 위한 데이터셋이며, 총 569 row와 31 columns을 가지고 있다. 0. Load Data우선 데이터를 로드 시킨 후 분석에 불필요한 칼럼은 제외시킨다.데이터는 아래와 같은 형태로 되어 있다.12345import pandas as pd cancer = pd.read_csv('./input/data.csv')cancer.drop(['id','Unnamed: 32'], axis=1, inplace=True)cancer.head(10) 이 데이터에서 예측해야하는 타겟 칼럼은 ‘diagnosis’이며, ‘M’은 malignant로 양성을 의미하며, ‘B’는 Benign으로 음성을 의미한다.1cancer['diagnosis'].unique() # array(['M', 'B'], dtype=object) 1. Column distribution by target먼저 시각화 해 볼 것은 칼럼 별로 데이터 분포를 시각화해 보는 것이다. 이를 통해 각 칼럼 별로 데이터가 어떻게 분포되어 있는지를 파악할 수 있고, 우리가 예측하고자 하는 타겟(diagnosis)별로 분포가 어떻게 다르게 나타나는지도 파악이 가능하다. seaborn의 distplot을 통해 타겟 칼럼인 diagnosis별로 6번째 칼럼까지만 출력해보았다.1234567891011121314151617181920import seaborn as snsimport matplotlib.pyplot as pltfrom matplotlib import font_manager, rcfont_name = font_manager.FontProperties(fname=\"c:/Windows/Fonts/malgun.ttf\").get_name()rc('font', family=font_name) # 한글 출력 설정 부분for cnt, col in enumerate(cancer): try: plt.figure(figsize=(10, 5)) sns.distplot(cancer[col][cancer['diagnosis']=='M']) sns.distplot(cancer[col][cancer['diagnosis']=='B']) plt.legend(['malignant','benign'], loc='best') plt.title('histogram of features '+str(col)) plt.show() if cnt &gt;= 6: # 6개 칼럼까지만 출력 break except Exception as e: pass 위의 그림으로 보았을 때 radius_mean, area_mean, perimeter_mean 칼럼이 양성일때와 음성일때 분포가 크게 다른 것을 알 수 있고, 특히 area_mean 칼럼은 분포가 넓게 퍼져있는 것을 알 수 있다. 2. 2 Dimension Plot이번에는 지난 글에서 살펴보았던 차원축소(Dimensionality Reduction) 기법을 이용하여 2차원으로 데이터를 시각화하는 방법에 대해 알아보겠다. 우선 데이터 스케일 및 차원축소 기법인 PCA(Principal Component Analysis)를 적용하여 데이터를 2차원으로 변환시켜준 후, 타겟(음성/양성)별로 데이터를 구분하여 출력하였다. 12345678from sklearn.preprocessing import StandardScaler# Data ScalingX = cancer.drop(['diagnosis'], axis=1)y = cancer['diagnosis']scaler = StandardScaler()cancer_scale = pd.DataFrame(scaler.fit_transform(X), columns=X.columns) 12345678910111213# plot 2Dfrom sklearn.decomposition import PCAimport matplotlib.pyplot as pltpca2 = PCA(n_components=2)data_pca2 = pca2.fit_transform(cancer_scale)plt.figure(figsize=(12, 8))plt.scatter(data_pca2[:,0], data_pca2[:,1], c=cancer['diagnosis'], s=40, edgecolors='white')plt.title(\"2D of Target distribution by diagnosis\")plt.xlabel('pcomp 1')plt.ylabel('pcomp 2')plt.show() 2차원으로 표현해본 결과 양성일때와 음성일 때 극명하게 분포가 나뉘는 것을 확인해볼 수 있다. 3. 3 Dimension Plot위와 같은 방식으로 PCA를 이용하여 데이터를 3차원으로 변환 후 데이터를 타겟(양성/음성) 별로 시각화 한다.123456789101112131415from sklearn.decomposition import PCAimport matplotlib.pyplot as pltfrom mpl_toolkits.mplot3d import Axes3Dpca3 = PCA(n_components=3)data_pca3 = pca3.fit_transform(cancer_scale)fig = plt.figure(figsize=(12, 8))ax = fig.add_subplot(111, projection='3d')ax.scatter(data_pca3[:,0], data_pca3[:,1], data_pca3[:,2], c=cancer['diagnosis'], s=60, edgecolors='white')ax.set_title('3D of Target distribution by diagnosis')ax.set_xlabel('pcomp 1')ax.set_ylabel('pcomp 2')ax.set_zlabel('pcomp 3')plt.show() 4. Corrleation Heatmap이번에는 상관관계 분석을 통해 변수 간 상관관계가 얼마나 있는지 파악해본다. 이러한 상관관계 분석을 통해 타겟 값을 제외한 특정 두 변수가 상관관계가 0.9 이상일 경우 두 변수 중 하나를 제거해주는 것이 좋으며, 또한 어떤 변수가 타겟 값과 높은 상관성을 가지는지 파악하는데도 유용하게 사용된다. corr()함수를 적용하여 변수간 상관관계 분석 후, 상관관계가 0.3이상인 변수만 출력하였다.123456789101112131415import seaborn as snsimport matplotlib.pyplot as pltfrom matplotlib import font_manager, rcfont_name = font_manager.FontProperties(fname=\"c:/Windows/Fonts/malgun.ttf\").get_name()rc('font', family=font_name) # 한글 출력 설정 부분cancer_tmp = cancer.copy()cancer_tmp['diagnosis'] = cancer['diagnosis'].replace(&#123;'M':1, 'B':0&#125;)corrmat = cancer_tmp.corr()top_corr_features = corrmat.index[abs(corrmat[\"diagnosis\"])&gt;=0.3]# plotplt.figure(figsize=(13,10))g = sns.heatmap(cancer[top_corr_features].corr(), annot=True, cmap=\"RdYlGn\")plt.show() 5. Feature Importances머신러닝 및 딥러닝 예측 후 어떻게 이러한 결과가 나왔는지 의문이 들 때가 있다. 그럴 땐 변수 중요도를 통해 어떤 변수가 예측 성능에 주요하게 영향을 미쳤는지 파악할 수 있다. RandomForest 알고리즘을 통해 feature importances를 뽑아낸 후 상위 중요도 별로 중요도가 0이상만 출력하였다.12345678910111213141516171819202122from sklearn.ensemble import RandomForestClassifierimport seaborn as snsimport matplotlib.pyplot as pltfrom matplotlib import font_manager, rcfont_name = font_manager.FontProperties(fname=\"c:/Windows/Fonts/malgun.ttf\").get_name()rc('font', family=font_name) # 한글 출력 설정 부분# RandomForestclf = RandomForestClassifier(random_state=42, max_depth=6)clf.fit(X, y)feature_importance = clf.feature_importances_# plotdf_fi = pd.DataFrame(&#123;'columns':X.columns, 'importances':feature_importance&#125;)df_fi = df_fi[df_fi['importances'] &gt; 0] # importance가 0이상인 것만 df_fi = df_fi.sort_values(by=['importances'], ascending=False)fig = plt.figure(figsize=(15,7))ax = sns.barplot(df_fi['columns'], df_fi['importances'])ax.set_xticklabels(df_fi['columns'], rotation=80, fontsize=13)plt.tight_layout()plt.show() 변수 중요도 출력결과 concave_points_worst가 0.175로 가장 중요한 예측 변수이며, 그 뒤로 perimeter_worst, perimeter_mean, radius_word가 주요 예측 변수로 나타났다. Outro이번에는 기초적인 데이터 시각화를 알아보았는데, 데이터 시각화는 데이터와 상황에 따라 그때 그때 시각화해야하는 요소가 다르고 다양하기 때문에, 상황에 따라 원하는 그림을 그리며 자신만의 인사이트를 찾아나가면 될 듯 하다.","categories":[],"tags":[{"name":"visualization","slug":"visualization","permalink":"https://jaehyeongan.github.io/tags/visualization/"},{"name":"analytics","slug":"analytics","permalink":"https://jaehyeongan.github.io/tags/analytics/"},{"name":"matplotlib","slug":"matplotlib","permalink":"https://jaehyeongan.github.io/tags/matplotlib/"},{"name":"seaborn","slug":"seaborn","permalink":"https://jaehyeongan.github.io/tags/seaborn/"},{"name":"sklearn","slug":"sklearn","permalink":"https://jaehyeongan.github.io/tags/sklearn/"},{"name":"pca","slug":"pca","permalink":"https://jaehyeongan.github.io/tags/pca/"},{"name":"correaltion","slug":"correaltion","permalink":"https://jaehyeongan.github.io/tags/correaltion/"},{"name":"featureimportances","slug":"featureimportances","permalink":"https://jaehyeongan.github.io/tags/featureimportances/"},{"name":"breastcancer","slug":"breastcancer","permalink":"https://jaehyeongan.github.io/tags/breastcancer/"}]},{"title":"[Kaggle challenge] 보스턴 주택 가격 예측(House Prices: Advanced Regression Techniques)","slug":"Kaggle-challenge-보스턴-집값-예측-House-Prices-Advanced-Regression-Techniques","date":"2019-07-08T12:59:56.000Z","updated":"2020-10-28T15:14:34.474Z","comments":true,"path":"2019/07/08/Kaggle-challenge-보스턴-집값-예측-House-Prices-Advanced-Regression-Techniques/","link":"","permalink":"https://jaehyeongan.github.io/2019/07/08/Kaggle-challenge-보스턴-집값-예측-House-Prices-Advanced-Regression-Techniques/","excerpt":"","text":"Intro캐글의 고전적인 문제이며 머신러닝을 공부하는 사람이라면 누구나 한번쯤 다뤄봤을 Boston house price Dataset을 통해 regression하는 과정을 소개하려 한다. 정식 competition 명칭은 ‘House Prices: Advanced Regression Techniques’이며, 현재 누구나 submission을 제출할 수 있다. 위에서 말했듯이 boston house price데이터셋은 왠만한 머신러닝 공부하는 사람들은 한번쯤 봤을 것이며, 대부분의 머신러닝 입문 교재에도 꼭 한번씩은 소개가 되는 데이터셋이다. 하지만, 대부분의 교재나 강의에서는 이미 feature engineering을 거친 아주 잘 정형화 된(모델에 바로 적용 가능한)데이터셋을 사용하며 데이터 처리 과정은 생략하는 경우가 대부분인 것 같다. 하지만 boston house price 데이터셋은 무려 81개의 다양한 칼럼 변수를 가지고 있으며, 각 칼럼 특성에 맞는 전처리가 필요하다. 따라서, 여기서는 boston house price 데이터셋에 어떻게 적절한 feature engineering을 적용하고, 최근 kaggle에서 가장 인기 있는 모델인 XGBoost 모델을 어떻게 적용하였는지 소개한다. Import Library1234import numpy as np import pandas as pdimport matplotlib.pyplot as pltimport seaborn as sns 1. Load Data 데이터의 경우 train, test set이 분리되어 제공되며 test set에 대한 예측결과가 추후 submission으로 제출된다. train 데이터의 경우 1,460건, test 데이터가 1459건이며 총 81개의 칼럼을 가진다. 12345# laod datatrain_df = pd.read_csv('house_train.csv')test_df = pd.read_csv('house_test.csv')train_df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Id MSSubClass MSZoning LotFrontage LotArea Street Alley LotShape LandContour Utilities ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold YrSold SaleType SaleCondition SalePrice 0 1 60 RL 65.0 8450 Pave NaN Reg Lvl AllPub ... 0 NaN NaN NaN 0 2 2008 WD Normal 208500 1 2 20 RL 80.0 9600 Pave NaN Reg Lvl AllPub ... 0 NaN NaN NaN 0 5 2007 WD Normal 181500 2 3 60 RL 68.0 11250 Pave NaN IR1 Lvl AllPub ... 0 NaN NaN NaN 0 9 2008 WD Normal 223500 3 4 70 RL 60.0 9550 Pave NaN IR1 Lvl AllPub ... 0 NaN NaN NaN 0 2 2006 WD Abnorml 140000 4 5 60 RL 84.0 14260 Pave NaN IR1 Lvl AllPub ... 0 NaN NaN NaN 0 12 2008 WD Normal 250000 5 rows × 81 columns 12345# set indextrain_df.set_index('Id', inplace=True)test_df.set_index('Id', inplace=True)len_train_df = len(train_df)len_test_df = len(test_df) 2. Feature Selection - Variables of Corrleation &gt;= 0.3 고려해야 할 변수가 많을 땐 각 독립변수와 종속변수 간의 상관관계(Corrleation)을 검토해보는 것이 좋다. 모든 변수를 사용하는 것도 좋지만 그 중 좀더 의미 있는 변수만을 골라내어 모델을 구축하는 것이 모델의 예측 정확도를 높이는 방법이다. corr()함수를 통해 dataframe내의 모든 변수간의 상관관계를 그린 후 상관관계가 0.3이상인 변수만 heatmap으로 출력하였다. 123corrmat = train_df.corr()top_corr_features = corrmat.index[abs(corrmat[\"SalePrice\"])&gt;=0.3]top_corr_features Index([&#39;LotFrontage&#39;, &#39;OverallQual&#39;, &#39;YearBuilt&#39;, &#39;YearRemodAdd&#39;, &#39;MasVnrArea&#39;, &#39;BsmtFinSF1&#39;, &#39;TotalBsmtSF&#39;, &#39;1stFlrSF&#39;, &#39;2ndFlrSF&#39;, &#39;GrLivArea&#39;, &#39;FullBath&#39;, &#39;TotRmsAbvGrd&#39;, &#39;Fireplaces&#39;, &#39;GarageYrBlt&#39;, &#39;GarageCars&#39;, &#39;GarageArea&#39;, &#39;WoodDeckSF&#39;, &#39;OpenPorchSF&#39;, &#39;SalePrice&#39;], dtype=&#39;object&#39;) 123# heatmapplt.figure(figsize=(13,10))g = sns.heatmap(train_df[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\") 123# feature selection# train_df = train_df[top_corr_features]# test_df = test_df[top_corr_features.drop(['SalePrice'])] 123# split y_labeltrain_y_label = train_df['SalePrice'] # target 값을 미리 분리하였음.train_df.drop(['SalePrice'], axis=1, inplace=True) 3. Concat train &amp; test set train과 test 셋에 동일한 feature engineering을 적용해주기 위해 우선 두개의 데이터 셋을 하나로 합쳐주었다. 합쳐주니 2,919개로 데이터가 늘었다. 123456# concat train &amp; testboston_df = pd.concat((train_df, test_df), axis=0)boston_df_index = boston_df.indexprint('Length of Boston Dataset : ',len(boston_df))boston_df.head() Length of Boston Dataset : 2919 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } MSSubClass MSZoning LotFrontage LotArea Street Alley LotShape LandContour Utilities LotConfig ... ScreenPorch PoolArea PoolQC Fence MiscFeature MiscVal MoSold YrSold SaleType SaleCondition Id 1 60 RL 65.0 8450 Pave NaN Reg Lvl AllPub Inside ... 0 0 NaN NaN NaN 0 2 2008 WD Normal 2 20 RL 80.0 9600 Pave NaN Reg Lvl AllPub FR2 ... 0 0 NaN NaN NaN 0 5 2007 WD Normal 3 60 RL 68.0 11250 Pave NaN IR1 Lvl AllPub Inside ... 0 0 NaN NaN NaN 0 9 2008 WD Normal 4 70 RL 60.0 9550 Pave NaN IR1 Lvl AllPub Corner ... 0 0 NaN NaN NaN 0 2 2006 WD Abnorml 5 60 RL 84.0 14260 Pave NaN IR1 Lvl AllPub FR2 ... 0 0 NaN NaN NaN 0 12 2008 WD Normal 5 rows × 79 columns 4. Check NaN ratio and Remove null ratio &gt;= 0.5 데이터를 처리할 때 항상 Null값을 어떻게 처리할지 고민해야 한다. 추후 모델에 입력되는 input값에는 절대 어떠한 Null 값이 있어서는 안되며 있더라도 에러가 발생하기 때문에 미리 꼭 처리해주어야 한다. 우선 각 칼럼별로 Null값 비율이 50%이상인 칼럼을 찾아 해당 칼럼을 제거해주었다. 보통 null값 처리를 위해 평균, 최대값, 최소값 등으로 대체하곤 하는데 위와 같이 대부분의 칼럼이 Null인 데이터는 차라리 없애주는 것이 좋다. 12345# check null check_null = boston_df.isna().sum() / len(boston_df)# columns of null ratio &gt;= 0.5check_null[check_null &gt;= 0.5] Alley 0.932169 PoolQC 0.996574 Fence 0.804385 MiscFeature 0.964029 dtype: float64 12345# remove columns of null ratio &gt;= 0.5remove_cols = check_null[check_null &gt;= 0.5].keys()boston_df = boston_df.drop(remove_cols, axis=1)boston_df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } MSSubClass MSZoning LotFrontage LotArea Street LotShape LandContour Utilities LotConfig LandSlope ... OpenPorchSF EnclosedPorch 3SsnPorch ScreenPorch PoolArea MiscVal MoSold YrSold SaleType SaleCondition Id 1 60 RL 65.0 8450 Pave Reg Lvl AllPub Inside Gtl ... 61 0 0 0 0 0 2 2008 WD Normal 2 20 RL 80.0 9600 Pave Reg Lvl AllPub FR2 Gtl ... 0 0 0 0 0 0 5 2007 WD Normal 3 60 RL 68.0 11250 Pave IR1 Lvl AllPub Inside Gtl ... 42 0 0 0 0 0 9 2008 WD Normal 4 70 RL 60.0 9550 Pave IR1 Lvl AllPub Corner Gtl ... 35 272 0 0 0 0 2 2006 WD Abnorml 5 60 RL 84.0 14260 Pave IR1 Lvl AllPub FR2 Gtl ... 84 0 0 0 0 0 12 2008 WD Normal 5 rows × 75 columns 5. Check Object &amp; Numeric variables 해당 데이터 셋에는 수치형 데이터만 있는 것이 아니다. [성별: 남자, 여자], [학급: 햇님반, 꽃님반, 달님반]과 같은 카테고리형 데이터도 존재한다. 이러한 카테고리형 데이터는 각 칼럼을 0과 1로 변환해주는 one-hot encoding을 적용해주어 수치값과 가중치를 달리해주어야 한다. 수치형 데이터와 카테고리형 데이터를 구분하기 위해 select_dtypes()를 이용하였다. parameter값으로 include와 exclude를 적용할 수 있는데 이를 통해 데이터를 분리한다. 123# split object &amp; numericboston_obj_df = boston_df.select_dtypes(include='object') # 카테고리형boston_num_df = boston_df.select_dtypes(exclude='object') # 수치형 123print('Object type columns:\\n',boston_obj_df.columns)print('---------------------------------------------------------------------------------')print('Numeric type columns:\\n',boston_num_df.columns) Object type columns: Index([&#39;MSZoning&#39;, &#39;Street&#39;, &#39;LotShape&#39;, &#39;LandContour&#39;, &#39;Utilities&#39;, &#39;LotConfig&#39;, &#39;LandSlope&#39;, &#39;Neighborhood&#39;, &#39;Condition1&#39;, &#39;Condition2&#39;, &#39;BldgType&#39;, &#39;HouseStyle&#39;, &#39;RoofStyle&#39;, &#39;RoofMatl&#39;, &#39;Exterior1st&#39;, &#39;Exterior2nd&#39;, &#39;MasVnrType&#39;, &#39;ExterQual&#39;, &#39;ExterCond&#39;, &#39;Foundation&#39;, &#39;BsmtQual&#39;, &#39;BsmtCond&#39;, &#39;BsmtExposure&#39;, &#39;BsmtFinType1&#39;, &#39;BsmtFinType2&#39;, &#39;Heating&#39;, &#39;HeatingQC&#39;, &#39;CentralAir&#39;, &#39;Electrical&#39;, &#39;KitchenQual&#39;, &#39;Functional&#39;, &#39;FireplaceQu&#39;, &#39;GarageType&#39;, &#39;GarageFinish&#39;, &#39;GarageQual&#39;, &#39;GarageCond&#39;, &#39;PavedDrive&#39;, &#39;SaleType&#39;, &#39;SaleCondition&#39;], dtype=&#39;object&#39;) --------------------------------------------------------------------------------- Numeric type columns: Index([&#39;MSSubClass&#39;, &#39;LotFrontage&#39;, &#39;LotArea&#39;, &#39;OverallQual&#39;, &#39;OverallCond&#39;, &#39;YearBuilt&#39;, &#39;YearRemodAdd&#39;, &#39;MasVnrArea&#39;, &#39;BsmtFinSF1&#39;, &#39;BsmtFinSF2&#39;, &#39;BsmtUnfSF&#39;, &#39;TotalBsmtSF&#39;, &#39;1stFlrSF&#39;, &#39;2ndFlrSF&#39;, &#39;LowQualFinSF&#39;, &#39;GrLivArea&#39;, &#39;BsmtFullBath&#39;, &#39;BsmtHalfBath&#39;, &#39;FullBath&#39;, &#39;HalfBath&#39;, &#39;BedroomAbvGr&#39;, &#39;KitchenAbvGr&#39;, &#39;TotRmsAbvGrd&#39;, &#39;Fireplaces&#39;, &#39;GarageYrBlt&#39;, &#39;GarageCars&#39;, &#39;GarageArea&#39;, &#39;WoodDeckSF&#39;, &#39;OpenPorchSF&#39;, &#39;EnclosedPorch&#39;, &#39;3SsnPorch&#39;, &#39;ScreenPorch&#39;, &#39;PoolArea&#39;, &#39;MiscVal&#39;, &#39;MoSold&#39;, &#39;YrSold&#39;], dtype=&#39;object&#39;) 6. Change object type to dummy variables 위에서 분리한 카테고리형 데이터에 one-hot encoding을 적용하기 위해 pandas의 pd.get_dummies()를 적용하였다. one-hot encoding 적용시 [남자, 여자]의 경우 [[1,0], [0,1]]과 같은 형태로 변환된다. 123boston_dummy_df = pd.get_dummies(boston_obj_df, drop_first=True)boston_dummy_df.index = boston_df_indexboston_dummy_df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } MSZoning_FV MSZoning_RH MSZoning_RL MSZoning_RM Street_Pave LotShape_IR2 LotShape_IR3 LotShape_Reg LandContour_HLS LandContour_Low ... SaleType_ConLI SaleType_ConLw SaleType_New SaleType_Oth SaleType_WD SaleCondition_AdjLand SaleCondition_Alloca SaleCondition_Family SaleCondition_Normal SaleCondition_Partial Id 1 0 0 1 0 1 0 0 1 0 0 ... 0 0 0 0 1 0 0 0 1 0 2 0 0 1 0 1 0 0 1 0 0 ... 0 0 0 0 1 0 0 0 1 0 3 0 0 1 0 1 0 0 0 0 0 ... 0 0 0 0 1 0 0 0 1 0 4 0 0 1 0 1 0 0 0 0 0 ... 0 0 0 0 1 0 0 0 0 0 5 0 0 1 0 1 0 0 0 0 0 ... 0 0 0 0 1 0 0 0 1 0 5 rows × 200 columns 7. Impute NaN of numeric data to ‘mean’ 4번쨰 과정에서 null값이 50%이상인 변수들을 제거해주었었는데, 그 이하로 null값이 있는 데이터를 마저 처리해주어야 한다. 여기서는 각 칼럼의 null값을 해당하는 각 변수들의 평균(mean)으로 대체(imputation)해주었다. 평균값 대체를 위하여 scikit-learn의 Imputer 함수를 이용하였으며, strategy 값에 대체해주고자 하는 이름을 넣어주면 해당 값으로 처리한다. 12345from sklearn.preprocessing import Imputerimputer = Imputer(strategy='mean')imputer.fit(boston_num_df)boston_num_df_ = imputer.transform(boston_num_df) 12boston_num_df = pd.DataFrame(boston_num_df_, columns=boston_num_df.columns, index=boston_df_index)boston_num_df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } MSSubClass LotFrontage LotArea OverallQual OverallCond YearBuilt YearRemodAdd MasVnrArea BsmtFinSF1 BsmtFinSF2 ... GarageArea WoodDeckSF OpenPorchSF EnclosedPorch 3SsnPorch ScreenPorch PoolArea MiscVal MoSold YrSold Id 1 60.0 65.0 8450.0 7.0 5.0 2003.0 2003.0 196.0 706.0 0.0 ... 548.0 0.0 61.0 0.0 0.0 0.0 0.0 0.0 2.0 2008.0 2 20.0 80.0 9600.0 6.0 8.0 1976.0 1976.0 0.0 978.0 0.0 ... 460.0 298.0 0.0 0.0 0.0 0.0 0.0 0.0 5.0 2007.0 3 60.0 68.0 11250.0 7.0 5.0 2001.0 2002.0 162.0 486.0 0.0 ... 608.0 0.0 42.0 0.0 0.0 0.0 0.0 0.0 9.0 2008.0 4 70.0 60.0 9550.0 7.0 5.0 1915.0 1970.0 0.0 216.0 0.0 ... 642.0 0.0 35.0 272.0 0.0 0.0 0.0 0.0 2.0 2006.0 5 60.0 84.0 14260.0 8.0 5.0 2000.0 2000.0 350.0 655.0 0.0 ... 836.0 192.0 84.0 0.0 0.0 0.0 0.0 0.0 12.0 2008.0 5 rows × 36 columns 8. Merge numeric_df &amp; dummies_df 위에서 각각 처리한 카테고리형 데이터와 수치형 데이터를 이제 최종적으로 다시 하나로 merge해준다. merge시 index 순서가 꼬이지 않게 left_index=True, right_index=True를 지정하여 merge를 수행한다. 12boston_df = pd.merge(boston_dummy_df, boston_num_df, left_index=True, right_index=True)boston_df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } MSZoning_FV MSZoning_RH MSZoning_RL MSZoning_RM Street_Pave LotShape_IR2 LotShape_IR3 LotShape_Reg LandContour_HLS LandContour_Low ... GarageArea WoodDeckSF OpenPorchSF EnclosedPorch 3SsnPorch ScreenPorch PoolArea MiscVal MoSold YrSold Id 1 0 0 1 0 1 0 0 1 0 0 ... 548.0 0.0 61.0 0.0 0.0 0.0 0.0 0.0 2.0 2008.0 2 0 0 1 0 1 0 0 1 0 0 ... 460.0 298.0 0.0 0.0 0.0 0.0 0.0 0.0 5.0 2007.0 3 0 0 1 0 1 0 0 0 0 0 ... 608.0 0.0 42.0 0.0 0.0 0.0 0.0 0.0 9.0 2008.0 4 0 0 1 0 1 0 0 0 0 0 ... 642.0 0.0 35.0 272.0 0.0 0.0 0.0 0.0 2.0 2006.0 5 0 0 1 0 1 0 0 0 0 0 ... 836.0 192.0 84.0 0.0 0.0 0.0 0.0 0.0 12.0 2008.0 5 rows × 236 columns 9. Split train &amp; validation &amp; test set 모델 학습 및 검증을 위해 데이터를 split한다. 여기서 test set의 경우 정답값이 없는 예측해야 하는 값이므로 검증을 위해 validation set을 train set의 20%만큼을 지정해주었다. 최종적으로 1,168개의 데이터로 학습 및 292개의 데이터로 검증 후 1,459개의 test셋을 예측한다. 1234567train_df = boston_df[:len_train_df]test_df = boston_df[len_train_df:]train_df['SalePrice'] = train_y_labelprint('train set length: ',len(train_df))print('test set length: ',len(test_df)) train set length: 1460 test set length: 1459 123456789from sklearn.model_selection import train_test_splitX_train = train_df.drop(['SalePrice'], axis=1)y_train = train_df['SalePrice']X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, shuffle=True)X_test = test_dftest_id_idx = test_df.index 123print('X_train : ',len(X_train))print('X_val : ',len(X_val))print('X_test :',len(X_test)) X_train : 1168 X_val : 292 X_test : 1459 10. Training by XGBRegression Model 모델 학습을 위해 최근 kaggle에서 가장 인기 있는 모델인 XGBoost 모델을 이용하였다. 해당 예측은 regression 예측이므로 XGBRegressor() 모델을 이용하였다. 최적의 모델 파라미터 설정을 위하여 GridSearch를 이용하였으며, 5번의 cross-validation으로 검증을 진행하였다. 학습 후 bestparams를 출력하면 최적의 파라미터 값이 출력된다. 1234567891011121314151617from sklearn.model_selection import GridSearchCVimport xgboost as xgbparam = &#123; 'max_depth':[2,3,4], 'n_estimators':range(550,700,50), 'colsample_bytree':[0.5,0.7,1], 'colsample_bylevel':[0.5,0.7,1],&#125;model = xgb.XGBRegressor()grid_search = GridSearchCV(estimator=model, param_grid=param, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)grid_search.fit(X_train, y_train)print(grid_search.best_params_)print(grid_search.best_estimator_) {&#39;colsample_bylevel&#39;: 0.5, &#39;colsample_bytree&#39;: 0.7, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 600} XGBRegressor(base_score=0.5, booster=&#39;gbtree&#39;, colsample_bylevel=0.5, colsample_bytree=0.7, gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3, min_child_weight=1, missing=None, n_estimators=600, n_jobs=1, nthread=None, objective=&#39;reg:linear&#39;, random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None, silent=True, subsample=1) 11. Prediction &amp; Score 검증을 위해 Mean Absolute Error(MAE) 지표를 활용하였다. MSE를 활용할 경우 error값이 클 경우 그에 제곱된 값이 출력되기 때문에 값이 너무 커져 보기 불편하다는 단점이 있다. 검증 결과 validation mae 값이 14,000정도인데, 워낙 집 가격에 대한 값의 범위가 넓기 때문에 이 정도 error값은 심각한 정도는 아니며 납득할만한 수준이라고 할 수 있다. 123456from sklearn.metrics import mean_squared_error, mean_absolute_errorpred_train = grid_search.predict(X_train)pred_val = grid_search.predict(X_val)print('train mae score: ', mean_absolute_error(y_train, pred_train))print('val mae score:', mean_absolute_error(y_val, pred_val)) train mae score: 4790.379391186858 val mae score: 14178.155835295376 이후 validation set을 대상으로 예측을 수행한 후 실제 값과의 결과를 plotting하였다. 어느 정도 경향을 잘 예측하고 있는 것을 확인할 수 있다. 123456plt.figure(figsize=(17,7))plt.plot(range(0, len(y_val)), y_val,'o-', label='Validation Actual')plt.plot(range(0, len(pred_val)), pred_val, '-', label='Validation Predict')plt.title('Prediction of House Prices')plt.ylabel('Prices')plt.legend() 12. Predict test set &amp; Submit submission.csv1test_y_pred = grid_search.predict(X_test) 123id_pred_df = pd.DataFrame()id_pred_df['Id'] = test_id_idxid_pred_df['SalePrice'] = test_y_pred 1id_pred_df.to_csv('submission.csv', index=False) Outro실제 submission에서 성능 평가 시에는 Root Mean Squarred Error이용하는데, 제출 결과 0.12875로 총 4,465팀 중 1,867등을 기록하였다(상위 42%). 이번에는 단순히 테스트를 위해 머신러닝 모델만 적용해보았는데, LSTM 모델을 적용하면 확실히 더 낮은 error 값이 나올 것으로 기대된다.","categories":[],"tags":[{"name":"machinelearning","slug":"machinelearning","permalink":"https://jaehyeongan.github.io/tags/machinelearning/"},{"name":"kaggle","slug":"kaggle","permalink":"https://jaehyeongan.github.io/tags/kaggle/"},{"name":"regression","slug":"regression","permalink":"https://jaehyeongan.github.io/tags/regression/"},{"name":"bostonhouse","slug":"bostonhouse","permalink":"https://jaehyeongan.github.io/tags/bostonhouse/"},{"name":"price","slug":"price","permalink":"https://jaehyeongan.github.io/tags/price/"},{"name":"xgboost","slug":"xgboost","permalink":"https://jaehyeongan.github.io/tags/xgboost/"},{"name":"featureengineering","slug":"featureengineering","permalink":"https://jaehyeongan.github.io/tags/featureengineering/"}]},{"title":"[Kaggle challenge] 직소 악성 대화 분류(Jigsaw Unintended Bias in Toxicity Classification)","slug":"Kaggle-challenge-직소-악성-댓글-분류-Jigsaw-Unintended-Bias-in-Toxicity","date":"2019-07-03T15:11:36.000Z","updated":"2020-10-28T15:14:34.474Z","comments":true,"path":"2019/07/04/Kaggle-challenge-직소-악성-댓글-분류-Jigsaw-Unintended-Bias-in-Toxicity/","link":"","permalink":"https://jaehyeongan.github.io/2019/07/04/Kaggle-challenge-직소-악성-댓글-분류-Jigsaw-Unintended-Bias-in-Toxicity/","excerpt":"","text":"Intro얼마 전 캐글에서 구글 Jigsaw/Conversation AI팀에 의해 ‘Jigsaw Unintended Bias in Toxicity Classification’라는 주제로 competition이 개최되어 호기심에 도전해보았다. Jigsaw라는 곳을 처음 들어봤는데 알아보니 구글의 자회사로 온라인 상의 욕설이나 선동적, 폭력적 대화를 잡아내는 기술을 연구하는 곳이었고,Description상에 의한 이 Competition의 주요문제는 다음과 같았다. 현재 Jigsaw의 Conversation AI팀은 Perspective라는 제품을 통해 온라인 상의 악성 대화(위협, 외설, 모욕 등)를 잡아내고 있는데, 모델을 좀 더 정교하게 하여 낮은 에러율의 다양한 악성 대화를 잡아내는 모델을 만드는 것. 데이터의 경우 train데이터와 test데이터를 따로 제공하며, train 데이터의 경우 180만건 정도 되는데 텍스트 데이터 위주로 되어있다보니 사이즈가 상당히 컸다.해당 competition의 결과 제출은 Kernels에 의해서만 가능한데, 데이터 사이즈가 크다보니 모델에 의한 학습도 굉장히 오래걸리고 kaggle내에서도 kernel 학습시간에 제한을 두기 때문에 모델을 정교하게 학습시키는 것이 쉽지 않았다. 코드 작성은 Jupyter notebook을 이용하였으며, 아래 작성된 코드는 ipynb파일을 markdown으로 변환하여 업로드하였다. Import Library12345678910111213141516171819import os import numpy as npimport pandas as pdimport matplotlib.pyplot as pltfrom collections import Counterimport nltknltk.download('stopwords')nltk.download('punkt')from nltk.corpus import stopwords from nltk.tokenize import word_tokenize stop_words = set(stopwords.words('english')) import tensorflow as tffrom sklearn.metrics import roc_auc_scorefrom keras import models, layers, Modelfrom keras.preprocessing import text, sequencefrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateauimport osprint(os.listdir(\"./input\")) [&#39;test.csv&#39;, &#39;train.csv&#39;] 1. Load Data 데이터는 train 데이터가 180만건, test 데이터가 9만7천건 정도로 이루어져 있다. train 데이터는 id, target, comment_text를 포함하여 총 45개의 칼럼으로 이루어져 있지만, test 데이터의 경우 id, target, comment_text 총 3개의 칼럼으로만 이루어져 있다. 12345## load datatrain_data = pd.read_csv('./input/train.csv')test_data = pd.read_csv('./input/test.csv')print(train_data.shape)print(test_data.shape) (1804874, 45) (97320, 2) 1train_data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id target comment_text severe_toxicity obscene identity_attack insult threat asian atheist ... article_id rating funny wow sad likes disagree sexual_explicit identity_annotator_count toxicity_annotator_count 0 59848 0.000000 This is so cool. It's like, 'would you want yo... 0.000000 0.0 0.000000 0.00000 0.0 NaN NaN ... 2006 rejected 0 0 0 0 0 0.0 0 4 1 59849 0.000000 Thank you!! This would make my life a lot less... 0.000000 0.0 0.000000 0.00000 0.0 NaN NaN ... 2006 rejected 0 0 0 0 0 0.0 0 4 2 59852 0.000000 This is such an urgent design problem; kudos t... 0.000000 0.0 0.000000 0.00000 0.0 NaN NaN ... 2006 rejected 0 0 0 0 0 0.0 0 4 3 59855 0.000000 Is this something I'll be able to install on m... 0.000000 0.0 0.000000 0.00000 0.0 NaN NaN ... 2006 rejected 0 0 0 0 0 0.0 0 4 4 59856 0.893617 haha you guys are a bunch of losers. 0.021277 0.0 0.021277 0.87234 0.0 0.0 0.0 ... 2006 rejected 0 0 0 1 0 0.0 4 47 5 rows × 45 columns 2. Set index &amp; target label 다른 커널을 보니 train 데이터의 다양한 칼럼을 활용하는 것 같던데 여기선 텍스트 데이터와 타겟 값만을 이용하여 학습 및 예측을 수행하였다. id 값은 index로 지정해두었으며, target값의 경우 Data Description의 설명에 따라 0.5이상은 positive 0.5미만은 negative 라벨로 분류하였다. 12345678910train_df = train_data[['id','comment_text','target']]test_df = test_data.copy()# set indextrain_df.set_index('id', inplace=True)test_df.set_index('id', inplace=True)# y_labeltrain_y_label = np.where(train_df['target'] &gt;= 0.5, 1, 0) # Label 1 &gt;= 0.5 / Label 0 &lt; 0.5train_df.drop(['target'], axis=1, inplace=True) 12# ratio by ClassCounter(train_y_label) Counter({0: 1660540, 1: 144334}) 3. View text data comment_text 칼럼을 출력해보면 아래와 같이 다양한 주제의 대화 내용을 확인할 수 있다. 1train_data['comment_text'].head(20) 0 This is so cool. It&#39;s like, &#39;would you want yo... 1 Thank you!! This would make my life a lot less... 2 This is such an urgent design problem; kudos t... 3 Is this something I&#39;ll be able to install on m... 4 haha you guys are a bunch of losers. 5 ur a sh*tty comment. 6 hahahahahahahahhha suck it. 7 FFFFUUUUUUUUUUUUUUU 8 The ranchers seem motivated by mostly by greed... 9 It was a great show. Not a combo I&#39;d of expect... 10 Wow, that sounds great. 11 This is a great story. Man. I wonder if the pe... 12 This seems like a step in the right direction. 13 It&#39;s ridiculous that these guys are being call... 14 This story gets more ridiculous by the hour! A... 15 I agree; I don&#39;t want to grant them the legiti... 16 Interesting. I&#39;ll be curious to see how this w... 17 Awesome! I love Civil Comments! 18 I&#39;m glad you&#39;re working on this, and I look fo... 19 Angry trolls, misogynists and Racists&quot;, oh my.... Name: comment_text, dtype: object 4. Remove Punctuation &amp; Stopword 가장 기본적인 텍스트 전처리를 위하여 간단히 텍스트 내의 punctuation과 stopwords를 제거하는 함수를 정의하였다. 워낙 데이터가 커서 함수 호출 시 처리 속도가 오래 걸린다. 그래서 속도를 위해 list comprehension과 lambda로 처리하였는데 그래도 처리까지 시간이 꽤 걸렸다. 123456789101112131415161718## Clean Punctuation &amp; Stopwordsclass clean_text: def __init__(self, text): self.text = text # Remove Punctuation def rm_punct(text): punct = set([p for p in \"/-'?!.,#$%\\'()*+-/:;&lt;=&gt;@[\\\\]^_`&#123;|&#125;~`\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&amp;']) text = [t for t in text if t not in punct] return \"\".join(text) # Remove Stopwords def rm_stopwords(text): word_tokens = word_tokenize(text) result = [w for w in word_tokens if w not in stop_words] return \" \".join(result) 1234567# remove punctuation train_df['comment_text'] = train_df['comment_text'].apply(lambda x: clean_text.rm_punct(x))test_df['comment_text'] = test_df['comment_text'].apply(lambda x: clean_text.rm_punct(x))# remove stopwordsX_train = train_df['comment_text'].apply(lambda x: clean_text.rm_stopwords(x))X_test = test_df['comment_text'].apply(lambda x: clean_text.rm_stopwords(x)) 5. Tokenize 전처리된 데이터를 keras.Tokenizer를 이용하여 sequences 데이터로 변환한다. Tokenizer의 처리 순서는 아래와 같다.— Tokenizer 객체를 통해 데이터를 토큰화시킨 후 각 토큰에 고유 index를 부여하여 word index 생성— texts_to_sequences()를 통해 word index를 기반으로 시퀀스 데이터 생성— pad_sequences()를 통해 padding 추가 12345678910## tokenizemax_words = 100000tokenizer = text.Tokenizer(num_words=max_words) # Tokenizer 객체생성tokenizer.fit_on_texts(X_train) # 토큰 별 word index 생성# texts_to_sequencessequences_text_train = tokenizer.texts_to_sequences(X_train)sequences_text_test = tokenizer.texts_to_sequences(X_test)print(sequences_text_train[:5]) [[21, 2188, 39, 6, 3, 32, 1115, 116, 48, 91, 277, 26, 138], [323, 21, 3, 25, 107, 142, 144, 105, 7, 159, 125, 9, 28], [21, 9494, 2834, 94, 4342, 340, 1102, 4913], [241, 90, 384, 316, 5764, 1027, 164, 6388], [5230, 586, 998, 2593]] texts_to_sequences()함수를 이용하면 토큰화 된 문자들이 위와 같이 고유 index 번호로 바뀐 채 sequnce 형태로 출력된다. 123456# add paddingmax_len = max(len(l) for l in sequences_text_train)pad_train = sequence.pad_sequences(sequences_text_train, maxlen=max_len)pad_test = sequence.pad_sequences(sequences_text_test, maxlen=max_len)print(pad_train[:5]) array([[ 0, 0, 0, ..., 277, 26, 138], [ 0, 0, 0, ..., 125, 9, 28], [ 0, 0, 0, ..., 340, 1102, 4913], [ 0, 0, 0, ..., 1027, 164, 6388], [ 0, 0, 0, ..., 586, 998, 2593]]) max_len 값은 방금 위에서 sequence로 변환한 데이터 중 가장 많은 word 수를 가지는 데이터의 길이를 받은 것이고,모든 데이터를 그 길이 만큼 맞춰주기 위하여 pad_seqences()함수를 통해 0값을 채워주게 된다. 6. Embedding + LSTM model 예측을 위해서 embedding 레이어와 lstm 레이어를 연결하여 딥러닝 모델을 구축하였다. Embedding 레이어는 텍스트 데이터의 단어 사이의 의미관계를 학습하는데 효과적이므로 텍스트 데이터 학습시 많이 사용되며, LSTM 모델은 양방향 LSTM(Bidirectional LSTM)으로 구축하여 시간적 의미와 상관없이 단어들 사이의 양방향적으로 의미 순서를 학습하도록 하였다. 123456789101112131415161718def Embedding_CuDNNLSTM_model(max_words, max_len): sequence_input = layers.Input(shape=(None, )) x = layers.Embedding(max_words, 128, input_length=max_len)(sequence_input) x = layers.SpatialDropout1D(0.3)(x) x = layers.Bidirectional(layers.CuDNNLSTM(64, return_sequences=True))(x) x = layers.Bidirectional(layers.CuDNNLSTM(64, return_sequences=True))(x) avg_pool1d = layers.GlobalAveragePooling1D()(x) max_pool1d = layers.GlobalMaxPool1D()(x) x = layers.concatenate([avg_pool1d, max_pool1d]) x = layers.Dense(32, activation='relu')(x) x = layers.BatchNormalization()(x) output = layers.Dense(1, activation='sigmoid')(x) model = models.Model(sequence_input, output) return model 1234567## embedding_lstm models model = Embedding_CuDNNLSTM_model(max_words, max_len)# model compilemodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc', auroc])model.summary() __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_1 (InputLayer) (None, None) 0 __________________________________________________________________________________________________ embedding_1 (Embedding) (None, 306, 128) 12800000 input_1[0][0] __________________________________________________________________________________________________ spatial_dropout1d_1 (SpatialDro (None, 306, 128) 0 embedding_1[0][0] __________________________________________________________________________________________________ bidirectional_1 (Bidirectional) (None, 306, 128) 99328 spatial_dropout1d_1[0][0] __________________________________________________________________________________________________ bidirectional_2 (Bidirectional) (None, 306, 128) 99328 bidirectional_1[0][0] __________________________________________________________________________________________________ global_average_pooling1d_1 (Glo (None, 128) 0 bidirectional_2[0][0] __________________________________________________________________________________________________ global_max_pooling1d_1 (GlobalM (None, 128) 0 bidirectional_2[0][0] __________________________________________________________________________________________________ concatenate_1 (Concatenate) (None, 256) 0 global_average_pooling1d_1[0][0] global_max_pooling1d_1[0][0] __________________________________________________________________________________________________ dense_1 (Dense) (None, 32) 8224 concatenate_1[0][0] __________________________________________________________________________________________________ batch_normalization_1 (BatchNor (None, 32) 128 dense_1[0][0] __________________________________________________________________________________________________ dense_2 (Dense) (None, 1) 33 batch_normalization_1[0][0] ================================================================================================== Total params: 13,007,041 Trainable params: 13,006,977 Non-trainable params: 64 __________________________________________________________________________________________________ Train model callback함수는 아래와 같이 사용— ReduceLROnPlateau() : 초기에 학습률을 높게 지정한 후 일정 epoch동안 성능이 향상되지 않을 시 점차 learning rate를 줄여나감— EarlyStopping() : 일정 epoch동안 성능 향상이 없을 시 학습을 조기 종료함.— ModelCheckPoint() : epoch마다 학습 된 모델을 저장, save_best_only=True를 지정하여 성능이 가장 좋은 모델만 지정할 수 있음 12def auroc(y_true, y_pred): return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double) 해당 competition의 평가 모델의 경우 ROC-AUC를 사용하기 때문에 해당 평가지표로 검증하기 위해 acroc라는 함수를 정의. 12345678910111213141516171819# keras.callbackscallbacks_list = [ ReduceLROnPlateau( monitor='val_auroc', patience=2, factor=0.1, mode='max'), # val_loss가 patience동안 향상되지 않으면 학습률을 0.1만큼 감소 (new_lr = lr * factor) EarlyStopping( patience=5, monitor='val_auroc', mode='max', restore_best_weights=True), ModelCheckpoint( filepath='./input/best_embedding_lstm_model.h5', monitor='val_auroc', mode='max', save_best_only=True)]# model fit &amp; savemodel_path = './input/best_embedding_lstm_model.h5'if os.path.exists(model_path): model.load_weights(model_path)else: history = model.fit(pad_train, train_y_label, epochs=7, batch_size=1024, callbacks=callbacks_list, validation_split=0.3, verbose=1) Train on 1263411 samples, validate on 541463 samples Epoch 1/7 1263411/1263411 [==============================] - 579s 458us/step - loss: 0.1831 - acc: 0.9398 - auroc: 0.9263 - val_loss: 0.2086 - val_acc: 0.9169 - val_auroc: 0.9479 Epoch 2/7 1263411/1263411 [==============================] - 577s 457us/step - loss: 0.1187 - acc: 0.9540 - auroc: 0.9600 - val_loss: 0.1792 - val_acc: 0.9356 - val_auroc: 0.9479 Epoch 3/7 1263411/1263411 [==============================] - 577s 456us/step - loss: 0.1017 - acc: 0.9606 - auroc: 0.9717 - val_loss: 0.2070 - val_acc: 0.9359 - val_auroc: 0.9424 Epoch 4/7 1263411/1263411 [==============================] - 576s 456us/step - loss: 0.0707 - acc: 0.9739 - auroc: 0.9866 - val_loss: 0.1806 - val_acc: 0.9386 - val_auroc: 0.9227 Epoch 5/7 1263411/1263411 [==============================] - 576s 456us/step - loss: 0.0639 - acc: 0.9762 - auroc: 0.9890 - val_loss: 0.1942 - val_acc: 0.9345 - val_auroc: 0.9218 Epoch 6/7 1263411/1263411 [==============================] - 577s 457us/step - loss: 0.0584 - acc: 0.9785 - auroc: 0.9908 - val_loss: 0.1988 - val_acc: 0.9374 - val_auroc: 0.9190 12345678# plot score by epochsauroc = history.history['auroc']val_auroc = history.history['val_auroc']epochs = range(1, len(auroc)+1)plt.figure(figsize=(7,3))plt.plot(epochs, auroc, 'b', label='auroc')plt.plot(epochs, val_auroc, 'r', label='validation auroc') [&lt;matplotlib.lines.Line2D at 0x1176f6fdba8&gt;] 결과를 보니검증 성능이 epoch이 증가할 수록 떨어지는 것으로 보아 모델이 과대적합 된 듯 함. dropout 비율을 더 높이거나, 레이어 수를 줄여야 할 것 같음. Predict test set12## predict test_settest_pred = model.predict(pad_test) 7. submit submission.csv123456sample_result = pd.DataFrame()sample_result['id'] = test_df.indexsample_result['prediction'] = test_pred## submit sample_submission.csvsample_result.to_csv('submission.csv', index=False) Outro최종 제출 결과 91.1% 라는 검증 결과가 나와 상위 84%…. 문제를 제대로 이해를 안하고 시작해서 그런지 모델 수정으로는 이 이상 성능 향상이 되지 않았다. 다른 상위 커널을 살펴보니 대부분 feature engineering부분에서 텍스트 처리에 많은 노력을 기울인 것 같다.더 수정해서 해보려고 했는데, 제출 기간이 아쉽게 종료가 되어 더 진행해보지는 않았다. 최근 정권우님이 쓰신 ‘머신러닝 탐구생활’이라는 책을 구매하였는데, 다양한 kaggle문제를 어떻게 접근해야 하는지, 또 최근 kaggle내에서 어떤 모델이 주로 사용되는지 트렌드를 살펴볼 수 있을 것 같아 열심히 읽어보는 중이다. 완독 후 다시 다른 캐글 문제에 도전해봐야겠다!","categories":[],"tags":[{"name":"lstm","slug":"lstm","permalink":"https://jaehyeongan.github.io/tags/lstm/"},{"name":"deeplearning","slug":"deeplearning","permalink":"https://jaehyeongan.github.io/tags/deeplearning/"},{"name":"embedding","slug":"embedding","permalink":"https://jaehyeongan.github.io/tags/embedding/"},{"name":"kaggle","slug":"kaggle","permalink":"https://jaehyeongan.github.io/tags/kaggle/"},{"name":"competition","slug":"competition","permalink":"https://jaehyeongan.github.io/tags/competition/"},{"name":"google","slug":"google","permalink":"https://jaehyeongan.github.io/tags/google/"},{"name":"jigsaw","slug":"jigsaw","permalink":"https://jaehyeongan.github.io/tags/jigsaw/"},{"name":"toxicity","slug":"toxicity","permalink":"https://jaehyeongan.github.io/tags/toxicity/"},{"name":"classification","slug":"classification","permalink":"https://jaehyeongan.github.io/tags/classification/"},{"name":"bidirectionallstm","slug":"bidirectionallstm","permalink":"https://jaehyeongan.github.io/tags/bidirectionallstm/"}]},{"title":"딥러닝을 위한 수학기초 03 - 미분, 편미분","slug":"딥러닝을-위한-수학기초-3","date":"2019-06-25T14:59:47.000Z","updated":"2020-10-28T15:14:34.484Z","comments":true,"path":"2019/06/25/딥러닝을-위한-수학기초-3/","link":"","permalink":"https://jaehyeongan.github.io/2019/06/25/딥러닝을-위한-수학기초-3/","excerpt":"","text":"Intro본 글은 ‘처음 배우는 딥러닝 수학(한빛미디어)’이라는 책의 ‘chap.02 신경망을 위한 수학기초’ 를 정리한 글입니다. 1. 미분 도함수: 어떤 함수 안에 포함도니 값 각각이 0에 한없이 가까워지는 극한값(미분계수)을 구하는 함수 y = f(x)의 도함수 f’(x)는 아래와 같이 정의 f(x) = 3x일 때, 도함수 계산 과정 f(x) = x^2일떄, 도함수 계산 과정 함수 f(x)의 도함수f’(x)를 구하는 것을 “함수 f(x)를 미분한다”라고 하며, 위와 같이 값을 계산할 수 있다면 미분 가능이라고 함 미분기호 y = f(x)의 도함수 f’(x)를 극한 개념이 아닌 분수로 표현하는 방법 f’(x) = dy/dx 미분 성질 미분의 선형성 함수 합의 미분은 각 함수를 미분한 합과 같다.상수를 곱한 함수의 미분은 미분한 함수에 상수를 곱한 것과 같다. 시그모이드 함수의 미분 위의 식을 이용하면 미분하지 않아도 시그모이드 함수의 도함수값을 sigma(x)의 값에서 얻을 수 있음. 최솟값의 필요조건 함수 f(x)가 x = 0일 때 최솟값이라면 f’(a)=0 f’(a)=0는 함수 f(x)가 x = a에서 최솟값이 되기 위한 “필요”조건. 이는 접선의 기울기가 0이더라도 꼭 최솟값이라는 보장이 없다는 의미 경사하강법은 접선의 기울기가 낮은 쪽으로 계속 이동시켜서 최솟값을 구함 함수 전체의 최솟값과 값이 커지거나 작아질 때 발생하는 극솟값/극댓값을 혼동할 수 있으니, 경사하강법으로 최솟값을 구할 때 주의해야 함 2. 편미분다변수 함수 독립변수가 2개 이상인 함수 편미분 다변수 함수의 경우 변수가 여러 개 있으므로 어떤 변수를 미분할지 명시해야 하는데, 이렇게 특정 변수를 명시해 미분하는 것을 편미분이라고 함 z = f(x, y)일 때, 변수 x를 미분하고 y를 상수로 취급하는 것을 ‘x에 관한 편미분’이라고 함 x에 관한 편미분 y에 관한 편미분 z = wx+b에 관한 편미분","categories":[],"tags":[{"name":"deeplearning","slug":"deeplearning","permalink":"https://jaehyeongan.github.io/tags/deeplearning/"},{"name":"neuralnetwork","slug":"neuralnetwork","permalink":"https://jaehyeongan.github.io/tags/neuralnetwork/"},{"name":"math","slug":"math","permalink":"https://jaehyeongan.github.io/tags/math/"},{"name":"derivative","slug":"derivative","permalink":"https://jaehyeongan.github.io/tags/derivative/"},{"name":"partialderivative","slug":"partialderivative","permalink":"https://jaehyeongan.github.io/tags/partialderivative/"}]},{"title":"딥러닝을 위한 수학기초 02 - 수열, 시그마, 벡터, 행렬","slug":"딥러닝을-위한-수학기초-2","date":"2019-06-20T09:38:25.000Z","updated":"2020-10-28T15:14:34.484Z","comments":true,"path":"2019/06/20/딥러닝을-위한-수학기초-2/","link":"","permalink":"https://jaehyeongan.github.io/2019/06/20/딥러닝을-위한-수학기초-2/","excerpt":"","text":"Intro본 글은 ‘처음 배우는 딥러닝 수학(한빛미디어)’이라는 책의 ‘chap.02 신경망을 위한 수학기초’ 를 정리한 글입니다. 1. 수열 수열은 ‘숫자 열’을 의미, 예를 들어 2,4,6,8,10,.. 로 진행되면 짝수열이라는 수열임 수열에서 정렬하는 숫자 하나하나를 항이라고 함. 첫번째 항은 1항, 두번째 항은 2항, n번째 항은 n항 신경망의 수열은 유한개의 항 수를 갖는 수열로서 유한수열이라고 함 수열과 점화식 점화식이란 이웃에 있는 항의 관계로 표현하는 수열의 귀납적 정의. 일반적으로 1항 A1과 인접한 2개의 항 An, An+1의 관계식으로 수열 {An}을 표현 연립 점화식 여러 수열이 몇 가지 관계식으로 연결된 것을 연립 점화식이라고 함. 신경망에서는 모든 유닛의 입력과 출력이 연립 점화식으로 연결되어 있음 2. 시그마 기호 위에서 살펴본 수열의 합을 간결하게 표현하는 것이 시그마 기호. 1항부터 n항까지의 수열의 합을 시그마 기호로 나타내면 아래와 같음 시그마 기호에 있는 문자 k는 항 숫자를 의미. 시그마 기호의 특징 - 선형성 시그마 기호는 ‘선형성’이라는 특징을 가지고 있음. 위 식을 실제 수열로 전개하면 아래와 같음. 3. 행렬행렬(matrix)은 수와 식을 사각 형태로 나열한 것으로 다음과 같이 표현 가로줄은 행, 세로줄은 열 위 그림과 같이 3행, 3열로 구성된 행렬은 3x3행렬 정사각행렬이란 행과 열 수가 같은 행렬 아래와 같이 하나의 열이나 행으로 구성된 행렬 X, Y를 차례로 열벡터, 행백터라고 함. 행렬의 상등 두 행렬 A, B는 대응하는 각 성분이 같을 때 상등이라고 하며, 기호로는 A = B로 표현 예를 들어, 행렬 A와 B가 아래와 같을 때, 이때 A = B가 되는 x, y, u, v는 2, 7, 1, 8 행렬의 합과 차, 상수 배 두 행렬 A, B의 합 A+B, 차 A-B는 같은 위치 성분끼리의 합과 차로 정의 행렬의 상수 배는 각 성분에 해당 상수를 곱한 것으로 정의 행렬의 곱셈 행렬의 곱셈은 신경망 층 사이의 신호의 합 등을 계산할 때 이용되기 때문에 특히 중요 행렬의 곱셈은 다음과 같이 정의 두 행렬 A, B의 곱 AB는 i행을 행벡터로, B의 j열을 열벡터로 생각했을 때, 행벡터와 열벡터의 내적을 i행 j열의 성분으로 하는 행렬 행렬 곱셈의 예 위 예처럼 행렬 곱셈에서는 교환법칙이 성립하지 않음(AB ≠ BA) 아다마르 곱 같은 행과 열 수를 갖는 행렬 A, B에서 같은 위치의 성분을 곱한 행렬을 ‘행렬 A, B의 아다마르 곱’이라고 함 기호 AㅇB로 표현 전치행렬 행렬 A의 i행 j열 값을 j행 i열로 바꿔 얻는 행렬을 행렬 A의 전치행렬(Transposed Matrix)라고 함 행렬 A와 B가 아래와 같을 때 전치행렬 A와 행렬B의 곱셈구하는 식","categories":[],"tags":[{"name":"deeplearning","slug":"deeplearning","permalink":"https://jaehyeongan.github.io/tags/deeplearning/"},{"name":"neuralnetwork","slug":"neuralnetwork","permalink":"https://jaehyeongan.github.io/tags/neuralnetwork/"},{"name":"sigma","slug":"sigma","permalink":"https://jaehyeongan.github.io/tags/sigma/"},{"name":"vector","slug":"vector","permalink":"https://jaehyeongan.github.io/tags/vector/"},{"name":"matrix","slug":"matrix","permalink":"https://jaehyeongan.github.io/tags/matrix/"}]},{"title":"딥러닝을 위한 수학기초 01 - 신경망의 필수 함수","slug":"딥러닝-위한-수학기초-1","date":"2019-06-19T14:21:25.000Z","updated":"2020-10-28T15:14:34.483Z","comments":true,"path":"2019/06/19/딥러닝-위한-수학기초-1/","link":"","permalink":"https://jaehyeongan.github.io/2019/06/19/딥러닝-위한-수학기초-1/","excerpt":"","text":"Intro본 글은 ‘처음 배우는 딥러닝 수학(한빛미디어)’이라는 책의 ‘chap.02 신경망을 위한 수학기초’ 를 정리한 글입니다. 1. 1차 함수 a를 기울기, b를 절편이라고 하며 두 변수 x, y가 위 식의 관계를 만족할 때 변수 y는 변수 x와 ‘1차 함수 관계’라고 함 1차함수를 그래프로 그리면 아래와 같은 직선으로 나타남 y = 2x+1의 그래프라면, 절편은 1, 기울기는 2(아래 왼쪽) y = -2x-1의 그래프라면, 절편은 -1, 기울기는 -2(아래 오른쪽) 1차 함수는 독립변수가 여러 개일 때도 있음 신경망에서 유닛이 받는 ‘가중 입력’은 1차 함수 관계로 표현. 예를 들어 아래층에서 3개의 입력 신호를 받은 유닛의 가중 입력 z는 아래와 같이 표현. 가중치 w1, w2, w3와 편향 b를 상수 파라미터라고 생각하면 가중 입력 z는 입력 x1, x2, x3과 1차함수 관계 또한 유닛이 받는 x1, x2, x3을 입력 데이터값으로 확정했다면 가중 입력 z는 가중치 w1, w2, w3 및 편향 b와 1차 함수 관계 2. 2차 함수 2차 함수 그래프에서 중요한 것은 a가 양수일 때는 아래로 볼록한 그래프고, 최솟값이 존재한다는 점. 3. 단위 계단 함수 u(-1) = 0, u(1) = 1, u(0) = 1 단위 계단 함수는 원점에서 불연속 즉, ‘미분 불가능’ 미분 불가능한 특성으로 인해 신경망의 활성화 함수로 잘 사용되지 않음. 4. 지수함수와 시그모이드 함수 위와 같은 식을 지수함수라고 함. 상수 a는 지수함수의 밑(base)라고 하며, 밑의 값으로 특히 중요한 것은 자연상수 e e = 2.718281828… 자연상수를 포함하는 지수함수를 분모로 갖는 함수가 시그모이드 함수 신경망에서 사용되는 대표적인 활성화 함수 시그모이드 함수는 아래와 같이 S자형태의 그래프로 그려짐 5. 정규분포의 확률밀도함수 신경망을 컴퓨터에서 설정할 때 가중치 및 편향의 초깃값을 설정해야 하는데 이 초기값을 구할 때 도움이 되는 것이 정규 분포임. 이 분포를 따르는 정규분포 난수를 초깃값으로 사용하면 신경망 계산 시 좋은 결과를 얻는다고 알려져 있음. 정규분포는 확률밀도함수 f(x)를 따르는 확률분포를 말함(아래 식) mu은 기댓값(평균값), sigma는 표준편차라고 하며 모두 상수임. 그래프는 종 모양","categories":[],"tags":[{"name":"deeplearning","slug":"deeplearning","permalink":"https://jaehyeongan.github.io/tags/deeplearning/"},{"name":"sigma","slug":"sigma","permalink":"https://jaehyeongan.github.io/tags/sigma/"},{"name":"vector","slug":"vector","permalink":"https://jaehyeongan.github.io/tags/vector/"},{"name":"matrix","slug":"matrix","permalink":"https://jaehyeongan.github.io/tags/matrix/"},{"name":"math","slug":"math","permalink":"https://jaehyeongan.github.io/tags/math/"},{"name":"sigmoid","slug":"sigmoid","permalink":"https://jaehyeongan.github.io/tags/sigmoid/"}]},{"title":"아나콘다(Anaconda) 가상환경 만들기","slug":"아나콘다-가상환경-만들기","date":"2019-06-13T15:29:09.000Z","updated":"2020-10-28T15:14:34.485Z","comments":true,"path":"2019/06/14/아나콘다-가상환경-만들기/","link":"","permalink":"https://jaehyeongan.github.io/2019/06/14/아나콘다-가상환경-만들기/","excerpt":"","text":"Intro지난번에 data science를 위해 아나콘다를 설치하는 법을 알아보았는데, 실제 업무에서 여러 머신러닝 프로젝트를 동시에 수행할 경우 각 프로젝트가 요구하는 환경이 다를 수가 있다. 그런데 한 가지 환경에서 모두 진행할 경우 서로 dependency 에러가 발생하거나 환경이 꼬여버릴 수 있기 때문에 각각 독립된 환경을 만들어주는 것이 좋다.이를 위해 아나콘다에서 독립된 가상환경을 어떻게 만드는지 간단히 알아보자. Anaconda 가상환경 생성anaconda prompt를 실행하여 아래와 같이 실행해보자.12345&gt; conda info --envs# conda environments:#base * C:\\Users\\nonam\\Miniconda3 위 명령어는 현재 본인의 아나콘다에 존재하는 환경 목록을 보여주는 명령어이다. base는 기본적인 아나콘다 환경을 말하며 아직 가상환경이 추가되지 않았기 때문에 base 환경만 나오게 된다. 1. 가상환경 추가1&gt; conda create -n test_envs python=3.6 ‘conda create -n’ 이라는 명령어를 통해 가상환경을 추가할 수 있으며 바로 뒤에 원하는 가상환경 이름을 적는다.이후 ‘python= x.x’ 을 통해 가상환경의 python version을 설정해줄 수 있다. 2. 가상환경 목록 확인123456&gt; conda info --envs# conda environments:#base * C:\\Users\\nonam\\Miniconda3test_envs C:\\Users\\nonam\\Miniconda3\\envs\\test_envs 이후 다시 목록을 확인하면 test_envs라는 이름의 환경이 새로 추가된 것을 확인할 수 있다. 3. 가상환경 활성화1&gt; conda activate test_envs ‘activate test_envs’와 같이 수행하면 해당 가상환경이 활성화가 되며,원래 활성화 되어있던 (base)가 (test_envs)로 바뀌는 것을 확인할 수 있다. 4. 가상환경 비활성화1&gt; conda deactivate conda deactivate를 통해 현재 활성화 된 가상환경을 비활성화 시킨다. 5. 가상환경 제거1&gt; conda env remove -n test_env Outro가끔 프로젝트를 수행하다보면 실행되는 모듈들이 python 버전에 따라 실행이 되는 것이 있고 안되는 것이 있기도 하고, 의존성 문제로 인해 잘 사용하던 다른 모듈들이 갑자기 다운그레이드 되거나 삭제되는 현상도 있으므로 이렇게 가상환경을 만들어 주어 독립된 환경에서 프로젝트를 수행하는 것이 좋을 것 같다.","categories":[],"tags":[{"name":"machinelearning","slug":"machinelearning","permalink":"https://jaehyeongan.github.io/tags/machinelearning/"},{"name":"python","slug":"python","permalink":"https://jaehyeongan.github.io/tags/python/"},{"name":"anaconda","slug":"anaconda","permalink":"https://jaehyeongan.github.io/tags/anaconda/"},{"name":"miniconda","slug":"miniconda","permalink":"https://jaehyeongan.github.io/tags/miniconda/"},{"name":"datascience","slug":"datascience","permalink":"https://jaehyeongan.github.io/tags/datascience/"},{"name":"envs","slug":"envs","permalink":"https://jaehyeongan.github.io/tags/envs/"}]},{"title":"차원축소(Dimensionality Reduction)","slug":"Dimension-Reduction","date":"2019-05-27T12:53:37.000Z","updated":"2020-10-28T15:14:34.473Z","comments":true,"path":"2019/05/27/Dimension-Reduction/","link":"","permalink":"https://jaehyeongan.github.io/2019/05/27/Dimension-Reduction/","excerpt":"","text":"Intro많은 경우 머신러닝 문제는 훈련 샘플이 수천 심지어 수백만 개의 특성을 가지고 있다. 이는 모델의 학습을 느리게 할 뿐만 아니라 정교한 모델을 만들기 어렵게 하는데 이러한 문제를 차원의 저주(curse of dimensionality)라고 한다.그리고 이러한 차원의 저주를 해결하기 위해 차원 축소 기법이 이용된다. 차원의 저주(curse of dimensionality)현재 우리는 3차원의 공간에 살고 있어 그 보다 큰 4차원, 5차원 이상의 공간을 머리속으로 떠올리기 힘들다. 즉, 차원(Dimensionality)이라는 것은 공간을 뜻하고 위 그림과 같이 1개의 점인 0차원 부터 시작하여 4차원까지 공간은 몇 개의 점과 선을 그리느냐에 따라 무수히 많은 차원을 가지게 된다. 예를 들어, 단위 면적에서 임의의 두 점을 선택하였을 경우 두 점 사이의 거리는 대략 0.52가 된다. 이를 3차원 큐브에 나타낼 경우 두 점사이의 거리는 0.66정도가 된다. 하지만 만약 1,000,000차원의 초입방체에서 두 점을 무작위로 선택할 경우는 어떨까? 평균 거리는 대략 428.25가 된다. 차원의 높아짐으로써 두 점 사이를 표현하는 거리가 고무줄처럼 늘어나버렸는데, 이렇듯 고차원의 공간은 사실상 데이터 간 거리가 먼 굉장히 희박한 상태라 할 수 있다. 이것을 데이터 관점에서 보자면 데이터의 사이즈와 크기가 바로 차원이 되며, 데이터의 변수의 크기가 차원의 크기가 되며 변수가 많으면 많을수록 데이터의 차원은 계속해서 커지는 것이다. 고차원 데이터 셋의 모델 학습 문제데이터를 표현하는 특징과 수가 많을 경우 모델이 더 잘 학습하는 것이 아닐까라는 생각이 들지만, 위에서 말했듯이 차원의 정도가 너무 클 경우 오히려 데이터의 주요 특징들이 희박해지는 현상이 발생하게 되어 모델이 과대적합하게 되는 문제가 발생한다. 이를 해결하기 위한 한가지 해결책은 훈련 샘플의 밀도가 충분히 높아질 때까지 훈련 세트를 키우는 것인데, 실제로는 일정 밀도에 도달하기 위해 필요한 훈련 샘플 수는 차원 수가 커짐에 따라 기하급수적으로 늘어나게되는 문제가 있다. 따라서, 다른 해결책으로 차원을 저차원 공간에 펼치는 투영(projection)이나 차원을 축소하는 주성분 분석(PCA)등이 이용된다. 차원 축소를 위한 방법1. 투영(projection)고차원 공간에 있는 훈렴 샘플을 저차원 공간으로 그대로 수직으로 투영하는 방법이며, 아래 그림과 같이 3차원 공간에 있는 샘플들은 사실 2차원 공간에 놓아도 데이터들의 특성이 많이 뭉개지지 않게 된다. 하지만, 투영하는 것이 모든 상황에 최적인 것은 아니다.위 그림과 같이 데이터가 말려있을 경우 데이터를 그대로 투영하게 되면 어떻게 될까? 위 데이터 샘플을 그대로 수직으로 투영할 경우, 아래 왼쪽과 같은 그림이 된다. 검정, 빨강, 노랑 샘플이 뭉개져버렸기 때문에 2차원에서는 표현을 잘 나타내지를 못하고 있다. 하지만 우리가 원하는 것은 바로 오른쪽과 같이 특성이 뭉개지지 않게 펼쳐진 그림일 것이다. 그리고 이렇게 구부려져 있는 데이터를 반듯이 펴기위해 사용되는 것이 바로 매니폴드 학습이다 2. 매니폴드 학습(manifold learning)위에서 보았던 스위스 롤(swiss roll)데이터는 2D 매니폴드의 한 예였다. 한 가지 예를 더 들어 아래와 같은 데이터가 있다고 해보자. 위 데이터들 간의 거리를 직선상의 거리로 보았을 때 A와 C가 서로 가까울까, 아니면 A와 G가 서로 가까울까?위 그림대로 보았을 때는 A와 C보다는 A와 G사이의 거리가 더 가까워 보인다. 하지만 위 데이터가 실은 아래의 그림을 구부려 놓은 그림이었다면 어떨까? 실제로는 어떤 점이 더 가까운가? 위와 같이, 저차원의 데이터가 고차원의 공간에서 휘어지거나 뒤틀려 있는 것을 매니폴드(manifold)라고 하며, 고차원 공간내에서 뒤틀려있는 데이터를 곧게 펴 유클리디안 거리(euclidean distance) 계산을 통해 데이터들 간의 거리를 찾는 학습을 매니폴드 학습(manifold learning)이라고 한다. 3. 주성분 분석(PCA)주성분 분석(Principal Component Analysis)은 데이터의 차원을 축소하고자 할 떄 가장 인기 있게 사용되는 알고리즘이다. 주성분 분석이란 데이터를 가장 잘 표현하는 초평면을 찾아 분산을 최대로 보존하는 축을 찾는 것이다. 즉, 데이터를 가장 잘 표현하는 n개의 구간을 찾아 그것을 n개의 차원으로 축소하여 표현하는 방법이다. 주성분을 찾는 과정 데이터에 가장 가까운 초평면을 정의한 후, 데이터를 이 평면에 투영 임의의 축을 선택 후, 데이터의 분산을 최대한 보존하는 축을 선택 선택한 축을 기준으로 직교하는 축을 선택(두번째로 분산을 최대한 보존하는 축) 위 과정을 반복하며 찾으려는 차원 수만큼 수행 위의 과정을 통해 찾은 i번째 축을 정의하는 단위 벡터를 i번째 주성분(PC, principal component)라고 하며, 이러한 주성분을 찾는 과정은 특이값 분해(SVD, Singular Value Decomposition)라는 표준 행렬 분해 기술을 통해 이루어 진다. scikit-learn에서 PCA 사용하기사이킷런에서 pca를 사용하기 위해서는 sklearn의 preprocessing모듈에서 PCA모델을 이용하면 된다.1234from sklearn.decomposition import PCApca = PCA(n_components=3)data_3d = pca.fit_transform(data) PCA모델의 중요 파라미터는 n_components인데 이것이 바로 축소할 차원의 수이다. 위의 과정은 3차원으로 축소하게 된다. breast cancer 데이터셋에 PCA를 적용123456import pandas as pd cancer = pd.read_csv('breast_caner.csv')cancer.drop(['id','diagnosis'], axis=1, inplace=True) # 일단 필요없으므로 제외print(cancer.shape)print(cancer.head()) 위 데이터는 총 30개의 차원을 가진 데이터셋이다. 이를 3개의 차원을 가진 데이터로 차원 축소를 해보려고 한다. 여기서 알아두어야 할 것은 보통 차원 축소를 하기 전에는 먼저 데이터를 정규화 한다는 것이다. 데이터 범위를 정규화 함으로써 데이터간 특성 비교를 쉽게하기 위해서다. 먼저 Standard Scaler를 통해 데이터를 정규화 한다.12345from sklearn.preprocessing import StandardScalerscaler = StandardScaler()cancer_scaled = pd.DataFrame(scaler.fit_transform(cancer), columns=cancer.columns)print(cancer_scaled.head()) 이제 PCA를 통해 3차원으로 축소 후 결과를 살펴본다.12345from sklearn.decomposition import PCApca = PCA(n_components=3)cancer_pca = pca.fit_transform(cancer_scaled)print(cancer_pca[:1] 데이터가 numpy array 형태로 3개의 차원으로 축소가 된 것을 확인할 수 있다.","categories":[],"tags":[{"name":"machinelearning","slug":"machinelearning","permalink":"https://jaehyeongan.github.io/tags/machinelearning/"},{"name":"pca","slug":"pca","permalink":"https://jaehyeongan.github.io/tags/pca/"},{"name":"dimensionality","slug":"dimensionality","permalink":"https://jaehyeongan.github.io/tags/dimensionality/"},{"name":"manifold","slug":"manifold","permalink":"https://jaehyeongan.github.io/tags/manifold/"}]},{"title":"텐서플로우(Tensorflow 2.0) GPU버전 사용하기","slug":"tensorflow GPU버전 사용하기","date":"2019-05-01T09:29:10.000Z","updated":"2020-10-28T15:14:34.482Z","comments":true,"path":"2019/05/01/tensorflow GPU버전 사용하기/","link":"","permalink":"https://jaehyeongan.github.io/2019/05/01/tensorflow GPU버전 사용하기/","excerpt":"","text":"Intro머신러닝 모델을 학습할때는 크게 와닿지 않지만 복잡한 딥러닝 연산을 할 때 크게 느껴지는 것이 바로 GPU의 유무이다. 딥러닝과 같은 복잡한 matrix 연산을 하기 위해 CPU로 모델을 돌렸다가는 컴퓨터가 운명을 다 할 수 있다.이전에 텍스트 처리 딥러닝 모델을 CPU와 GPU로 돌렸을 때 얼마나 차이나는지 보려고 실험을 했었는데, GPU의 경우 3시간 정도만에 학습이 끝난 반면 CPU의 경우 거의 한나절을 돌아가고도 결과가 나오지 않아 중간에 끊은 적이 있었다. 본인 컴퓨터에 외장 그래픽이 없다면 할 수 없지만 GPU가 갖춰져 있을 경우 이를 적극 활용하는 것이 정신건강에 좋을 것 같다.하지만, GPU도 다 같은 GPU가 아니다.현재 tensorflow에서 지원하는 GPU는 Nvidia를 기본으로 하며 AMD의 경우 아직 이용하기에 많이 불편하다. tensorflow-gpu 설치1. CUDA 설치우선 CUDA를 설치해야 한다. 현재 CUDA의 경우 최신 버전이 10.2이지만, 확인 결과 아직까지는 공식적으로 tensorflow가 CUDA 10.0버전까지만 지원한다. CUDA Toolkit Arcive(https://developer.nvidia.com/cuda-toolkit-archive)로 이동하여 아래 화면과 같이 CUDA Toolkit 10.0버전을 클릭한다. 클릭 후 아래와 같이 자신의 운영체제 맞는 것을 선택한 후 다운로드를 실시하고 다운로드 된 설치파일을 다른 조건 변경없이 그대로 설치하면 된다. 2. cuDNN 다운로드CUDA 설치를 완료하였다면 이제 cuDNN(https://developer.nvidia.com/rdp/cudnn-download)을 다운로드하여 CUDA 디렉토리에 넣어줘야 한다.cuDNN을 설치하기 위해서는 nvidia에 로그인을 해야하므로 가입이 안되어있다면 가입을 한 후 접속하면 된다. 주의할 점은 위에서 설치한 CUDA버전에 호환되는 cuDNN을 다운로드 해야 한다는 것이다. 위에서 CUDA 10.0버전을 설치해주었기 때문에 cuDNN도 CUDA 10.0에 호환되는 버전(for CUDA 10.0)으로 다운받는다. 위 파일을 다운로드 하면 cudnn-10.0-windows10-x64-v7.5.1.10 라는 압축파일이 다운로드 되는데, 압축파일을 풀게 되면 그 안에 아래와 같은 파일이 들어있다. 3. cuDNN파일 CUDA 폴더로 복사이제부터가 중요한데,방금 전 압축해제 한 폴더의 파일을 모두 복사하여 그대로 처음 설치한 CUDA 폴더로 전부 복사해주어야 한다.우선 압축해제 한 파일들을 전부 복사한 후, C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0 이 경로로 가서 복사한 파일을 그대로 붙여 넣기 해준다.(안에 동일한 파일이 있는데 그냥 덮어씌워주는 것이다.) (위 경로에 그대로 복사한 파일을 덮어씌운다.) 4. 환경변수 지정보통 다른 설정을 건드리지 않고 진행하였을 경우, 환경변수에 아래와 같은 CUDA 경로가 들어있을 것이다. 없다면 아래와 같은 경로를 그대로 환경변수에 지정해준다. 5. tensorflow-gpu 버전 설치이후 Anacoda prompt 혹은 CMD 창을 열어 아래와 같은 명령어로 tensorflow-gpu버전을 설치한다. &gt; pip install tensorflow-gpu혹은&gt; conda install tensorflow-gpu (이미 설치되어 있어서 위와 같이 나옴.) 6. tensorflow 실행 및 확인promt창을 열어 아래와 같이 tensorflow를 import하였을 때 error가 나지 않는다면 우선 tensorflow 설치에 성공한 것이다.설치 된 tensorflow 버전을 확인하고 싶을 때는 tf._version_ 을 통해 확인할 수 있다.12import tensorflow as tf tf.__version__ tensorflow가 GPU버전으로 잘 설치되었고, 나의 GPU를 잘 인식하고 있는지 확인하고 싶다면 아래와 같은 코드를 통해 확인할 수 있다. tensorflow가 인식하는 로컬 device 목록을 보여주게 된다.12from tensorflow.python.client import device_libdevice_lib.list_local_devices() 내 컴퓨터의 GPU의 경우 GeForce GTX 1050 with MAX-Q인 것을 확인할 수 있다.","categories":[],"tags":[{"name":"keras","slug":"keras","permalink":"https://jaehyeongan.github.io/tags/keras/"},{"name":"tensorflow","slug":"tensorflow","permalink":"https://jaehyeongan.github.io/tags/tensorflow/"},{"name":"python","slug":"python","permalink":"https://jaehyeongan.github.io/tags/python/"},{"name":"nvidia","slug":"nvidia","permalink":"https://jaehyeongan.github.io/tags/nvidia/"},{"name":"cuda","slug":"cuda","permalink":"https://jaehyeongan.github.io/tags/cuda/"},{"name":"cudnn","slug":"cudnn","permalink":"https://jaehyeongan.github.io/tags/cudnn/"}]},{"title":"로지스틱 회귀(logistic regression)","slug":"logistic-regression","date":"2019-04-27T05:54:01.000Z","updated":"2020-10-28T15:14:34.480Z","comments":true,"path":"2019/04/27/logistic-regression/","link":"","permalink":"https://jaehyeongan.github.io/2019/04/27/logistic-regression/","excerpt":"","text":"Intro이름은 regression이나 분류 알고리즘으로 주로 사용되는 알고리즘이 있다. 바로 로지스틱 회귀(Logistic Regression)이다. 로지스틱 회귀는 샘플이 특정 클래스에 속할 확률을 추정하는 방식으로 동작하는 이진 분류(Binary Classification) 모델이다. Logistic Regression(로지스틱 회귀)로지스틱 회귀(logistic regression)는 샘플이 특정 클래스에 속할 확률을 추정하는 데 널리 사용된다.(예를 들어 해당 이메일이 spam일 확률과 spam이 아닐 확률)추정 확률이 50% 이상이면 모델은 그 샘플을 해당 클래스(label: 1)에 속한다고 예측하고 50%이하이면 클래스에 속하지 않는다고(label: 0) 예측한다. 확률 추정기존 Regresion으로 분류문제를 해결하고자 할 경우 가장 큰 문제점은 바로 1이상 또는 0이하의 수로 나오는 예측값을 해석하는 일이다. 따라서 0과 1사이의 확률로 표현하여 0.5보다 크면 positive, 0.5보다 작으면 negative로 직관적으로 표현하고자 하는 것이 확률 추정이다.어떤 사건이 일어날 확률은 아래와 같이 표현된다. 선형 회귀 모델과 같이 로지스틱 회귀 모델은 입력 특성의 가중치 합을 계산하고 편향을 더한다. 대신 선형 회귀처럼 바로 결과를 출력하지 않고 결괏값의 로지스틱(logistic)을 아래의 식을 통해 출력한다. Logit Function로짓 함수는 X의 값이 주어졌을 때 Y의 확률을 이용한 log odds이며 아래와 같이 나타낸다. Sigmoid(=logistic) Fuction로지스틱(또는 로짓)은 0과 1사이의 값을 출력하는 시그모이드 함수(Sigmoid Function)이다.(즉, S자 형태)로지스틱 함수는 logit 함수의 역함수 형태로 z에 관환 확률을 산출하며 아래와 같은 식으로 표현된다. 위 그래프와 같이 sigmoid function은 S자 형태로 이루어져 있으며 각 클래스에 속할 확률을 0.5를 기준으로 0.5이상이면 양성 클래스(1)로 예측하고, 0.5이하이면 음성 클래스(0)으로 예측한다. 로지스틱 회귀의 비용 함수(Cost Function)로지스틱 회귀 모델의 훈련 목적은 양성 샘플(y=1)에 대해서는 높은 확률을 추정하고 음생 샘플(y=0)에 대해서는 낮은 확률을 추정하는 모델의 파라미터 벡터 theta를 찾는 것이다. 전체 훈련 세트에 대한 비용 함수는 모든 훈련 샘플의 비용을 평균한 것이다. 이를 로그 손실(log loss)라고 부르며 아래와 같은 식으로 표현된다. 로지스틱 회귀 적용 breast cancer 데이터셋에 로지스틱 회귀를 적용해본다.우선 breast cancer 데이터 셋은 총 30개의 features와 암 여부에 해당하는 1개의 y label값을 가지고 있다.scikit-learn의 데이터 셋을 활용하였는데 이 데이터의 경우 이미 z-score 정규화가 되어있는 데이터 셋이다.123456789from sklearn.datasets import load_breast_cancerfrom sklearn.model_selection import train_test_splitbreast_cancer = load_breast_cancer()X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, stratify=cancer.target, random_state=42)print(X_train.shape) # (426, 30)print(y_train.shape) # (426, ) 훈련 셋과 테스트 셋을 나눈 데이터에 scikit-learn에서 제공하는 로지스틱 회귀 모델을 적용한다.12345from sklearn.linear_model import LogisticRegressionlogreg = LogisticRegression(C=0.01).fit(X_train, y_train)print(\"train set score : \", logreg.score(X_train, y_train)) # 0.934print(\"test set score : \", logreg.score(X_test, y_test)) # 0.930 결과를 보았을 때 모델이 train set과 test set에 모두 93%의 정확도로 암을 분류하였는데, 실제로 잘 분류되었는지 test data 중 앞 10개만 실제 값과 비교해보았다.1234pred_10 = logreg.predict(X_test)print('실제 값: ',y_test[:10]) # 실제 값: [1 0 1 1 0 0 0 0 0 1]print('예측 값: ',pred_10[:10]) # 예측 값: [1 0 1 1 1 0 1 0 0 1] 두 개의 예측값만 빼고(암 아님을 암이라고 예측) 실제 값과 같은 예측결과를 보여준다. Softmax Regression(소프트맥스 회귀)로지스틱 회귀 모델은 여러 개의 이진 분류기를 훈련시켜 연결하지 않고도 직접 다중 클래스를 예측하도록 할 수 있다.이를 소프트맥스 회귀(Softmax Regression) 또는 다항 로지스틱 회귀(Multinomial Logistic Regression)라고 한다. — 샘플 x가 주어지면 소프트맥스 회귀 모델이 각 클래스 k에 대한 점수를 계산 — 그 점수에 소프트맥스 함수(softmax function)을 적용하여 각 클래스의 확률을 추정 — 로지스틱 회귀와 마찬가지로 추정 확률이 가장 큰 클래스를 선택 — 각 클래스가 될 확률 값을 모두 더하면 1이 됨 소프트맥스 함수— k는 클래스의 수이며, s(x)는 샘플 x에 대한 각 클래스의 점수를 담고 있는 벡터 소프트맥스 회귀 분류기의 예측 크로스 엔트로피(cross-entropy) 비용 함수 — 크로스 엔트로피는 추정된 클래스의 확률이 타깃 클래스에 얼마나 잘 맞는지 측정하는 용도로 사용 Softmax Regression 적용scikit-learn의 LogisticRegression() 클래스에 multi_class 매개변수를 “multinomial”로 바꾸면 소프트맥스 회귀를 사용 — 소프트맥스 회귀 사용 시 solver 매개변수를 “lbfgs” 지정 — C를 사용하여 l2규제 적용 1234from sklearn.linear_model import LogisticRegressionsoftmax_reg = LogisticRegression(multi_class='multinomial', solver='lbfgs', C=10)spftmax_reg.fit(X_train, y_train)","categories":[],"tags":[{"name":"machinelearning","slug":"machinelearning","permalink":"https://jaehyeongan.github.io/tags/machinelearning/"},{"name":"regression","slug":"regression","permalink":"https://jaehyeongan.github.io/tags/regression/"},{"name":"classification","slug":"classification","permalink":"https://jaehyeongan.github.io/tags/classification/"},{"name":"python","slug":"python","permalink":"https://jaehyeongan.github.io/tags/python/"},{"name":"logistic","slug":"logistic","permalink":"https://jaehyeongan.github.io/tags/logistic/"},{"name":"scikitlearn","slug":"scikitlearn","permalink":"https://jaehyeongan.github.io/tags/scikitlearn/"}]},{"title":"선형 모델(Linear Model)","slug":"Linear-Regression","date":"2019-04-25T06:27:13.000Z","updated":"2020-10-28T15:14:34.476Z","comments":true,"path":"2019/04/25/Linear-Regression/","link":"","permalink":"https://jaehyeongan.github.io/2019/04/25/Linear-Regression/","excerpt":"","text":"Intro머신러닝을 원리를 이해하기 위해 가장 먼저 배우게 되는 선형 모델(linear models)에 대한 글이다. 선형 모델은 100여 년 전게 개발되었고, 지난 몇십 년 동안 폭넓게 연구되고 현재도 널리 쓰이고 있다. 기본적으로 선형 모델은 입력 특성에 대한 선형 함수를 만들어 예측을 수행한다. Linear Regression(선형 회귀)회귀의 경우 선형 모델을 위한 일반화된 예측 함수는 아래와 같다. 위 식에서 x[0]부터 x[n]까지는 하나의 데이터 포인트에 대한 특성을 나타내며(특성의 개수는 n+1), w와 b는 모델이 학습할 파라미터이다.그리고 y^은 모델이 만들어낸 예측값이다.위 식은 특성이 하나인 데이터 셋이라면 아래와 같이 1차 방정식으로 단순하게 나타낼 수 있다. w[0]는 기울기이고, b는 y축과 만나는 절편(또는 편향)이다. 특성이 많아지면 w는 각 특성에 해당하는 기울기를 모두 가진다. [선형 회귀] 선형 회귀는 가장 간단하고 오래된 회귀용 선형 알고리즘이다. 선형 회귀는 예측 값 y^과 실제 값 y 사이의 평균제곱오차(mean squared error)를 최소화 하는 파라미터 w와 b를 찾는다. 평균제곱 오차는 예측값(y^)과 실제값(y)의 차이를 제곱하여 더한 후에 샘플 개수로 나눈 값이다. 아래는 scikit-learn의 LinearRegression을 통해 boston house price를 통한 집 값 예측을 수행하는 코드이다. 12345678910111213141516import numpy as npfrom sklearn.datasets import load_bostonfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import mean_squared_error# datasetX, y = load_boston(True)X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)####### Linear Regression #######lin_reg = LinearRegression().fit(X_train, y_train)train_pred = lin_reg.predict(X_train)test_pred = lin_reg.predict(X_test)print('MSE of train set: ', mean_squared_error(y_train, train_pred)) # 19.640print('MSE of test set: ', mean_squared_error(y_test, test_pred)) # 29.782 training set과 test set에 대한 MSE가 각각 19.6, 29.7로 나타났다. 이는 모델이 training set에 과대적합(Overfitting)되었다는 이야기다. 하지만 선형회귀에서는 이런 과대적합을 방지할 규제(regularization)방안이 없다. 때문에 규제 방안이 포함되어 있는 알고리즘(Ridge, Lasso, ElasticNet 등)을 이용하는 것이 효율적일 수 있다. 이는 좀 더 아래에서 살펴볼 것이다. Polynomial Regression(다항 회귀)가지고 있는 데이터가 단순한 직선보다 복잡한 형태라면 어떻게 선형회귀를 적용해야 할까? 신기하게도 비선형(Non-lieaner) 데이터를 학습하는 데 선형 모델을 사용할 수 있다. 바로 각 특성의 거듭제곱을 새로운 특성으로 추가하고, 이 확장된 특성을 포함한 데이터셋에 선형 모델을 훈련시키는 것이다. 이런 기법을 다항 회귀(Polynomial Regression)이라고 한다. 만약 아래와 같이 임의로 만든 2차 방정식의 비선형 데이터가 있다고 해보자.123m = 100X = 6 * np.random.rand(m,1)-3y = 0.5*X**2+X+2+np.random.randn(m,1) 위와 같은 데이터가 non-linear 데이터인데 선형회귀 모델을 위 데이터에 적용해보면,123lin_reg = LinearRegression()lin_reg.fit(X, y)pred = lin_reg.predict(X) 위 그림과 같이 데이터의 비선형적 패턴을 전혀 파악하지 못한 채 1차 직선으로만 예측을 하게 된다.이제 이러한 비선형 데이터를 선형 예측하기 위해 위 데이터에 각 특성을 제곱하여 새로운 특성을 추가한다. 1234567from sklearn.preprocessing import PolynomialFeaturespoly_features = PolynomialFeatures(degree=2, include_bias=False)X_poly = poly_features.fit_transform(X)lin_reg = LinearRegression()lin_reg.fit(X_poly, y)pred = lin_reg.predict(X_poly) 데이터에 새로운 다항 특성을 추가하였을 때 선형 모델이 데이터의 패턴을 파악하여 예측하는 특성을 보여주고 있다.위 데이터의 실제 함수는,이고, 예측 모델의 함수는,이므로 실제 값과 예측 값의 차이가 많지 않음을 알 수 있다. Regularized Linear Regression(규제가 있는 선형 모델)위에서 살펴보았듯 이 선형 모델의 경우 모델이 훈련 데이터 셋에 과대적합(overfitting)되더라도 모델을 규제할 방안이 없다. 따라서, 과대적합을 감소시키기 위해서는 모델의 가중치(weight)를 규제(제한)함으로써 과대적합되기 어렵게 만들어야 한다. 이렇게 가중치를 제한하는 알고리즘으로 릿지(Ridge), 라쏘(Lasso), 엘라스팃넷(ElasticNet) 회귀에 대해 살펴보려고 한다. 1. Ridge Regression(릿지 회귀)선형 회귀에 규제가 추가된 회귀 모델이다. 규제항이 비용함수에 추가 되며 이는 학습 알고리즘을 데이터에 맞추는 것뿐만 아니라 모델의 가중치가 가능한 한 작게 유지되도록 한다. 규제항은 훈련하는 동안에만 비용함수에 추가되고, 훈련이 끝나면 모델의 성능을 규제가 없는 성능 지표로 평가한다. 선형회귀에 규제(L2: 가중치들의 제곱합을 최소화)를 걸어 과대적합을 방지 하이퍼파라미터 a(alpha)는 모델을 얼마나 규제할지 조절 a = 0 이면, 릿지 회쉬는 선형회귀와 같음 a 가 아주 크면 모든 가중치가 거의 0에 가까워지고 결국 데이터의 평균을 지나는 수평선이 됨릿지 회귀의 비용함수는 아래 수식과 같다. 위에서 보았던 boston house price 예측에 ridge 회귀를 적용할 경우 선형 회귀보다 더 나은 성능을 얻을 수 있다. 123456789####### Ridge Regression ####### from sklearn.linear_model import Ridgeridge = Ridge(alpha=0.1).fit(X_train, y_train)train_pred = ridge.predict(X_train)test_pred = ridge.predict(X_test)print('MSE of train set: ', mean_squared_error(y_train, train_pred)) # 19.645print('MSE of test set: ', mean_squared_error(y_test, test_pred)) # 29.878 2. Lasso Regression(라쏘 회귀)라쏘 회귀 역시 선형 회귀에 규제가 추가된 모델이며, 릿지 회귀에서 사용된 L2 규제가 아닌 가중치 벡터의 L1 노름을 사용한다. 라쏘 회귀의 가장 중효한 특징은 덜 중요한 특성의 가중치를 완전히 제거하려고 한다는 것이다. 자동으로 덜 중요한 특성을 제거하는 특성 선택(feature selection)을 수행하고 희소 모델(spare model)을 만듬(즉, 0이아닌 특성의 가중치가 작음) 이를 통해 모델을 이해하기 쉬워지고 모델의 가장 중요한 특성이 무엇인지 파악 가능라쏘 회귀의 비용함수는 아래 수식과 같다. 마찬가지로 boston house에 Lasso 모델을 적용한다. lasso모델의 coef_ 파라미터를 이용하면 몇 개의 특성이 제외되고 사용되었는지 알 수 있다. 12345678910####### Lasso Regression #######from sklearn.linear_model import Lassolasso = Lasso(alpha=0.01, max_iter=100000).fit(X_train, y_train)train_pred = lasso.predict(X_train)test_pred = lasso.predict(X_test)print('MSE of train set: ', mean_squared_error(y_train, train_pred)) # 19.678print('MSE of test set: ', mean_squared_error(y_test, test_pred)) # 30.091print('사용한 특성의 수 : ',np.sum(lasso.coef_ != 0)) # 13 3. Elastic Net(엘라스틱넷)엘라스틱넷은 릿지 회귀와 라쏘 회귀를 절충한 모델이다. 규제항은 릿지와 회귀의 규제항을 단순히 더해서 사용하며, 혼합 정도는 혼합 비율 r을 사용해 조절한다. r = 0이면, 엘라스틱넷은 릿지 회귀와 같고, r = 1이면, 라쏘 회귀와 같아짐 엘라스틱넷의 비용함수는 아래 수식과 같다. elastic net도 boston house price에 적용. 123456789####### ElasticNet #######from sklearn.linear_model import ElasticNetelastic = ElasticNet(alpha=0.001, max_iter=10000000).fit(X_train, y_train)train_pred = elastic.predict(X_train)test_pred = elastic.predict(X_test)print('MSE of train set: ', mean_squared_error(y_train, train_pred)) # 19.657print('MSE of test set: ', mean_squared_error(y_test, test_pred)) # 29.974 Outro그렇다면 선형회귀, 릿지, 라쏘, 엘라스틱넷을 각각 언제, 어떤 상황에 사용해야 좋을까?적어도 규제가 있는 모델이 대부분의 상황에서 좋으므로 일반적으로 선형회귀 모델을 사용하는 것은 피하는 것이 좋다. 기본적으로 ridge 회귀가 기본이 되어 사용 됨 하지만, 특성이 많고 그 중 일부분만 중요하다면 lasso나 elastic net이 더 좋은 선택일 수 있음 또한, 특성 수가 훈련 샘플 수보다 많거나 특성 몇 개가 강하게 연관되어 있을 때는 lasso보다는 elastic net이 선호 됨","categories":[],"tags":[{"name":"regression","slug":"regression","permalink":"https://jaehyeongan.github.io/tags/regression/"},{"name":"linear","slug":"linear","permalink":"https://jaehyeongan.github.io/tags/linear/"},{"name":"ridge","slug":"ridge","permalink":"https://jaehyeongan.github.io/tags/ridge/"},{"name":"lasso","slug":"lasso","permalink":"https://jaehyeongan.github.io/tags/lasso/"},{"name":"elsasticnet","slug":"elsasticnet","permalink":"https://jaehyeongan.github.io/tags/elsasticnet/"}]},{"title":"경사하강법(Gradient Descent)","slug":"경사하강법-Gradient-Descent","date":"2019-04-23T12:52:20.000Z","updated":"2020-10-28T15:14:34.483Z","comments":true,"path":"2019/04/23/경사하강법-Gradient-Descent/","link":"","permalink":"https://jaehyeongan.github.io/2019/04/23/경사하강법-Gradient-Descent/","excerpt":"","text":"Intro최적의 예측 모델을 만들기 위해서는 실제값(true)과 예측값(predict)과의 Error(cost function)가 최소가 되는 모델을 찾는 것이다. 하지만 분석자가 직접 모델의 Cost function을 최소화시키는 파라미터 값을 찾기 위해서는 수십 번의 파라미터 변경이 필요하기 때문에 모델이 학습과정에서 스스로 cost function이 최소가 되도록 파라미터를 조정해나가는 경사하강법(Gradient Decent)이 사용된다. Gradient Descent경사하강법(Gradient Descent) 경사하강법이란 비용함수(Cost Function)을 최소화하기 위하여 반복적해서 파라미터를 조정해나가는 것을 말한다.만약 한 밤 중에 산에서 길을 잃었을 때, 산 밑으로 내려가는 가장 좋은 방법은 무엇일까? 바로 가장 가파른 길을 따라 산 아래로 내려가는 것이다. 이와 같이 최적의 값에 도달하기 위해 가장 빠른 길을 찾는 과정을 경사 하강법의 기본원리라고 할 수 있다.— 파라미터 벡터 theta()에 대해 cost function의 현재 gradient를 계산— theta()의 경우 임의의 값으로 시작해서(random initialization) 조금씩 cost function이 감소되는 방향으로 진행 학습률(learning rate)경사 하강법에서 중요한 파라미터로서 학습 시 스템(step)의 크기 학습률이 너무 작을 경우— 알고리즘이 수렴하기 위해 반복을 많이 진행해야 하므로 학습 시간이 오래걸림— 지역 최솟값(local minimum)에 수렴할 수 있음 학습률이 너무 클 경우— 학습 시간이 적게 걸리나— 스텝이 너무 커 전역 최솟값(global minimum)을 가로질러 반대편으로 건너뛰어 최솟값에서 멀어질 수 있음 경사하강법의 문제점— 무작위 초기화(random initialization)으로 인해 알고리즘이 전역 최솟값이 아닌 지역 최솟값에 수렴할 수 있음— 평탄한 지역을 지나기 위해선 시간이 오래 걸리고 일찍 멈추게 되어 전역 최솟값에 도달하지 못할 수 있음— 하지만 선형 회귀(Linear Regression)를 위한 MSE(Mean Squared Error) cost function은 어떤 두점을 선택해 어디에서 선을 그어도 곡선을 가로지르지 않는 볼록 함수(convex function)임— 이는 지역 최솟값이 없고 하나의 전역 최솟값만을 가지는 것을 뜻하며, 연속된 함수이고 기울기가 갑자기 변하지 않음 Batch Gradient Descent— 경사하강법을 구현하려면 각 모델 파라미터 theta()에 대해 비용 함수의 그래디언트를 계산해야 함.— 즉, theta()가 조금 변경될 때 비용함수가 얼마나 변하는지 계산해야 하는데 이를 편도 함수(partial derivative)라고 함.— 매 gradient descent step에서 훈련 데이터 전체를 사용— 그렇기 때문에 매우 큰 training set에서는 학습이 매우 느림 Stochastic Gradient Descent(SGD) — 매 step에서 딱 한 개의 샘플을 무작위로 선택하고 그 하나의 샘플에 대한 gradient를 계산— 매우 적은 데이터를 처리 하기 때문에 학습 속도가 빠르고, 하나의 샘풀만 메모리에 있으면 되므로 매우 큰 training set도 훈련이 가능— cost function이 매우 불규칙할 경우 알고리즘이 local minimum을 건너뛰도록 도와주므로 global minimum을 찾을 가능성이 높음— 하지만 샘플 선택이 확률적(Stochastic)이기 때문에 배치 경사 하강법에 비해 불안정— cost function이 local minimum에 다다를 때까지 부드럽게 감소하지 않고 위아래로 요동치면서 평균적으로 감소 Mini-batch Gradient Descent— mini-batch라 불리는 임의의 작은 샘플 세트에 대해 gradient를 계산— SGD에 비해 matrix 연산에 최적화되어 있으며, 파라미터 공간에서 덜 불규칙하게 학습— 하지만, local minimum에 빠지면 빠져나오기 힘듬 Outro아래는 batch, mini-bath, stochastic gradient descent의 경사 하강법 진로를 살펴본 그림이다.(출처 : 핸즈온 머신러닝) 모두 최솟값 근처에 도달했지만 배치 경사 하강법의 경로가 실제로 최솟값에서 멈춘 반면 확률적 경사 하강법 및 미니배치 경사하강법은 근처에서 맴돌고 있다. 그렇지만 배치 경사 하강법에는 매 스텝에서 많은 시간이 소요되고, 확률적 경사 하강법과 미니배치 경사 하강법도 적절한 학습 스케쥴(learning schedule)을 사용하면 최솟값에 도달할 수 있다.","categories":[],"tags":[{"name":"deeplearning","slug":"deeplearning","permalink":"https://jaehyeongan.github.io/tags/deeplearning/"},{"name":"machinelearning","slug":"machinelearning","permalink":"https://jaehyeongan.github.io/tags/machinelearning/"},{"name":"learningrate","slug":"learningrate","permalink":"https://jaehyeongan.github.io/tags/learningrate/"},{"name":"gradientdescent","slug":"gradientdescent","permalink":"https://jaehyeongan.github.io/tags/gradientdescent/"},{"name":"batchgd","slug":"batchgd","permalink":"https://jaehyeongan.github.io/tags/batchgd/"},{"name":"minibatchgd","slug":"minibatchgd","permalink":"https://jaehyeongan.github.io/tags/minibatchgd/"},{"name":"sgd","slug":"sgd","permalink":"https://jaehyeongan.github.io/tags/sgd/"},{"name":"meansquarederror","slug":"meansquarederror","permalink":"https://jaehyeongan.github.io/tags/meansquarederror/"}]},{"title":"머신러닝을 위한 아나콘다(Anaconda) 개발환경 구축","slug":"machine-learning-개발환경-구축하기","date":"2019-04-09T13:06:02.000Z","updated":"2020-10-28T15:14:34.481Z","comments":true,"path":"2019/04/09/machine-learning-개발환경-구축하기/","link":"","permalink":"https://jaehyeongan.github.io/2019/04/09/machine-learning-개발환경-구축하기/","excerpt":"","text":"Intro대부분 처음 머신러닝를 하고자 마음먹고 시도하는 것이 개발환경을 만드는 것 입니다.물론 데이터 사이언스에 대한 기본적 이해가 먼저 뒷받침이 되어 있어야 하겠지만 처음부터 너무 어렵게 시작하면 재미없자나요? 먼저 python의 세계에서 hello world 부터 찍어봐야져. 오늘은 Machine learning, data science 뭐 등등을 하기 위해 가장 많이 사용되고 있는 언어인 Python 개발환경을 구축하는 방법을 소개합니다.(내 개발환경 기준으로)(JDK는 설치되어있어야 합니다.) 굳이 python을 설치할 필요 없이, data science 패키지 모음인 Anaconda를 설치할 것입니다.Anaconda는 수학 및 과학 관련 numpy, scipy, pandas, matplotlib과 같은 유용한 python package를 모아놓은 배포판입니다. 분명히 장점이긴 하지만 잘 사용하지 않는 패키지까지 모두 포함하고 있어 무겁다는 단점이 있습니다. 따라서, 저는 Anaconda의 축소판인 Miniconda를 깔도록 하겠습니다.Miniconda는 Anaconda와 다르게 사용하고자 하는 패키지를 스스로 설치해야하는 번거로움이 있지만 가볍다는 장점이 있습니다. Anaconda vs MinicondaAnaconda— python이나 conda를 처음 접하는 경우 좋음— python과 150개 이상의 과학 패키지를 한 번에 자동 설치하여 편리— 강력한 script editor인 Jupyter notebook이 포함되어 있음— 3gb의 여유 용량이 필요 Miniconda— 적은 용량(100mb 이하)— 스스로 원하는 패키지를 설치하고자 할 경우 좋음(좋은 습관) Install Miniconda(+python)우선 Miniconda 홈페이지를 가면 설치가 가능합니다. 위 링크에서는 현재 python 2.7버전과 3.7버전을 제공하고 있습니다. 최신버전이므로 다운받으셔도 무방하지만 한 가지 고려할 것이 있습니다. 만약 추후 deep learning을 하고자 한다면 3.7버전 보다 아래 버전을 사용하시는 것이 좋습니다. tensorflow가 아직 3.7버전에 호환되지 않거든여ㅜㅜ그래서 Miniconda installer archive에서 python3.6버전으로 되어있는 것을 찾으시면 됩니다.저는 안정성 문제를 고려하여 Miniconda2-4.5.4-Windows-x86_64.exe을 이용하고 있습니다. (수정)다시 알아보니 이제 python 3.7버전도 tensorflow와 호환이 된다네요! 그냥 Miniconda 홈페이지에서 최신버전을 다운받으셔도 무방할 것 같습니다. 다운받은 후 설치파일을 더블클릭하면 아래와 같은 화면이 나타납니다. 이후, next버튼을 눌러 설치를 진행하시면 됩니다. 설치가 완료되면 설치된 경로에 아래와 같이 miniconda가 깔립니다.(동시에 python도 같은 경로에 설치가 됩니다.) Enroll System path자, 이제 miniconda 및 python을 설치하였으니 환경변수에 등록해줘야 합니다. java jdk 등록하는거와 같습니다.우선 환경변수에 등록해야 하는 path는 아래와 같습니다.(본인이 설치한 path에 맞게 넣어주시면 됩니다.) C:\\Users\\nonam\\Miniconda3 C:\\Users\\nonam\\Miniconda3\\python.exe C:\\Users\\nonam\\Miniconda3\\Scripts C:\\Users\\nonam\\Miniconda3\\Library\\bin anaconda 및 miniconda를 설치하게 되면 anaconda prompt와 같은 콘솔창이 함께 설치됩니다.검색창에서 anaconda prompt를 실행하여 아래와 같이 python이 실행된다면 설치가 완료된 것입니다. Install Jupyter lab이제 강력한 data science 에디터인 jupyter lab을 설치해보겠습니다.(jupyter lab은 jupyter notebook보다 다양한 기능을 가지고 있고 파일 관리가 쉬워 애용합니다.)anaconda prompt를 열어 아래와 같이 명령어(jupyter lab 및 해당 kernel을 설치)를 실행합니다.123&gt; conda install -c conda-forge jupyterlab&gt; python -m ipykernel install --user 실행이 되면 http://localhost:8888/ 주소로 jupyter lab이 실행되며 아래와 같은 화면이 나타납니다. Outrojupyter notebook은 communication computing shell이라고 할 수 있습니다. python의 결과를 바로바로 확인가능하기 때문에 데이터 분석 및 시각화에서 아주 강력한 툴이지요.하지만, 대용량 데이터를 처리해야 하고 loop및 조건문이 자주 코드에 포함된다면 조금 다를 수 있습니다.jupyter notebook은 변수에 메모리를 적재 후 지속적으로 메모리를 차지하기 때문에 대용량 데이터와 같은 고성능 데이터 처리에는 그다지 추천드리지 않습니다. 따라서 저는 간단히 데이터의 분포 및 분석을 위해서만 jupyter를 사용하는 편입니다. 그 외 전체적인 코딩은 sublime text3라는 editor를 사용하고 있습니다.추후 python 코딩을 위한 강력한 또다른 editor인 sublime text3에 대해 소개해드리도록 하겠습니다.","categories":[],"tags":[{"name":"machinelearning","slug":"machinelearning","permalink":"https://jaehyeongan.github.io/tags/machinelearning/"},{"name":"python","slug":"python","permalink":"https://jaehyeongan.github.io/tags/python/"},{"name":"anaconda","slug":"anaconda","permalink":"https://jaehyeongan.github.io/tags/anaconda/"},{"name":"miniconda","slug":"miniconda","permalink":"https://jaehyeongan.github.io/tags/miniconda/"},{"name":"jupyter","slug":"jupyter","permalink":"https://jaehyeongan.github.io/tags/jupyter/"},{"name":"sublimetext3","slug":"sublimetext3","permalink":"https://jaehyeongan.github.io/tags/sublimetext3/"},{"name":"datascience","slug":"datascience","permalink":"https://jaehyeongan.github.io/tags/datascience/"}]},{"title":"Keras functional api - Multi-input 모델 구축하기","slug":"KERAS-FUNCTIONAL-API-MULTI-INPUT-모델-구축하기","date":"2019-03-25T15:40:27.000Z","updated":"2020-10-28T15:14:34.473Z","comments":true,"path":"2019/03/26/KERAS-FUNCTIONAL-API-MULTI-INPUT-모델-구축하기/","link":"","permalink":"https://jaehyeongan.github.io/2019/03/26/KERAS-FUNCTIONAL-API-MULTI-INPUT-모델-구축하기/","excerpt":"","text":"Intro지난 한달간 회사 프로젝트를 위해 공부한 내용을 정리할 겸 오늘은 keras functional api(함수형 api)에 대한 소개와 이것을 어떻게 적용하는지를 LSTM모델과 embedding모델을 통해 간단히 소개하려고 한다. 그동안 keras를 통해 딥러닝 모델을 구축하기 위해서는 Sequential 모델을 이용하였을 것이다.Sequential 모델은 네트워크의 입력과 출력이 하나라고 가정하고 층을 차레대로 쌓아 구성한다. 따라서 위와 같은 Sequential 모델에 데이터를 학습하기 위해서는 모든 데이터를 같은 방식으로 전처리하여 모델에 맞게 shape을 구성해주어야 한다.하지만, 위와 같은 구성이 맞지 않는 경우도 존재한다. 예를 들어, 중고 의류 시장 가격을 예측하는 딥러닝 모델을 만든다고 가정해보겠다. 이 모델은 시장 가격 예측을 위해 의류 브랜드, 제작 연도와 같은 정보(메타 데이터), 사용자가 제공한 제품 리뷰(텍스트 데이터), 해당 의류 사진(이미지 데이터)과 같은 데이터를 받는다. 모델은 데이터의 특성에 맞게 적절히 사용되어야 하는데, 해당 데이터가 text인지, image인지, time-series인지에 따라 학습하는 모델도 달라진다.위와 같은 경우,메타 데이터만 있다면 이를 one-hot encoding하여 단순한 DenseNet모델을 구현할 수 있을 것이고,텍스트 데이터의 경우 이를 word2vec 같은 기법을 통해 벡터로 변환하여 Embedding 모델이나 혹은 RNN모델을 구현할 수 있을 것이고,이미지 데이터의 경우 CNN과 같은 ConveNet 모듈을 이용하여 데이터를 학습할 수 있을 것이다. keras functional api하지만 방금 살펴본 것과 같이 예측에 사용되는 데이터가 여러 형태로 존재한다면 어떤 모델을 사용해야 할까?단순히 텍스트와 이미지를 vectorize하여 예측 변수로 추가하여 사용해야 할까?데이터 특성에 따라 각각 모델을 학습시킬 순 없을까? 이러한 의문을 해결해줄 것이 바로 오늘 살펴볼 Keras Functional API이다함수형 api라고 불리며, 말 그대로 모델을 함수처럼 필요할 때 호출하여 사용할 수 있도록 한다. 즉, 모델을 함수로 구현하여 모듈식으로 이용한다는 말이다. 다시 위의 예로 돌아가 함수형 API를 활용하면 아래 그림과 같이 모델별 학습 및 예측이 가능해진다. 위 그림과 같은 모델을 다중입력모델(multi-input model)이라고 하며 이 외에도 다중출력모델(multi-output model)이 존재합니다. 다중입력모델: 데이터 특성에 따른 서로 다른 여러개의 모델이 input으로 사용되어 하나의 output을 내는 네트워크 다중출력모델: 하나의 output이 아닌 데이터에 있는 여러 속성을 동시에 예측하는 네트워크 함수형 API는 기존 구현방법과 구조적으로 차이가 있다.보통 모델을 구현할 때 Sequential()객체를 생성 후 시퀀스 형태로 순차적으로 layer를 쌓아가지만 함수형 api는 Model()객체를 통해 모델을 구현한다. 기존 Sequential() 사용 시 123456789from keras import models, layersmodel = models.Sequential()model.add(layers.Dense(64, activation='relu', input_shape=(784,)))model.add(layers.Dense(64, activation='relu'))model.add(layers.Dense(10, activation='softmax'))model.fit(data, labels) # starts training funciontal api() 사용 시 1234567891011121314from keras.layers import Input, Densefrom keras.models import Model# This returns a tensorinputs = Input(shape=(784,))# a layer instance is callable on a tensor, and returns a tensorx = Dense(64, activation='relu')(inputs)x = Dense(64, activation='relu')(x)outputs = Dense(10, activation='softmax')(x)# This creates a model that includes# the Input layer and three Dense layersmodel = Model(inputs=inputs, outputs=outputs) 위와 같이 Sequential()객체는 input부터 output까지 순차적으로 이루어지지만, 함수형 api는 각각의 변수에 layer를 받아 모듈별로 구성할 수 있으며, 마지막에는 Model()객체에 input과 output텐서를 지정하여 모델을 생성한다. 적용그렇다면 직접 keras를 이용하여 적용하는 과정을 소개하려고 한다.데이터 셋과 전처리 과정은 공개할 순 없으나 해당 데이터는 일반 Sequence 데이터 및 Text 데이터로 이루어져 있고, 고장발생에 대한 여부를 예측하는 문제이다. functional api를 적용하기 위하여 두개의 모델을 구축하였다. Sequence 데이터를 위해서는 시간 및 순서가 있는 데이터에 효율적인 LSTM(Long Short Term Memory Network)를 이용하였고, text 데이터는 vectorize 후 Embedding 모델을 이용하였다. 개략적인 모델 구성도는 대략 아래 그림과 같다. 1. LSTM 모델 적용을 위한 Sequence 데이터 처리우선 LSTM과 같은 Recurrent 모델은 크기가 (timesteps, input_features)인 2D 텐서로 인코딩된 벡터의 시퀀스를 입력받기 때문에 shape을 맞추어 준다.shape을 맞춰주기 전에 우선 text변수와 target 값을 제외해준 후, 데이터를 normalize해주었다. 12345678910111213## preprocessing for lstm sequence_train = df[:train_size].drop(['text_data','target'], axis=1)sequence_test = df[train_size:].drop(['text_data','target'], axis=1)# normalizescaler = StandardScaler().fit(sequence_train)sequence_train_scale = scaler.transform(sequence_train)sequence_test_scale = scaler.transform(sequence_test)timesteps = 1columns_size = len(sequence_train.columns)sequence_train = sequence_train_scale.reshape((sequence_train_scale.shape[0], timesteps, columns_size))sequence_test = sequence_test_scale.reshape((sequence_test_scale.shape[0], timesteps, columns_size)) 2. Embedding 모델 적용을 위한 text 데이터 처리Embedding 모델을 구현하기 위하여 먼저 데이터를 3D 텐서로 변환시켜주어야 한다. 이를 위해 keras의 Tokenizer()객체를 이용하였다. 과정은 아래와 같다. fit_on_texts(): 텍스트 데이터를 통해 word index를 구축 texts_to_sequences(): word index를 통해 해당 텍스트를 시퀀스 형태로 변환 pad_sequences(): 3D 텐서로 변환하기 위해 padding을 추가 123456789101112131415## preprocessing for embeddingtext_embed = df.loc[:, ['text_data']]text_embed_train = text_embed[:train_size]text_embed_test = text_embed[train_size:]# tokenizemax_words = 1000 # 사용할 최대 단어 수 max_len = 50 # 단어의 길이tokenizer = text.Tokenizer(num_words=max_words) # top 1,000 wordstokenizer.fit_on_texts(text_embed_train) # word_index 구축sequences_text_train = tokenizer.texts_to_sequences(text_embed_train) # return sequencesequences_text_test = tokenizer.texts_to_sequences(text_embed_test) # add padding pad_train = sequence.pad_sequences(sequences_text_train, maxlen=max_len)# return 3D tensorpad_test = sequence.pad_sequences(sequences_text_test, maxlen=max_len) # return 3D tensor 3. multi-input model 구축우선 LSTM모델과 Embedding모델을 만든 후 concatenate(model1, model2)함수를 이용하여 서로 다른 두 개의 모델의 output을 하나의 모델로 통합할 수 있다. 12345678910111213141516171819202122232425262728293031323334def multi_input_lstm_embedding_model(timesteps, columns_size, max_words, max_len): # lstm model lstm_input = layers.Input(shape=(timesteps, columns_size)) lstm_out = layers.LSTM(64, dropout=0.3, recurrent_dropout=0.3)(lstm_input) lstm_model = Model(inputs=lstm_input, outputs=lstm_out) # embedding model embed_input = layers.Input(shape=(None,)) embed_out = layers.Embedding(max_words, 8, input_length=max_len)(embed_input) embed_out = layers.Bidirectional(layers.LSTM(64, dropout=0.3, recurrent_dropout=0.3))(embed_out) embed_model = Model(inputs=embed_input, outputs=embed_out) # concatenate concatenated = layers.concatenate([lstm_model.output, embed_model.output]) concatenated = layers.Dense(32, activation='relu')(concatenated) concatenated = layers.BatchNormalization()(concatenated) concat_out = layers.Dense(2, activation='sigmoid')(concatenated) concat_model = models.Model([lstm_input, embed_input], concat_out) return concat_model## model defineconcat_model = multi_input_lstm_embedding_model(timesteps, columns_size, max_words, max_len)concat_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])# model fitconcat_model.fit([df_label_train, pad_train], target_train epochs=7, batch_size=32, callbacks=callbacks_list, validation_data=([sequence_test, pad_test], target_test), shuffle=False) # because of time-series Outrokeras functional api를 이용한다면 좀 더 데이터 특성에 유연하게 모델을 학습시킬 수 있다는 것이 큰 장점인 것 같다.","categories":[],"tags":[{"name":"rnn","slug":"rnn","permalink":"https://jaehyeongan.github.io/tags/rnn/"},{"name":"lstm","slug":"lstm","permalink":"https://jaehyeongan.github.io/tags/lstm/"},{"name":"deeplearning","slug":"deeplearning","permalink":"https://jaehyeongan.github.io/tags/deeplearning/"},{"name":"keras","slug":"keras","permalink":"https://jaehyeongan.github.io/tags/keras/"},{"name":"machinelearning","slug":"machinelearning","permalink":"https://jaehyeongan.github.io/tags/machinelearning/"},{"name":"tensorflow","slug":"tensorflow","permalink":"https://jaehyeongan.github.io/tags/tensorflow/"},{"name":"functionalapi","slug":"functionalapi","permalink":"https://jaehyeongan.github.io/tags/functionalapi/"},{"name":"embedding","slug":"embedding","permalink":"https://jaehyeongan.github.io/tags/embedding/"}]},{"title":"CNN 모델을 통한 자동차 사고 이미지 분류","slug":"CNN-모델을-통한-자동차-사고-이미지-분류","date":"2018-07-01T03:50:26.000Z","updated":"2020-10-28T15:14:34.472Z","comments":true,"path":"2018/07/01/CNN-모델을-통한-자동차-사고-이미지-분류/","link":"","permalink":"https://jaehyeongan.github.io/2018/07/01/CNN-모델을-통한-자동차-사고-이미지-분류/","excerpt":"","text":"Intro회사 프로젝트에서 자동차 사고 이미지 분류 모델을 만들 일이 생겨 CNN 모델을 적용한 과정을 정리해 보고자 합니다.전체적으로 크롤링(crawling)을 통해 사고 이미지를 수집하였으며, 수집한 데이터를 바탕으 아래 7개의 사고를 분류하는 다중(multi class) 분류 모델을 생성하였습니다. 전방 추돌(Car front crash) 측면 추돌(Car side crash) 후방 추돌(Rear and crash) 유리창 깨짐(Car broken windshield) 차 스크래치Car scratch) 타이어 펑크(Flat tire) 전복 (Overturned vehicle) 사고 이미지 데이터 수집그 동안 크롤링을 할때 python의 lxml의 parse 함수를 이용하여 html 태그 기반으로 데이터를 수집하였는데, 정말 간딴하게! 구글에서 이미지를 수집할 수 있는 라이브러리인 icrawler를 알게 되어 쉽게 이미지를 수집할 수 있었습니다. 아래와 같이 icrawler의 GoogleImageCrawler()를 이용하였습니다.1234567from icrawler.builtin import GoogleImageCrawlergoogle_crawler = GoogleImageCrawler(parser_threads=2, downloader_threads=4, storage=&#123;'root_dir': '../data'&#125;)google_crawler.crawl(keyword='car crash', max_num=500, date_min=None, date_max=None, min_size=(200,200), max_size=None) keyward: 수집하고자 하는 이미지 max_num: 수집할 이미지 수 date_min/date_max: 수집할 기간 min_size/max_size: 이미지 크기 이후, 수집한 데이터를 이미지 처리 및 train/test set으로 나누었습니다.123456789101112131415161718192021222324252627282930313233343536373839404142rom PIL import Imageimport os, globimport numpy as npfrom sklearn.model_selection import train_test_split# 분류 대상 카테고리 선택하기 accident_dir = \"./image\"categories = [\"Car front crash\",\"Car side crash\",\"Rear and crash\",\"Car broken windshield\",\"Car scratch\",\"Flat tire\",\"Overturned vehicle\"]nb_classes = len(categories)# 이미지 크기 지정 image_w = 64 image_h = 64pixels = image_w * image_h * 3# 이미지 데이터 읽어 들이기 X = []Y = []for idx, cat in enumerate(categories): # 레이블 지정 label = [0 for i in range(nb_classes)] label[idx] = 1 # 이미지 image_dir = accident_dir + \"/\" + cat files = glob.glob(image_dir+\"/*.jpg\") for i, f in enumerate(files): img = Image.open(f) img = img.convert(\"RGB\") img = img.resize((image_w, image_h)) data = np.asarray(img) # numpy 배열로 변환 X.append(data) Y.append(label) if i % 10 == 0: print(i, \"\\n\", data)X = np.array(X)Y = np.array(Y)# 학습 전용 데이터와 테스트 전용 데이터 구분 X_train, X_test, y_train, y_test = \\ train_test_split(X, Y)xy = (X_train, X_test, y_train, y_test)print('&gt;&gt;&gt; data 저장중 ...')np.save(\"./image/7obj.npy\", xy)print(\"ok,\", len(Y)) 이미지를 RGB로 변환 후, 64x64 크기로 resize해주었습니다. CNN 모델 생성모델은 이미지 분류의 정석으로 불리는 CNN(Convolution Neural Network) 모델을 활용하였습니다.총 3개의 층으로 구성하였고, 활성화 함수로는 relu 및 softmax 함수를 적용하였습니다. dropout도 적용하여 과적합을 방지하였습니다. 모델 학습 후 .hdf5 파일로 저장합니다.12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758from keras.models import Sequentialfrom keras.layers import MaxPooling2Dfrom keras.layers import Conv2Dfrom keras.layers import Activation, Dropout, Flatten, Denseimport numpy as npimport os# 카테고리 지정하기categories = [\"Car front crash\",\"Car side crash\",\"Rear and crash\",\"Car broken windshield\",\"Car scratch\",\"Flat tire\",\"Overturned vehicle\"]nb_classes = len(categories)# 이미지 크기 지정하기image_w = 64image_h = 64# 데이터 열기 X_train, X_test, y_train, y_test = np.load(\"./image/7obj.npy\")# 데이터 정규화하기(0~1사이로)X_train = X_train.astype(\"float\") / 256X_test = X_test.astype(\"float\") / 256print('X_train shape:', X_train.shape)# 모델 구조 정의 model = Sequential()model.add(Conv2D(32, (3, 3), input_shape=X_train.shape[1:], padding='same'))model.add(Activation('relu'))model.add(MaxPooling2D(pool_size=(2, 2)))model.add(Dropout(0.25))model.add(Conv2D(64, (3, 3), padding='same'))model.add(Activation('relu'))model.add(Conv2D(64, (3, 3)))model.add(MaxPooling2D(pool_size=(2, 2)))model.add(Dropout(0.25))# 전결합층model.add(Flatten()) # 벡터형태로 reshapemodel.add(Dense(512)) # 출력model.add(Activation('relu'))model.add(Dropout(0.5))model.add(Dense(nb_classes))model.add(Activation('softmax'))# 모델 구축하기model.compile(loss='categorical_crossentropy', # 최적화 함수 지정 optimizer='rmsprop', metrics=['accuracy'])# 모델 확인#print(model.summary())# 학습 완료된 모델 저장hdf5_file = \"./image/7obj-model.hdf5\"if os.path.exists(hdf5_file): # 기존에 학습된 모델 불러들이기 model.load_weights(hdf5_file)else: # 학습한 모델이 없으면 파일로 저장 model.fit(X_train, y_train, batch_size=32, nb_epoch=10) model.save_weights(hdf5_file) 모델의 오차와 정확도를 살펴보겠습니다.1234# 모델 평가하기 score = model.evaluate(X_test, y_test)print('loss=', score[0]) # lossprint('accuracy=', score[1]) # acc 오차는 0.03, 정확도는 98% 정도의 성능을 나타냅니다. 확실히 데이터를 많이 학습시키니 성능이 좋은 것 같습니다. 신규 데이터 예측학습된 모델(7obj-model.hdf5)에 신규 이미지를 적용하여 이미지의 클래스를 예측해보도록 하겠습니다. 적용할 이미지는 아래의 차 전복(Overturned vehicle) 이미지 입니다. 모델에 적용해봅니다.1234567891011121314# 적용해볼 이미지 test_image = './image/test_overturned.jpg'# 이미지 resizeimg = Image.open(test_image)img = img.convert(\"RGB\")img = img.resize((64,64))data = np.asarray(img)X = np.array(data)X = X.astype(\"float\") / 256X = X.reshape(-1, 64, 64,3)# 예측pred = model.predict(X) result = [np.argmax(value) for value in pred] # 예측 값중 가장 높은 클래스 반환print('New data category : ',categories[result[0]]) 학습할때와 똑같이 이미지를 처리해 주고 저장된 모델을 통해 이미지를 예측합니다. 예측결과1New data category : Overturned vehicle Overturned Vehicle(차 전복) 클래스로 이미지가 모델에 의해 예측되었습니다! 모델이 잘 학습된 것 같습니다.각 클래스별로 500개 총 3500개의 이미지를 통해 학습한 CNN 다중 분류 모델의 성능이 생각보다 괜찮은 것 같습니다.","categories":[],"tags":[{"name":"cnn","slug":"cnn","permalink":"https://jaehyeongan.github.io/tags/cnn/"},{"name":"deeplearning","slug":"deeplearning","permalink":"https://jaehyeongan.github.io/tags/deeplearning/"},{"name":"keras","slug":"keras","permalink":"https://jaehyeongan.github.io/tags/keras/"},{"name":"crawling","slug":"crawling","permalink":"https://jaehyeongan.github.io/tags/crawling/"}]},{"title":"이상탐지 알고리즘을 통한 이상거래탐지(FDS)","slug":"이상탐지-알고리즘을-통한-이상거래탐지-FDS","date":"2018-06-30T09:47:01.000Z","updated":"2020-10-28T15:14:34.485Z","comments":true,"path":"2018/06/30/이상탐지-알고리즘을-통한-이상거래탐지-FDS/","link":"","permalink":"https://jaehyeongan.github.io/2018/06/30/이상탐지-알고리즘을-통한-이상거래탐지-FDS/","excerpt":"","text":"Intro금융거래 중 부정하게 사용되는 거래를 부정 거래라고 합니다. 그 중 신용카드 위변조, 도용, 부정거래에 대한 비율은 해마다 증가하고 있는 추세입니다. 아래 표는 연도별 신용카드 부정사용 금액. 따라서, 최근에는 국내 주요 은행들은 FDS(Fraud Detection System)을 도입하여 이러한 부정거래를 막기위해 노력하고 있지만 주로 룰(Rule) 기반으로 사람에 의해 이루어지기 때문에 실시간으로 정확한 탐지가 어려운 상황이라고 합니다. 목표여기서는 머신러닝을 이용하여, 이러한 부정거래를 탐지해 보고자 합니다. 하지만, 지도학습이 아닌 비지도 학습을 이용합니다. 그 중 이상 탐지(Outlier Detection) 알고리즘을 이용하여 라벨을 통한 학습이 아닌 이상치 데이터 집단을 찾아 그 이상치 집단이 부정거래 데이터와 일치 및 유사한지 알아볼 것입니다. 1. 신용카드 데이터 셋데이터 셋의 경우 kaggle에서 제공하는 Credit Card Fraud Detection Dataset을 이용하였습니다.위 데이터 셋은 2013년 9월 유럽의 실제 신용 카드 거래 데이터를 담고 있습니다. 데이터는 총 284,807건이며 그 중 492건만이 부정 거래 데이터 입니다.즉, 데이터가 매우 불균형(imbalanced) 합니다. 1234import pandas as pddf = pd.read_csv('./input/creditcard.csv')df.head(10) 위 데이터 셋은 개인정보 비식별화처리로 인해 칼럼정보를 알 수 없으며, 데이터 또한 스케일(scale) 및 PCA(principal component analysis) 처리 되어있습니다.총 31개의 칼럼으로 이루어져 있고, Time, Amount, Class를 제외한 모든 칼럼은 비식별화처리 되어있습니다. 1df.info() 데이터는 총 284,807건이며 null값은 존재하지 않는 정형 데이터 입니다. 2. 데이터 탐색(EDA) 시간(Time)대별 정상/부정 거래 비율 1234567891011import matplotlib.pyplot as plt# 시간대별 트랜잭션 양f, (ax1, ax2) = plt.subplots(2,1, sharex=True, figsize=(12,4))ax1.hist(df.Time[df.Class==1], bins=50)ax2.hist(df.Time[df.Class==0], bins=50)ax1.set_title('Fraud')ax2.set_title('Normal')plt.xlabel('Time(in Seconds)'); plt.ylabel('Number of Transactions')plt.show() 음.. 대체적으로 정상 거래의 경우 시간에 따라 주기적인 반면 부정 거래의 경우 불규칙한 특성을 보입니다. 금액(Amount)대별 정상/부정 거래 비율 12345678910111213import matplotlib.pyplot as plt# 금액대별 트랜잭션 양f, (ax1, ax2) = plt.subplots(2,1, sharex=True, figsize=(12,4))ax1.hist(df.Amount[df.Class==1], bins=30)ax2.hist(df.Amount[df.Class==0], bins=30)ax1.set_title('Fraud')ax2.set_title('Normal')plt.xlabel('Amount ($)')plt.ylabel('Number of Transactions')plt.yscale('log')plt.show() 정상 거래의 경우 다양한 금액대에서 발생되지만, 부정 거래의 경우 적은 금액에서 주로 발생하는 것 같습니다. 비식별칼럼 정상/부정거래 비율특성 차이가 심한 일부 변수만 표시하였습니다. 1234567891011import matplotlib.pyplot as pltimport seaborn as sns# 정상/비정산 럼간 값 분포v_features = df.ix[:,1:29].columnsfor cnt, col in enumerate(df[v_features]): sns.distplot(df[col][df.Class==1], bins=50) sns.distplot(df[col][df.Class==0], bins=50) plt.legend(['Y','N'], loc='best') plt.title('histogram of feature '+str(col)) plt.show() 3. Isolation Forest이상탐지 알고리즘으로는 Isolation Forest 알고리즘을 이용하였습니다. Isolation Forest 는 Tree 기반으로 데이터를 나누어 데이터의 관측치를 고립시키는 알고리즘입니다. 이상 데이터의 경우 root node와 가까운 depth를 가지고, 정상 데이터의 경우 tree의 말단 노드에 가까운 depth를 가집니다. 4. 이상 탐지 알고리즘 적용Isolation Forest 알고리즘은 현재 scikit-learn에서 제공되고 있으며, 링크를 통해 다큐먼트를 확인하실 수 있습니다.Isolation Forest는 이상치 점수(outlier score)를 제공합니다. 정상 거래/ 부정 거래에 대한 이상치 점수는 아래와 같습니다.위의 분포를 보았을 때, 정상 / 부정 거래 간 비율이 다르게 나타나는 것을 확인할 수 있습니다. Python우선 적용하기에 앞서, 데이터 불균형(Data Imbalance)를 해결하기 위하여, 정상 거래건에 대해 Down sampling을 70% 비율로 진행하였습니다.123456789101112import pandas as pd from imblearn.under_sampling import RandomUnderSamplercredit_data = pd.read_csv('./data/creditcard.csv')X = credit_data.drop(['Class'], axis=1)y = credit_data['Class']print(Counter(y)) # &#123;0: 284315, 1: 492&#125;# Under Samplingsampler = RandomUnderSampler(ratio=0.70, random_state=0)X, y = sampler.fit_sample(X, y)print('Class : ',Counter(y)) # &#123;0: 702, 1: 492&#125; 이후, Isolation Forest를 아래와 같은 파라미터를 통해 적용하였습니다.123456from sklearn.ensemble import IsolationForestclf = IsolationForest(n_estimators=300, contamination=0.40, random_state=42)clf.fit(X)pred_outlier = clf.predict(X)pred_outlier = pd.DataFrame(pred_outlier).replace(&#123;1:0, -1:1&#125;) n_estimators : 노드 수 contamination : 이상치 비율 이상탐지 예측값은 1이 정상, -1이 이상으로 분류됩니다. 이를 Class 라벨과의 오차를 계산하여야 하기 때문에, 같은 범위로 바꿔주었습니다. 시각화이상탐지 결과를 2d 및 3d로 시각화한 결과 입니다. ( 시각화를 위해 차원을 축소하였습니다.)12345678910111213141516import matplotlib.pyplot as pltfrom mpl_toolkits.mplot3d import Axes3D# plot 2dplt.scatter(X[:,0], X[:,1], c=pred_outlier, cmap='Paired', s=40, edgecolors='white')plt.title(\"Isolation Forest\")plt.show()# plot 3dfig = plt.figure()ax = fig.add_subplot(111, projection='3d')ax.scatter(X[:,0], X[:,1], X[:,2], c=pred_outlier)ax.set_xlabel('pcomp 1')ax.set_ylabel('pcomp 2')ax.set_zlabel('pcomp 3')plt.show() 2차원 시각화 3차원 시각화 예측 성능예측값을 실제 부정거래여부 칼럼인 Class와 비교하여 성능을 살펴보겠습니다.측정 지표로는 Accuracy(정확도), Recall(재현율), Precision(정밀도), F1-score 입니다. 금융 거래에서는 정확도도 물론 중요하지만, 실제 부정거래를 부정거래로 예측하는 비율인 Recall(재현율) 값이 중요하게 여겨집니다.1234567891011121314151617181920212223242526272829from sklearn.metrics import confusion_matrix, classification_report, accuracy_scoreimport itertoolsclass_name = [0, 1]def plot_confusion_matrix(classes, pred, y_test, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues): cm = confusion_matrix(y_test, pred) plt.imshow(cm, interpolation='nearest', cmap=cmap) plt.title(title) plt.colorbar() tick_marks = np.arange(len(classes)) plt.xticks(tick_marks, classes, rotation=0) plt.yticks(tick_marks, classes) if normalize: cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] thresh = cm.max() / 2. for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])): plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=\"white\" if cm[i, j] &gt; thresh else \"black\")# 평가print('confusion matrix\\n', confusion_matrix(pred_outlier, y))print('Accuracy: ',accuracy_score(pred_outlier, y))print('classification_report\\n', classification_report(pred_outlier, y))plot_confusion_matrix(class_name, pred_outlier, y, title='Isolation Forest') Confusion Matrix 결과 Classification report 결과 12345678Accuracy: 0.8442211055276382classification_report precision recall f1-score support 0 0.88 0.86 0.87 716 1 0.80 0.82 0.81 478avg / total 0.85 0.84 0.84 1194 위 결과를 보았을 때, Accuracy 무려 84% 입니다.Recall 값 또한 82% 로 높은 부정 거래 탐지 정확도를 나타냅니다. Outro데이터를 목표 변수를 통해 학습하는 지도학습 알고리즘에 비하면 적은 정확도이겠지만,데이터를 전혀 학습하지 않고, 데이터의 특성만을 고려하여 이상치를 찾아내는 비지도 학습으로도 충분히 부정 거래를 탐지할 수 있다는 것을 확인하였습니다. 최근에는 딥러닝 기법을 이용하여 오토인코더(Auto-encoder)나 GAN 알고리즘을 이용하여 이상탐지에 활용되고 있습니다. 저도 더 공부해서 한번 적용해봐야겠습니다ㅠㅠㅠ","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://jaehyeongan.github.io/tags/python/"},{"name":"outlier detection","slug":"outlier-detection","permalink":"https://jaehyeongan.github.io/tags/outlier-detection/"},{"name":"isolation forest","slug":"isolation-forest","permalink":"https://jaehyeongan.github.io/tags/isolation-forest/"},{"name":"unsupervised learning","slug":"unsupervised-learning","permalink":"https://jaehyeongan.github.io/tags/unsupervised-learning/"},{"name":"scikit-learn","slug":"scikit-learn","permalink":"https://jaehyeongan.github.io/tags/scikit-learn/"},{"name":"fraud detection system","slug":"fraud-detection-system","permalink":"https://jaehyeongan.github.io/tags/fraud-detection-system/"}]},{"title":"파이썬 소켓(socket) 프로그래밍","slug":"파이썬-소켓-socket-프로그래밍","date":"2018-06-29T08:22:55.000Z","updated":"2020-10-28T15:14:34.486Z","comments":true,"path":"2018/06/29/파이썬-소켓-socket-프로그래밍/","link":"","permalink":"https://jaehyeongan.github.io/2018/06/29/파이썬-소켓-socket-프로그래밍/","excerpt":"","text":"파이썬 소켓(socket) 프로그래밍 소켓(socket)을 통해 서버(server)와 클라이언트(client)간 어떻게 기본적인 네트워크 통신이 이루어지는지 알아보려고 합니다.먼저 통신을 위해 두개의 파일은 준비합니다. 파일은 각각 서버와 클라이언트에 해당합니다. server.py client.py 우선 server.py 작성 123456789101112131415161718192021222324252627282930313233from socket import *from select import *HOST = ''PORT = 10000BUFSIZE = 1024ADDR = (HOST, PORT)# 소켓 생성serverSocket = socket(AF_INET, SOCK_STREAM)# 소켓 주소 정보 할당 serverSocket.bind(ADDR)print('bind')# 연결 수신 대기 상태serverSocket.listen(100)print('listen')# 연결 수락clientSocekt, addr_info = serverSocket.accept()print('accept')print('--client information--')print(clientSocekt)# 클라이언트로부터 메시지를 가져옴data = clientSocekt.recv(65535)print('recieve data : ',data.decode())# 소켓 종료 clientSocekt.close()serverSocket.close()print('close') 우선 소켓을 설정하고, bind()함수를 통해 주소 정보를 할당한다. 이후, listen()함수를 통해 연결 수신 대기 상태로 전환 후 client가 연결할 시 accpet() 함수를 이용하여 연결을 수락한다. 만약 client가 보낸 메시지가 있을 경우, recv(byte크기)를 이용하여 메시지를 가져온다. 이제 client.py를 작성 123456789101112131415161718192021222324#! /usr/bin/python# -*- coding: utf-8 -*-from socket import *from select import *import sysfrom time import ctimeHOST = '127.0.0.1'PORT = 10000BUFSIZE = 1024ADDR = (HOST,PORT)clientSocket = socket(AF_INET, SOCK_STREAM)# 서버에 접속하기 위한 소켓을 생성한다.try: clientSocket.connect(ADDR)# 서버에 접속을 시도한다. clientSocket.send('Hello!'.encode()) # 서버에 메시지 전달except Exception as e: print('%s:%s'%ADDR) sys.exit()print('connect is success') 주소와 포트번호를 설정 server에 접속하기 위한 client socket을 생성하고 connect()함수를 이용하여 서버에 접속을 시도 send()함수를 이용해 메시지를 server에 전달 이제 위의 코드를 실행해보도록 하겠습니다. server.py를 실행 후 client.py를 통해 server에 접속하는 과정입니다. 먼저 server.py를 실행하여, client의 접속을 기다립니다. 이후, client.py를 실행하여 server에 접속을 시도합니다. server에서 client의 접속정보와 메시지를 확인합니다.","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://jaehyeongan.github.io/tags/python/"},{"name":"network","slug":"network","permalink":"https://jaehyeongan.github.io/tags/network/"},{"name":"server","slug":"server","permalink":"https://jaehyeongan.github.io/tags/server/"},{"name":"client","slug":"client","permalink":"https://jaehyeongan.github.io/tags/client/"},{"name":"socket","slug":"socket","permalink":"https://jaehyeongan.github.io/tags/socket/"}]}]}